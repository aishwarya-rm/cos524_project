title,abstract,authors,affiliation,conference,date
Manipulating and Measuring Model Interpretability - Microsoft Research,"Despite a growing literature on creating interpretable machine learning methods, there have been few experimental studies of their effects on end users. We present a series of large-scale, randomized, pre-registered experiments in which participants were shown functionally identical models that varied only in two factors thought to influence interpretability: the number of input features and the model transparency (clear or black-box). Participants who were shown a clear model with a small number of features were better able to simulate the model’s predictions. However, contrary to what one might expect when manipulating interpretability, we found no significant difference in multiple measures of trust across conditions. Even more surprisingly, increased transparency hampered people’s ability to detect when a model has made a sizeable mistake. These findings emphasize the importance of studying how models are presented to people and empirically verifying that interpretable models achieve their intended effects on end users.","['Forough Poursabzi-Sangdeh', 'Dan Goldstein', 'Jake Hofman', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,CHI 2021,2021-05-20
"""I Can’t Reply with That"": Characterizing Problematic Email Reply Suggestions - Microsoft Research","In email interfaces, providing users with reply suggestions may simplify or accelerate correspondence.  While the “success'” of such systems is typically quantified using the number of suggestions selected by users, this ignores the impact of social context, which can change how suggestions are perceived.  To address this, we developed a mixed-methods framework involving qualitative interviews and crowdsourced experiments to characterize problematic email reply suggestions.  Our interviews revealed issues with over-positive, dissonant, cultural, and gender-assuming replies, as well as contextual politeness.  In our experiments, crowdworkers assessed email scenarios that we generated and systematically controlled, showing that contextual factors like social ties and the presence of salutations impacts users’ perceptions of email correspondence.  These assessments created a novel dataset of human-authored corrections for problematic email replies. Our study highlights the social complexity of providing suggestions for email correspondence, raising issues that may apply to all social messaging systems.","['Ronald Robertson', 'Alexandra Olteanu', 'Fernando Diaz', 'Milad Shokouhi', 'Peter Bailey']",Microsoft,CHI Conference on Human Factors in Computing Systems (CHI ’21),2021-05-01
A Human-Centered Agenda for Intelligible Machine Learning - Microsoft Research,"To build machine learning systems that are reliable, trustworthy, and fair, we must be able to provide relevant stakeholders with an understanding of how these systems work. Yet what makes a system “intelligible” is difficult to pin down. Intelligibility is a fundamentally human-centered concept that lacks a one-size-fits-all solution. Although many intelligibility techniques have been proposed in the machine learning literature, there are many more open questions about how best to provide stakeholders with the information they need to achieve their desired goals. In this chapter, we begin with an overview of the intelligible machine learning landscape and give several examples of the diverse ways in which needs for intelligibility can arise. We provide an overview of the techniques for achieving intelligibility that have been proposed in the machine learning literature. We discuss the importance of taking a human-centered strategy when designing intelligibility techniques or when verifying that these techniques achieve their intended goals. We also argue that the notion of intelligibility should be expanded beyond machine learning models to other components of machine learning systems, such as datasets and performance metrics. Finally, we emphasize the necessity of tight integration between the machine learning and human–computer interaction communities.","['Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,UNKNOWN,2021-04-27
Overcoming Failures of Imagination in AI Infused System Development and Deployment - Microsoft Research,"NeurIPS 2020 requested that research paper submissions include impact statements on ‘potential nefarious uses and the consequences of failure.’ When researching, designing, and implementing systems, a key challenge to anticipating risks, however, is to overcome what Clarke (1962) called ‘failures of imagination.’ The growing research on bias, fairness, and transparency in computational systems aims to illuminate and mitigate harms, and could thus help inform reflections on possible negative impacts of particular pieces of technical work. The prevalent notion of computational harms — narrowly construed as either allocational or representational harms — does not fully capture the context dependent and unobservable nature of harms across the wide range of AI infused systems. The current literature primarily addresses only a small range of examples of harms to motivate algorithmic fixes, overlooking the wider scope of probable harms and the way these harms may affect different stakeholders. The system affordances and possible usage scenarios may also exacerbate harms in unpredictable ways, as they determine stakeholders’ control (including non-users) over how they interact with a system output. To effectively assist in anticipating and identifying harmful uses, we argue that frameworks of harms must be context-aware and consider a wider range of potential stakeholders, system affordances, uses, and outputs, as well as viable proxies for assessing harms in the widest sense.","['Margarita Boyarskaya', 'Alexandra Olteanu', 'Kate Crawford']",Microsoft,In the Navigating the Broader Impacts of AI Research Workshop at NeurIPS 2020,2020-12-01
Assessing and mitigating unfairness in credit models with the Fairlearn toolkit - Microsoft Research,"As AI plays an increasing role in the financial services industry, it is essential that financial services organizations anticipate and mitigate unintended consequences, including fairness-related harms, such as denying people services, initiating predatory lending, amplifying gender or racial biases, or violating laws such as the United States’ Equal Credit Opportunity Act (ECOA). To address these kinds of harms, fairness must be explicitly prioritized throughout the AI development and deployment lifecycle.","['Miro Dudík', 'William Chen', 'Solon Barocas', 'Mario Inchiosa', 'Nick Lewins', 'Miruna Oprescu', 'Joy Qiao', 'Mehrnoosh Sameki', 'Mario Schlener', 'Jason Tuo', 'Hanna Wallach']",Microsoft,UNKNOWN,2020-09-22
When Are Search Completion Suggestions Problematic? - Microsoft Research,"Problematic web search query completion suggestions—perceived as biased, offensive, or in some other way harmful—can reinforce existing stereotypes and misbeliefs, and even nudge users towards undesirable patterns of behavior.  Locating such suggestions is difficult, not only due to the long-tailed nature of web search, but also due to differences in how people assess potential harms.  Grounding our study in web search query logs, we explore when system-provided suggestions might be perceived as problematic through a series of crowd-experiments where we systematically manipulate: the search query fragments provided by users,  possible user search intents, and the list of query completion suggestions.  To examine why query suggestions might be perceived as problematic, we contrast them to an inventory of known types of problematic suggestions. We report our observations around differences in the prevalence of  a) suggestions that are problematic on their own versus  b) suggestions that are problematic for the query fragment provided by a user, for both common informational needs and in the presence of web search voids—topics searched by few to no users.  Our experiments surface a rich array of scenarios where suggestions are considered problematic, including due to the context in which they were surfaced. Compounded by the elusive nature of many such scenarios, the prevalence of suggestions perceived as problematic only for certain user inputs, raises concerns about blind spots due to data annotation practices that may lead to some types of problematic suggestions being overlooked.","['Alexandra Olteanu', 'Fernando Diaz', 'Gabriella Kazai']",Microsoft,Computer Supported Collaborative Work and Social Computing (CSCW),2020-08-01
The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons - Microsoft Research,"Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established “principal reason” explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant—and withholding others.","['Solon Barocas', 'Andrew Selbst', 'Manish Raghavan']",Microsoft,FAT* 2020,2020-08-01
Mitigating bias in algorithmic hiring: evaluating claims and practices - Microsoft Research,"There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.","['Manish Raghavan', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy']",Microsoft,FAT* 2020,2020-08-01
Operationalizing the Legal Principle of Data Minimization for Personalization - Microsoft Research,"Article 5(1)(c) of the European Union’s General Data Protection Regulation (GDPR) requires that “personal data shall be […] adequate, relevant, and limited to what is necessary in relation to the purposes for which they are processed (‘data minimisation’)”. To date, the legal and computational definitions of ‘purpose limitation’ and ‘data minimization’ remain largely unclear. In particular, the interpretation of these principles is an open issue for information access systems that optimize for user experience through personalization and do not strictly require personal data collection for the delivery of basic service.","['Asia J. Biega', 'Peter Potash', 'Hal Daumé III', 'Fernando Diaz', 'Michèle Finck']",Microsoft,SIGIR 2020,2020-07-28
Evaluating Stochastic Rankings with Expected Exposure - Microsoft Research,"We introduce the concept of expected exposure as the average attention ranked items receive from users over repeated samples of the same query. Furthermore, we advocate for the adoption of the principle of equal expected exposure: given a fixed information need, no item receive more or less expected exposure compared to any other item of the same relevance grade. We argue that this principle is desirable for many retrieval objectives and scenarios, including topical diversity and fair ranking. Leveraging user models from existing retrieval metrics, we propose a general evaluation methodology based on expected exposure and draw connections to related metrics in information retrieval evaluation. Importantly, this methodology relaxes classic information retrieval assumptions, allowing a system, in response to a query, to produce a distribution over rankings instead of a single fixed ranking. We study the behavior of the expected exposure metric and stochastic rankers across a variety of information access conditions, including ad hoc retrieval and recommendation. We believe that measuring and optimizing expected exposure metrics using randomization opens a new area for retrieval algorithm development and progress.","['Fernando Diaz', 'Bhaskar Mitra', 'Michael D. Ekstrand', 'Asia J. Biega', 'Ben Carterette']",Microsoft,Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM),2020-07-01
On the Social and Technical Challenges of Web Search Autosuggestion Moderation - Microsoft Research,"Past research shows that users benefit from systems that support them in their writing and exploration tasks. The autosuggestion feature of Web search engines is an example of such a system: It helps users in formulating their queries by offering a list of suggestions as they type. Autosuggestions are typically generated by machine learning (ML) systems trained on a corpus of search logs and document representations. Such automated methods can become prone to issues that result in problematic suggestions that are biased, racist, sexist or in other ways inappropriate. While current search engines have become increasingly proficient at suppressing such problematic suggestions, there are still persistent issues that remain. In this paper, we reflect on past efforts and on why certain issues still linger by covering explored solutions along a prototypical pipeline for identifying, detecting, and addressing problematic autosuggestions. To showcase their complexity, we discuss several dimensions of problematic suggestions, difficult issues along the pipeline, and why our discussion applies to the increasing number of applications beyond web search that implement similar textual suggestion features. By outlining persistent social and technical challenges in moderating web search suggestions, we provide a renewed call for action.","['T. J. Hazen', 'Alexandra Olteanu', 'Gabriella Kazai', 'Fernando Diaz', 'Michael Golebiewski']",Microsoft,UNKNOWN,2020-07-01
Language (Technology) is Power: A Critical Survey of “Bias” in NLP - Microsoft Research,Abstract,"['Su Lin Blodgett', 'Solon Barocas', 'Hal Daumé III', 'Hanna Wallach']",Microsoft,ACL,2020-06-01
Privacy Dependencies - Microsoft Research,"This Article offers a comprehensive survey of privacy dependencies—the many ways that our privacy depends on the decisions and disclosures of other people. What
we do and what we say can reveal as much about others as it does about ourselves, even when we don’t realize it or when we think we’re sharing information about ourselves alone.
We identify three bases upon which our privacy can depend: our social ties, our similarities to others, and our differences from others. In a tie-based dependency, an observer learns
about one person by virtue of her social relationships with others—family, friends, or other associates. In a similarity-based dependency, inferences about our unrevealed attributes are
drawn from our similarities to others for whom that attribute is known. And in difference-based dependencies, revelations about ourselves demonstrate how we are different from
others—by showing, for example, how we “break the mold” of normal behavior or establishing how we rank relative to others with respect to some desirable attribute. We
elaborate how these dependencies operate, isolating the relevant mechanisms and providing concrete examples of each mechanism in practice, the values they implicate, and the legal
and technical interventions that may be brought to bear on them. Our work adds to a growing chorus demonstrating that privacy is neither an individual choice nor an individual value—
but it is the first to systematically demonstrate how different types of dependencies can raise very different normative concerns, implicate different areas of law, and create different
challenges for regulation.","['Solon Barocas', 'Karen Levy']",Microsoft,Washington Law Review,2020-06-01
Fairlearn: A toolkit for assessing and improving fairness in AI - Microsoft Research,"We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.","['Sarah Bird', 'Miro Dudík', 'Richard Edgar', 'Brandon Horn', 'Roman Lutz', 'Vanessa Milan', 'Mehrnoosh Sameki', 'Hanna Wallach', 'Kathleen Walker']",Microsoft,UNKNOWN,2020-05-18
Towards Query Logs For Privacy Studies: On Deriving Search Queries From Questions - Microsoft Research,"Translating verbose information needs into crisp search queries is a phenomenon that is ubiquitous but hardly understood. Insights into this process could be valuable in several applications, including synthesizing large privacy-friendly query logs from public Web sources which are readily available to the academic research community. In this work, we take a step towards understanding query formulation by tapping into the rich potential of community question answering (CQA) forums. Specifically, we sample natural language (NL) questions spanning diverse themes from the Stack Exchange platform, and conduct a large-scale conversion experiment where crowdworkers submit search queries they would use when looking for equivalent information. We provide a careful analysis of this data, accounting for possible sources of bias during conversion, along with insights into user-specific linguistic patterns and search behaviors. We release a dataset of 7,000 question-query pairs from this study to facilitate further research on query understanding.","['Asia J. Biega', 'Jana Schmidt', 'Rishiraj Saha Roy']",Microsoft,ECIR 2020,2020-04-14
A Human in the Loop is Not Enough: The Need for Human-Subject Experiments in Facial Recognition - Microsoft Research,"The deployment of facial recognition systems in high-stakes scenarios has sparked widespread concerns about privacy, fairness, and accountability. A common response to these concerns is the suggestion of adding a human in the loop to provide oversight and ensure fairness and accountability. However, the effectiveness of this approach is seldom studied empirically, and humans are known to have biases of their own. In this position paper, we argue for the necessity of empirical studies of human-in-the-loop facial recognition systems. We outline several technical and ethical challenges that arise when conducting such empirical studies and when interpreting their results. Our goal is to initiate a discussion about ways for AI and HCI researchers to work together on human-centered approaches to empirically studying human-in-the-loop facial recognition systems.","['Forough Poursabzi-Sangdeh', 'Samira Samadi', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,UNKNOWN,2020-04-01
“I Don't Want Someone to Watch Me While I'm Working”: Gendered Views of Facial Recognition Technology in Workplace Surveillance - Microsoft Research,"Employers are increasingly using information and communication technologies to monitor employees. Such workplace surveillance is extensive in the United States, but its experience and potential consequences differ across groups based on gender. We thus sought to identify whether self‐reported male and female employees differ in the extent to which they find the use of workplace cameras equipped with facial recognition technology (FRT) acceptable, and examine the role of privacy attitudes more generally in mediating views on workplace surveillance. Using data from a nationally representative survey conducted by the Pew Research Center, we find that women are much less likely than men to approve of the use of cameras using FRT in the workplace. We then further explore whether men and women think differently about privacy, and if perceptions of privacy moderate the relationship between gender and approval of workplace surveillance. Finally, we consider the implications of these findings for privacy and surveillance via embedded technologies, and how the consequences of surveillance and technologies like FRT may be gendered. Note: We recognize evaluations based on a binary definition of gender are invariably partial and exclusionary. As we note in our discussion of the study’s limitations, we were constrained by the survey categories provided by Pew.","['Luke Stark', 'Amanda Stanhaus', 'Denise L. Anthony']",Microsoft,Journal of the Association for Information Science and Technology,2020-03-10
Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI - Microsoft Research,"Many organizations have published principles intended to guide the ethical development and deployment of AI systems; however, their abstract nature makes them difficult to operationalize. Some organizations have therefore produced AI ethics checklists, as well as checklists for more specific concepts, such as fairness, as applied to AI systems. But unless checklists are grounded in practitioners’ needs, they may be misused. To understand the role of checklists in AI ethics, we conducted an iterative co-design process with 48 practitioners, focusing on fairness. We co-designed an AI fairness checklist and identified desiderata and concerns for AI fairness checklists in general. We found that AI fairness checklists could provide organizational infrastructure for formalizing ad-hoc processes and empowering individual advocates. We discuss aspects of organizational culture that may impact the efficacy of such checklists, and highlight future research directions.","['Michael Madaio', 'Luke Stark', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,CHI Conference on Human Factors in Computing Systems,2020-03-01
"When not to design, build, or deploy - Microsoft Research","Recent debate within the FAT* community has focused on how the field conceptualizes the problems it seeks to address, what approach the field should take in attempting to address these problems, and whether the field should even pursue some of the proposed remedies. Questions regarding when not to design, build, or deploy a technology are perhaps the most common expression of this trend. Identifying the problems to address is inextricably linked to the broader question of how to collectively make decisions about what technologies our societies need and want.","['Solon Barocas', 'Asia J. Biega', 'Benjamin Fish', 'Jędrzej Niklas', 'Luke Stark']",Microsoft,"2020 Conference on Fairness, Accountability, and Transparency (FAT* '20)",2020-01-01
Roles for computing in social change - Microsoft Research,"A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems — roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined — changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing’s capacity to solve social problems on its own.","['Rediet Abebe', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy', 'Manish Raghavan', 'David Robinson']",Microsoft,FAT*,2020-01-01
"FACTS-IR: Fairness, Accountability, Confidentiality, Transparency, and Safety in Information Retrieval - Microsoft Research","The purpose of the SIGIR 2019 workshop on Fairness, Accountability, Confidentiality, Transparency, and Safety (FACTS-IR) was to explore challenges in responsible information retrieval system development and deployment. To this end, the workshop aimed to crowdsource from the larger SIGIR community and draft an actionable research agenda on five key dimensions of responsible information retrieval: fairness, accountability, confidentiality, transparency, and safety. Such an agenda can guide others in the community that are interested in pursuing FACTS-IR research, as well as inform potential funders about relevant research avenues. The workshop brought together a diverse set of researchers and practitioners interested in contributing to the development of a technical research agenda for responsible information retrieval.","['Alexandra Olteanu', 'Jean Garcia-Gathright', 'Maarten de Rijke', 'Michael D. Ekstrand']",Microsoft,ACM SIGIR Forum,2019-12-02
Stretching Human Laws to Apply to Machines: The Dangers of a 'Colorblind' Computer - Microsoft Research,"Automated decision making has become widespread in recent years, largely due to advances in machine learning. As a result of this trend, machine learning systems are increasingly used to make decisions in high-stakes domains, such as employment or university admissions. The weightiness of these decisions has prompted the realization that, like humans, machines must also comply with the law. But human decision-making processes are quite different from automated decision-making processes, which creates a mismatch between laws and the decision makers to which they are intended to apply. In turn, this mismatch can lead to counterproductive outcomes.","['Zach Harned', 'Hanna Wallach']",Microsoft,"Florida State University Law Review, Forthcoming",2019-12-01
Overview of the TREC 2019 Fair Ranking Track - Microsoft Research,"


The goal of the TREC Fair Ranking track was to develop a benchmark for evaluating retrieval systems in terms of fairness to different content providers in addition to classic notions of relevance. As part of the benchmark, we defined standardized fairness metrics with evaluation protocols and released a dataset for the fair ranking problem. The 2019 task focused on reranking academic paper abstracts given a query. The objective was to fairly represent relevant authors from several groups that were unknown at the system submission time. Thus, the track emphasized the development of systems which have robust performance across a variety of group definitions. Participants were provided with querylog data (queries, documents, and relevance) from Semantic Scholar. This paper presents an overview of the track, including the task definition, descriptions of the data and the annotation process, as well as a comparison of the performance of submitted systems.


","['Asia J. Biega', 'Fernando Diaz', 'Michael D. Ekstrand', 'Sebastian Kohlmeier']",Microsoft,TREC 2019,2019-11-01
Toward Fairness in AI for People with Disabilities: A Research Roadmap - Microsoft Research,"AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.","['Anhong Guo', 'Ece Kamar', 'Jennifer Wortman Vaughan', 'Hanna Wallach', 'Meredith Ringel Morris']",Microsoft,ASSETS 2019 Workshop on AI Fairness for People with Disabilities,2019-10-27
Fair Regression: Quantitative Definitions and Reduction-Based Algorithms - Microsoft Research,"In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems fair regression. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness–accuracy frontiers on several standard datasets.","['Alekh Agarwal', 'Miro Dudík', 'Zhiwei Steven Wu']",Microsoft,36th International Conference on Machine Learning,2019-09-01
Two-Sided Fairness for Repeated Matchings in Two-Sided Markets: A Case Study of a Ride-Hailing Platform - Microsoft Research,"Ride hailing platforms, such as Uber, Lyft, Ola or DiDi, have traditionally focused on the satisfaction of the passengers, or on boosting successful business transactions. However, recent studies provide a multitude of reasons to worry about the drivers in the ride hailing ecosystem. The concerns range from bad working conditions and worker manipulation to discrimination against minorities. With the sharing economy ecosystem growing, more and more drivers financially depend on online platforms and their algorithms to secure a living. It is pertinent to ask what a fair distribution of income on such platforms is and what power and means the platform has in shaping these distributions.","['Tom Sühr', 'Asia J. Biega', 'Meike Zehlike', 'Krishna P. Gummadi', 'Abhijnan Chakraborty']",Microsoft,KDD 2019,2019-08-04
"Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries - Microsoft Research","Social data in digital form, including user-generated content, expressed or implicit relations between people, and behavioral traces, are at the core of popular applications and platforms, driving the research agenda of many researchers. The promises of social data are many, including understanding “what the world thinks” about a social issue, brand, celebrity, or other entity, as well as enabling better decision-making in a variety of fields including public policy, healthcare, and economics. Many academics and practitioners have warned against the naive usage of social data. There are biases and inaccuracies occurring at the source of the data, but also introduced during processing. There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked. This paper recognizes the rigor with which these issues are addressed by different researchers varies across a wide range. We identify a variety of menaces in the practices around social data use, and organize them in a framework that helps to identify them.","['Alexandra Olteanu', 'Carlos Castillo', 'Fernando Diaz', 'Emre Kiciman']",Microsoft,Frontiers in Big Data,2019-07-11
Improving fairness in machine learning systems: What do industry practitioners need? - Microsoft Research,"The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams’ challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by industry practitioners and solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address industry practitioners’ needs.","['Ken Holstein', 'Jennifer Wortman Vaughan', 'Hal Daumé III', 'Miro Dudík', 'Hanna Wallach']",Microsoft,2019 ACM CHI Conference on Human Factors in Computing Systems,2019-06-01
Understanding the Effect of Accuracy on Trust in Machine Learning Models - Microsoft Research,"


We address a relatively under-explored aspect of human–computer interaction: people’s abilities to understand the relationship between a machine learning model’s stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople’s trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model’s stated accuracy on held-out data and on its observed accuracy in practice. We find that people’s trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to re- cent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.


","['Ming Ying', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,2019 ACM CHI Conference on Human Factors in Computing Systems,2019-05-01
What’s in a Name? Reducing Bias in Bios without Access to Protected Attributes - Microsoft Research,"There is a growing body of work that proposes methods for mitigating bias in machine learning systems. These methods typically rely on access to protected attributes such as race, gender, or age. However, this raises two significant challenges: (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classiﬁcation, we propose a method for discouraging correlation between the predicted probability of an individual’s true occupation and a word embedding of their name. This method leverages the societal biases that are encoded in word embeddings, eliminating the need for access to protected attributes. Crucially, it only requires access to individuals’ names at training time and not at deployment time. We evaluate two variations of our proposed method using a large-scale data set of online biographies. We ﬁnd that both variations simultaneously reduce race and gender biases, with almost no reduction in the classiﬁer’s overall true positive rate.","['Alexey Romanov', 'Maria De-Arteaga', 'Hanna Wallach', 'Jennifer Chayes', 'Christian Borgs', 'Alexandra Chouldechova', 'Sahin Geyik', 'krishnaram Kenthapadi', 'Anna Rumshisky', 'Adam Tauman Kalai']",Microsoft,NAACL 2019,2019-04-10
The Disparate Effects of Strategic Manipulation - Microsoft Research,"When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system’s approval. Models of agent responsiveness, termed “strategic manipulation,” analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to “trick” a published classifier. In cases of real world classification, however, an agent’s ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group’s costs are higher than the other’s, the learner’s equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner’s utility while actually making both candidate groups worse-off–even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual’s “quality” when agents’ capacities to adaptively respond differ.","['Lily Hu', 'Nicole Immorlica', 'Jennifer Wortman Vaughan']",Microsoft,"2nd ACM Conference on Fairness, Accountability, and Transparency",2019-01-27
Datasheets for Datasets - Microsoft Research,"The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.","['Timnit Gebru', 'Jamie Morgenstern', 'Briana Vecchione', 'Jennifer Wortman Vaughan', 'Hanna Wallach', 'Hal Daumé III', 'Kate Crawford']",Microsoft,UNKNOWN,2018-03-01
The Externalities of Exploration and How Data Diversity Helps Exploitation - Microsoft Research,"Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users for information that will lead to better decisions in the future. Recently, concerns have been raised about whether the process of exploration could be viewed as unfair, placing too much burden on certain individuals or groups. Motivated by these concerns, we initiate the study of the externalities of exploration – the undesirable side effects that the presence of one party may impose on another – under the linear contextual bandits model. We introduce the notion of a group externality, measuring the extent to which the presence of one population of users impacts the rewards of another. We show that this impact can in some cases be negative, and that, in a certain sense, no algorithm can avoid it. We then study externalities at the individual level, interpreting the act of exploration as an externality imposed on the current user of a system by future users. This drives us to ask under what conditions inherent diversity in the data makes explicit exploration unnecessary. We build on a recent line of work on the smoothed analysis of the greedy algorithm that always chooses the action that currently looks optimal, improving on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most O~(T1/3)","['Manish Raghavan', 'Alex Slivkins', 'Jennifer Wortman Vaughan', 'Zhiwei Steven Wu']",Microsoft,31st Annual Conference on Learning Theory,2018-02-01
Towards measuring fairness in AI: the Casual Conversations dataset,"This paper introduces a novel dataset to help researchers evaluate their computer vision and audio models for accuracy across a diverse set of age, genders, apparent skin tones and ambient lighting conditions. Our dataset is composed of 3,011 subjects and contains over 45,000 videos, with an average of 15 videos per person. The videos were recorded in multiple U.S. states with a diverse set of adults in various age, gender and apparent skin tone groups. A key feature is that each subject agreed to participate for their likenesses to be used. Additionally, our age and gender annotations are provided by the subjects themselves. A group of trained annotators labeled the subjects’ apparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations for videos recorded in low ambient lighting are also provided. As an application to measure robustness of predictions across certain attributes, we provide a comprehensive study on the top five winners of the DeepFake Detection Challenge (DFDC). Experimental evaluation shows that the winning models are less performant on some specific groups of people, such as subjects with darker skin tones and thus may not generalize to all people. In addition, we also evaluate the state-of-the-art apparent age and gender classification methods. Our experiments provides a through analysis on these models in terms of fair treatment of people from various backgrounds.","['Caner Hazirbas', 'Joanna Bitton', 'Brian Dolhansky', 'Jacqueline Pan', 'Albert Gordo', 'Cristian Canton Ferrer']",Facebook,UNKNOWN,2021-04-08
Practical Compositional Fairness: Understanding Fairness in Multi-Component Recommender Systems,"Most literature in fairness has focused on improving fairness with respect to one single model or one single objective. However, real-world machine learning systems are usually composed of many different components. Unfortunately, recent research has shown that even if each component is ""fair"", the overall system can still be ""unfair"".  In this paper, we focus on how well fairness composes over multiple components in real systems. We consider two recently proposed fairness metrics for rankings: exposure and pairwise ranking accuracy gap. We provide theory that demonstrates a set of conditions under which fairness of individual models does compose. We then present an analytical framework for both understanding whether a system's signals can achieve compositional fairness, and diagnosing which of these signals lowers the overall system's end-to-end fairness the most. Despite previously bleak theoretical results, on multiple data-sets -- including a large-scale real-world recommender system -- we find that the overall system's end-to-end fairness is largely achievable by improving fairness in individual components.","['Xuezhi Wang', 'Nithum Thain', 'Flavien Prost', 'Ed H. Chi', 'Alex Beutel']",Google,WSDM 2021,
Attribute-based Propensity for Unbiased Learning in Recommender Systems: Algorithm and Case Studies,"Many modern recommender systems train their models based on a large amount of implicit user feedback data. Due to the inherent bias in this data (e.g., position bias), learning from it directly can lead to suboptimal models. Recently, unbiased learning was proposed to address such problems by leveraging counterfactual techniques like inverse propensity weighting (IPW). In these methods, propensity scores estimation is usually limited to item's display position in a single user interface (UI).","['Zhen Qin', 'Don Metzler', 'Xuanhui Wang']",Google,26TH ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) (2020),
Bringing the People Back In: Contesting Benchmark Machine Learning Datasets,"In response to algorithmic unfairness embedded in sociotechnical systems, significant attention has been focused on the contents of machine learning datasets which have revealed biases towards white, cisgender, male, and Western data subjects. In contrast, comparatively less attention has been paid to the histories, values, and norms embedded in such datasets. In this work, we outline a research program - a genealogy of machine learning data - for investigating how and why these datasets have been created, what and whose values influence the choices of data to collect, the contextual and contingent conditions of their creation. We describe the ways in which benchmark datasets in machine learning operate as infrastructure and pose four research questions for these datasets. This interrogation forces us to ""bring the people back in"" by aiding us in understanding the labor embedded in dataset construction, and thereby presenting new avenues of contestation for other researchers encountering the data.",['Alex Hanna'],Google,"Participatory Approaches to Machine Learning, ICML 2020 Workshop (2020)",
CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation,"NLP models are shown to suffer from robustness issues, for example, a model's prediction can be easily changed under small perturbations to the input. In this work, we aim to present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, it can generate adversarial texts through controllable attributes that are known to be invariant to task labels. For example, for a main task like sentiment classification, an example attribute can be different categories/domains, and a model should have similar performance across them; for a coreference resolution task, a model's performance should not differ across different demographic attributes. Different from many existing adversarial text generation approaches, we show that our model can generate adversarial texts that are more fluent, diverse, and with better task-label invariance guarantees. We aim to use this model to generate counterfactual texts that could better improve robustness in NLP models (e.g., through adversarial training), and we argue that our generation can create more natural attacks.","['Xuezhi Wang', 'Alex Beutel', 'Ed H. Chi']",Google,EMNLP 2020,
Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditin,"Rising concern for the societal implications of artificial intelligencesystems has inspired a wave of academic and journalistic literaturein which deployed systems are audited for harm by investigatorsfrom outside the organizations deploying the algorithms. However,it remains challenging for practitioners to identify the harmfulrepercussions of their own systems prior to deployment, and, oncedeployed, emergent issues can become difficult or impossible totrace back to their source.In this paper, we introduce a framework for algorithmic auditingthat supports artificial intelligence system development end-to-end,to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that togetherform an overall audit report, drawing on an organizationâs valuesor principles to assess the fit of decisions made throughout the pro-cess. The proposed auditing framework is intended to contribute toclosing theaccountability gapin the development and deploymentof large-scale artificial intelligence systems by embedding a robustprocess to ensure audit integrity.","['Ben Hutchinson', 'Daniel Theron']",Google,"FAT* Barcelona, 2020 (2020)",
Deontological Ethics By Monotonicity Shape Constraints,"We demonstrate how easy it is for modern machine-learned systems to violate common deontological ethical principles and social norms such as ""favor the less fortunate,"" and ""do not penalize good attributes."" We propose that in some cases such ethical principles can be incorporated into a machine-learned model by adding shape constraints that constrain the model to respond only positively to relevant inputs. We analyze the relationship between these deontological constraints that act on individuals and the consequentialist group-based fairness goals of one-sided statistical parity and equal opportunity. This strategy works with sensitive attributes that are Boolean or real-valued such as income and age, and can help produce more responsible and trustworthy AI.",['Serena Wang'],Google,AISTATS (2020),
Diversity and Inclusion Metrics for Subset Selection,"The concept of fairness has recently been applied in machine learning settings to describe a wide range of constraints and objectives. When applied to ranking, recommendation, or subset selection problems for an individual, it becomes less clear that fairness goals are more applicable than goals that prioritize diverse outputs and instances that represent the individual's goals well.  In this work, we discuss the relevance of the concept of fairness to the concepts of diversity and inclusion, and introduce metrics that quantify the diversity and inclusion of an instance or set.  Diversity and inclusion metrics can be used in tandem, including additional fairness constraints, or may be used separately, and we detail how the different metrics interact.  Results from human subject experiments demonstrate that the proposed criteria for diversity and inclusion are consistent with social notions of these two concepts, and human judgments on the diversity and inclusion of example instances are correlated with the defined metrics.","['Dylan Baker', 'Ben Hutchinson', 'Alex Hanna']",Google,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES) (2020)",
Evading the Curse of Dimensionality in Unconstrained Private Generalized Linear Problems,"Differentially private gradient descent (DP-GD) has been extremely effective both theoretically, and in practice, for solving private empirical risk minimization (ERM) problems. In this paper, we focus on understanding the impact of the clipping norm, a critical component of DP-GD, on its convergence. We provide the first formal convergence analysis of clipped DP-GD.","['Shuang Song', 'Om Thakkar']",Google,24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021) (2020),
Explaining Deep Neural Networks using Unsupervised Clustering,"We propose a novel method to explain trained deep neural networks (DNNs), by distilling them into surrogate models using unsupervised clustering. Our method can be flexibly applied to any subset of layers of a DNN architecture and can incorporate low-level and high-level information. On image datasets given pre-trained DNNs, we demonstrate strength of our method in finding similar training samples, and shedding light on the concepts the DNN bases its decision on. Via user studies, we show that our model can improve user trust in modelâs prediction.",['Sercan Arik'],Google,2020 Workshop on Human Interpretability in Machine Learning (2020),
Fairness Indicators Demo: Scalable Infrastructure for Fair ML Systems,"The rise of machine learning around the globe in fields like medicine, education, employment, credit lending, and criminal sentencing has the potential to reflect and reinforce societal biases at large scale through the models deployed. While fairness concerns are multifaceted, technical evaluations and improvements of models are a critical aspect of a developer's role. And, for these considerations to truly scale, they must integrate into existing processes. In particular, we focus on seamlessly integrating known technical methods with existing libraries used for the training, evaluation, and deployment of models.  To showcase the suite of tools built in Tensorflow, we present an interactive case study demo in conjunction with Conversation AI, an ML research initiative to make online conversations more inclusive.",['Manasi N Joshi'],Google,(2020) (2020),
"Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives","How should we decide which fairness criteria or
definitions to adopt in machine learning systems?
To answer this question, we must study the fair-
ness preferences of actual users of machine learn-
ing systems. Stringent parity constraints on treat-
ment or impact can come with trade-offs, and
may not even be preferred by the social groups
in question (Zafar et al., 2017). Thus it might
be beneficial to elicit what the groupâs prefer-
ences are, rather than rely on a priori defined
mathematical fairness constraints. Simply asking
for self-reported rankings of users is challenging
because research has shown that there are often
gaps between peopleâs stated and actual prefer-
ences(Bernheim et al., 2013).",['Ben Hutchinson'],Google,Proceedings of ICML 2020 Workshop on Participatory Approaches to Machine Learning,
Federated Heavy Hitters with Differential Privacy,"The discovery of heavy hitters (most frequent items) in user-generated data streams drives improvements in the app and web ecosystems, but can incur substantial privacy risks if not done with care. To address these risks, we propose a distributed and privacy-preserving algorithm for discovering the heavy hitters in a population of user-generated data streams. We leverage the sampling property of our distributed algorithm to prove that it is inherently differentially private, without requiring additional noise. We also examine the trade-off between privacy and utility, and show that our algorithm provides excellent utility while also achieving strong privacy guarantees. A significant advantage of this approach is that it eliminates the need to centralize raw data while also avoiding the significant loss in utility incurred by local differential privacy. We validate our findings both theoretically, using worst-case analyses, and practically, using a Twitter dataset with 1.6M tweets and over 650k users. Finally, we carefully compare our approach to Apple's local differential privacy method for discovering heavy hitters.","['Wennan Zhu', 'Peter Kairouz', 'Brendan McMahan']",Google,International Conference on Artificial Intelligence and Statistics (AISTATS) 2020,
On Completeness-aware Concept-Based Explanations in Deep Neural Networks,"Concept-based explanations can be a key direction to understand how DNNs make decisions. In this paper, we study concept-based explainability in a systematic framework. First, we define the notion of completeness, which quantifies how sufficient a particular set of concepts is in explaining the model's behavior. Based on performance and variability motivations, we propose two definitions to quantify completeness. We show that they yield the commonly-used PCA method under certain assumptions. Next, we study two additional constraints to ensure the interpretability of discovered concept, based on sparsity principles. Through systematic experiments, on specifically-designed synthetic dataset and real-world text and image datasets, we demonstrate the superiority of our framework in finding concepts that are complete (in explaining the decision) and that are interpretable.","['Been Kim', 'Sercan Arik', 'Chun-Liang Li', 'Tomas Pfister']",Google,NeurIPS (2020),
Pairwise Fairness for Ranking and Regression,"Improving fairness for ranking and regression models has less mature algorithmic tooling than classifiers. Here, we present pairwise formulations of fairness for ranking and regression models that can express analogues of statistical fairness notions like equal opportunity or equal accuracy, as well as statistical parity. The proposed framework supports both discrete protected groups, and continuous protected attributes. We show that the resulting training problems can be efficiently and effectively solved using constrained optimization or robust optimization algorithms. Experiments illustrate the broad applicability and trade-offs of these methods.","['Harikrishna Narasimhan', 'Andy Cotter', 'Serena Lutong Wang']",Google,33rd AAAI Conference on Artificial Intelligence (2020),
Privacy Amplification via Random Check-Ins,"Differentially Private Stochastic Gradient Descent (DP-SGD) forms a fundamental building block in many applications for learning over sensitive data. Two standard approaches, privacy amplification by subsampling, and privacy amplification by shuffling, permit adding lower noise in DP-SGD than via na\""&lbrace;\i&rbrace;ve schemes. A key assumption in both these approaches is that the elements in the data set can be uniformly sampled, or be uniformly permuted ---  constraints that may become prohibitive when the data is processed in a decentralized or distributed fashion. In this paper, we focus on conducting iterative methods like DP-SGD in the setting of federated learning (FL) wherein the data is distributed among many devices (clients). Our main contribution is the random check-in distributed protocol, which crucially relies only on randomized participation decisions made locally and independently by each client. It has privacy/accuracy trade-offs similar to privacy amplification by subsampling/shuffling. However, our method does not require server-initiated communication, or even knowledge of the population size. To our knowledge, this is the first privacy amplification tailored for a distributed learning framework, and it may have broader applicability beyond FL. Along the way, we extend privacy amplification by shuffling to incorporate $(\epsilon,\delta)$-DP local randomizers, and exponentially improve its guarantees. In practical regimes, this improvement allows for similar privacy and utility using data from an order of magnitude fewer users.","['Peter Kairouz', 'H. Brendan McMahan', 'Om Thakkar']",Google,"Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020",
ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring,"We improve the recently-proposed ``MixMatch'' semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring. Distribution alignment encourages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of groundtruth labels. Augmentation anchoring feeds multiple strongly augmented versions of an input into the model and encourages each output to be close to the prediction for a weakly-augmented version of the same input. To produce strong augmentations, we propose a variant of AutoAugment which learns the augmentation policy while the model is being trained. Our new algorithm, dubbed ReMixMatch, is significantly more data-efficient than prior work, requiring between 5x and 16x less data to reach the same accuracy. For example, on CIFAR10 with 250 labeled examples we reach 93.73% accuracy (compared to MixMatchâs accuracy of 93.58% with 4,000 examples) and a median accuracy of 84.92% with just four labels per class. We make our code and data open-source at https://github.com/google-research/remixmatch.","['Alex Kurakin', 'Ekin Dogus Cubuk', 'Kihyuk Sohn', 'Nicholas Carlini']",Google,ICLR (2020),
Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing,"Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of five ethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.",['Joonseok Lee'],Google,"Proceedings of the 3rd AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) (2020)",
Social Biases in NLP Models as Barriers for Persons with Disabilities,"Building equitable and inclusive technologies
demands paying attention to how social attitudes towards persons with disabilities are
represented within technology. Representations perpetuated by NLP models often inadvertently encode undesirable social biases
from the data on which they are trained. In this
paper, first we present evidence of such undesirable biases towards mentions of disability in
two different NLP models: toxicity prediction
and sentiment analysis. Next, we demonstrate
that neural embeddings that are critical first
steps in most NLP pipelines also contain undesirable biases towards mentions of disabilities.
We then expose the topical biases in the social
discourse about some disabilities which may
explain such biases in the models; for instance,
terms related to gun violence, homelessness,
and drug addiction are over-represented in discussions about mental illness.","['Ben Hutchinson', 'Vinodkumar Prabhakaran', 'Kellie Webster', 'Yu Zhong']",Google,Proceedings of ACL 2020,
The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?,"There is a recent surge of papers that focus on attention as explanation of model predictions, giving mixed evidence on whether attention can be used as such. This has led some to try and `improve' attention so as to make it more interpretable. We argue that we should pay attention no heed.
While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear towards what goal it is used as explanation. We argue that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction. When that is the case, input saliency methods better suit our needs, and there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal for their explanations.","['Jasmijn Bastings', 'Katja Filippova']",Google,Proceedings of the 2020 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,
"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models","We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models--including classification, seq2seq, and structured prediction--and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.","['Ian Tenney', 'James Wexler', 'Jasmijn Bastings', 'Tolga Bolukbasi', 'Sebastian Gehrmann', 'Mahima Pushkarna', 'Emily Reif']",Google,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,
Thieves of Sesame Street: Model Extraction on BERT-based APIs,"We study the problem of model extraction in natural language processing, where an adversary with query access to a victim model attempts to reconstruct a local copy of the model. We show that when both the adversary and victim model fine-tune existing pretrained models such as BERT, the adversary does not need to have access to any training data to mount the attack. Indeed, we show that randomly sampled sequences of words, which do not satisfy grammar structures, make effective queries to extract textual models. This is true even for complex tasks such as natural language inference or question answering. ","['Gaurav Singh Tomar', 'Ankur Parikh', 'Nicolas Papernot']",Google,ICLR 2020 (2020),
Towards a Critical Race Methodology in Algorithmic Fairness,"We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.",['Alex Hanna'],Google,"ACM Conference on Fairness, Accountability, and  Transparency (ACM FAT*) (2020)",
Why Reliabilism Is Not Enough:Epistemic and Moral Justification in Machine Learning,"In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed widespread adoption of machine learning? We argue that, in general, people implicitly adopt reliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method. We argue that, in cases where model deployments require moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral âwrapperâ around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justificationâmoral justification. Finally, we offer cautions relevant to the (implicit or explicit)adoption of the reliabilist interpretation of machine learning.",['Ben Hutchinson'],Google,AIES 2020 (2020),
50 Years of Test (Un)fairness: Lessons for Machine Learning,"Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.",['Ben Hutchinson'],Google,Proceedings of FAT* 2019.,
Advances and Open Problems in Federated Learning,"Federated learning (FL) is a machine learning setting where many clients (e.g., mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g., service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and mitigates many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents a comprehensive list of open problems and challenges.","['Peter Kairouz', 'H. Brendan McMahan', 'K. A. Bonawitz', 'Zachary Charles', 'Zachary Garrett', 'Badih Ghazi', 'Ben Hutchinson', 'Jakub Konečný', 'Mehryar Mohri', 'Hang Qi', 'Daniel Ramage', 'Ananda Theertha Suresh', 'Zheng Xu', 'Sen Zhao']",Google,Arxiv (2019),
Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity,"Sensitive statistics are often collected across sets of users, with repeated collection of reports done over time. For example, trends in users' private preferences or software usage may be monitored via such reports. We study the collection of such statistics in the local differential privacy (LDP) model, and describe an algorithm whose privacy cost is polylogarithmic in the number of changes to a user's value. 
",[],Google,ACM-SIAM Symposium on Discrete Algorithms (SODA) (2019),
Audio De-identification: A New Entity Recognition Task,"Named Entity Recognition (NER) has been mostly studied in the context of written text. Specifically, NER is an important step in de-identification (de-ID) of medical records, many of which are recorded conversations between a patient and a doctor. In such recordings, audio spans with personal information should be redacted, similar to the redaction of sensitive character spans in de-ID for written text. The application of NER in the context of audio de-identification has yet to be fully investigated. To this end, we define the task of audio de-ID, in which audio spans with entity mentions should be detected. We then present our pipeline for this task, which involves Automatic Speech Recognition (ASR), NER on the transcript text, and text-to-audio alignment. Finally, we introduce a novel metric for audio de-ID and a new evaluation benchmark consisting of a large labeled segment of the Switchboard and Fisher audio datasets and detail our pipeline's results on it.","['Ido Cohn', 'Itay Laish', 'Genady Beryozkin', 'Gang Li', 'Izhak Shafran', 'Idan Szpektor', 'Avinatan Hassidim', 'Yossi Matias']",Google,NAACL (2019),
Characterizing Sources of Uncertainty to Proxy Calibration and Disambiguate Annotator and Data Bias,"Supporting model interpretability for complex phenomena where annotators can legitimately disagree, such as emotion recognition, is a challenging machine learning task. In this work, we show that explicitly quantifying the uncertainty in such settings has interpretability benefits. We use a simple modification of a classical network inference using Monte Carlo dropout to give measures of epistemic and aleatoric uncertainty. We identify a significant correlation between aleatoric uncertainty and human annotator disagreement (r â .3). Additionally, we demonstrate how difficult and subjective training samples can be identified using aleatoric uncertainty and how epistemic uncertainty can reveal data bias that could result in unfair predictions. We identify the total uncertainty as a suitable surrogate for model calibration, i.e. the degree we can trust model's predicted confidence. In addition to explainability benefits, we observe modest performance boosts from incorporating model uncertainty.",['Brendan Jou'],Google,ICCV Workshop on Interpreting and Explaining Visual Artificial Intelligence Models (2019),
Counterfactual Fairness in Text Classification through Robustness,"In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that ""Some people are gay'' is toxic while ""Some people are straight'' is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.","['Vincent Perot', 'Ed H. Chi', 'Alex Beutel']",Google,"AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) (2019)",
Debiasing Embeddings for Fairer Text Classification,"(Bolukbasi et al., 2016) demonstrated that pre-trained  word embeddings  can  inherit  gender bias from the data they were trained on.  We investigate  how  this  bias  affects  downstream classification  tasks,  using  the  case  study  of occupation  classification  (De-Arteaga  et  al.,2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy  channel  for  communicating  gender information.   With  a  relatively  minor  adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and obtain high classification accuracy.","['Flavien Prost', 'Nithum Thain', 'Tolga Bolukbasi']",Google,1st ACL Workshop on Gender Bias for Natural Language Processing (2019),
Deep determinantal generative classifier: robustness on noisy and adversarial samples,"Large-scale datasets may contain significant proportions of noisy (incorrect) class labels, and it is well-known that modern deep neural networks (DNNs) poorly generalize from such noisy training datasets. To mitigate the issue, we propose a novel inference method, termed Robust Generative classifier (RoG), applicable to any discriminative (e.g., softmax) neural classifier pre-trained on noisy datasets. In particular, we induce a generative classifier on top of hidden feature spaces of the pre-trained DNNs, for obtaining a more robust decision boundary. By estimating the parameters of generative classifier using the minimum covariance determinant estimator, we significantly improve the classification accuracy with neither re-training of the deep model nor changing its architectures. With the assumption of Gaussian distribution for features, we prove that RoG generalizes better than baselines under noisy labels. Finally, we propose the ensemble version of RoG to improve its performance by investigating the layer-wise characteristics of DNNs. Our extensive experimental results demonstrate the superiority of RoG given different learning models optimized by several training techniques to handle diverse scenarios of noisy labels.",[],Google,ICML (2019),
Detecting Bias with Generative Counterfactual Face  Attribute Augmentation,"We introduce a simple framework for identifying biases of a smiling attribute classifier. Our method poses counterfactual questions of the form: how would the prediction change if this face characteristic had been different? We leverage recent advances in generative adversarial networks to build a realistic generative model of faces that affords controlled manipulation of specific facial characteristics. Empirically, we identify several different factors of variation (that we believe should be in-dependent of a smiling) that  affect the predictions of a smiling classifier trained on CelebA.",['Ben Hutchinson'],Google,"Fairness, Accountability, Transparency and Ethics in Computer Vision Workshop (in conjunction with CVPR) (2019)",
Direct Uncertainty Prediction for Medical Second Opinions,"The issue of disagreements amongst human experts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresponds to doctor disagreements on a patient diagnosis. In this work, we show that machine learning models can be successfully trained to give uncertainty scores to data instances that result in high expert disagreements. In particular, they can identify patient cases that would benefit most from a medical second opinion. Our central methodological finding is that Direct Uncertainty Prediction (DUP), training a model to predict an uncertainty score directly from the raw patient features, works better than Uncertainty Via Classification, the two step process of training a classifier and postprocessing the output distribution to give an uncertainty score. We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging application.","['Maithra Raghu', 'Rory Abbott Sayres']",Google,ICML (2019),
Discovering User Bias in Ordinal Voting Systems,"Crowdsourcing systems increasingly rely on users to provide more
subjective ground truth for intelligent systems - e.g. ratings, aspect
of quality and perspectives on how expensive or lively a place feels,
etc. We focus on the ubiquitous implementation of online user ordinal voting (e.g 1-5, 1 star-4 stars) on some aspect of an entity, to
extract a relative truth, measured by a selected metric such as vote
plurality or mean. We argue that this methodology can aggregate
results that yield little information to the end user. In particular,
ordinal user rankings often converge to a indistinguishable rating.
This is demonstrated by the trend in certain cities for the majority of restaurants to all have a 4 star rating. Similarly, the rating of an establishment can be significantly affected by a few users.
User bias in voting is not spam, but rather a preference that can
be harnessed to provide more information to users. We explore
notions of both global skew and user bias. Leveraging these bias
and preference concepts, the paper suggests explicit models for
better personalization and more informative ratings.","['Alyssa Whitlock Lees', 'Chris Welty']",Google,"SAD-2019: Workshop on Subjectivity, Ambiguity and Disagreement",
Diversity-Sensitive Conditional Generative Adversarial Networks,"We propose a simple yet highly effective method that addresses the mode-collapse
problem in the Conditional Generative Adversarial Network (cGAN). Although
conditional distributions are multi-modal (i.e., having many modes) in practice,
most cGAN approaches tend to learn an overly simplified distribution where an
input is always mapped to a single output regardless of variations in latent code.
To address such issue, we propose to explicitly regularize the generator to produce
diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives.
Additionally, explicit regularization on generator allows our method to control a
balance between visual quality and diversity. We demonstrate the effectiveness
of our method on three conditional generation tasks: image-to-image translation,
image inpainting, and future video prediction. We show that simple addition of
our regularization to existing models leads to surprisingly diverse generations,
substantially outperforming the previous approaches for multi-modal conditional
generation specifically designed in each individual task.",[],Google,ICLR (2019),
Fairness in Recommendation Ranking through Pairwise Comparisons,"Recommender systems are one of the most pervasive applications of machine learning in industry, with many services using them to match users to products or information.  As such it is important to ask: what are the possible fairness risks, how can we quantify them, and how should we address them?","['Alex Beutel', 'Zhe Zhao', 'Ed H. Chi']",Google,KDD (2019),
Fairness Sample Complexity and the Case for Human Intervention,"With the aim of building machine learning systems that incorporate standards of fairness and accountability, we explore explicit subgroup sample complexity bounds. The work is motivated by the observation that classifier predictions for real world datasets often demonstrate drastically different metrics, such as accuracy, when subdivided by specific sensitive variable subgroups.  The reasons for these discrepancies are varied and not limited to the influence of mitigating variables, institutional bias, underlying population distributions as well as selection bias. Among the numerous definitions of fairness that exist, we argue that at a minimum, principled ML practices should ensure that classification predictions are able to mirror the underlying sub-population distributions as a prelude to bias mitigation, and not amplify discrepancies due to sampling/selection bias. However, as the number of sensitive variables grow, populations meeting at the intersectionality of these variables may simply not exist or be large enough to accurately sample from. In theses increasingly likely scenarios, the case for human intervention and applying situational and individual definitions of fairness should be made..    In this paper we explore, setting Pareto-efficient subgroup sample complexity lower bounds based on the complexity of the ML classifier using VC dimension and Rademacher complexity.  We demonstrate that for a classifier to approach a definition of fairness in terms of specific sensitive variables, adequate subgroup population samples need to exist and the model dimensionality has to be aligned with subgroup population distributions.  In cases where this is not feasible human intervention is explored.  We look at two commonly explored UCI datasets under this lens.",['Alyssa Whitlock Lees'],Google,Where is the Human? Bridging the Gap Between AI and HCI at Chi (2019),
Flexibly Fair Representation Learning by Disentanglement,"We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also \emph&lbrace;flexibly fair&rbrace;, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder---which does not require the sensitive attributes for inference---allows for the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.",['Kevin Jordan Swersky'],Google,ICML (2019),
"Generative Models for Effective ML on Private, Decentralized Datasets","To improve real-world applications of machine learning, experienced practitioners develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data - of representative samples, of outliers, of misclassifications, or alike - is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses, and c) assigning human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the  practitioner may only access aggregated outputs such as metrics or model parameters. This paper outlines a research agenda to address data-oriented tooling needs of ML practitioners who work with privacy-sensitive or decentralized datasets. We demonstrate that generative models - trained using federated methods and with formal differential privacy guarantees - can be used to effectively debug data issues even when the data cannot be directly inspected.","['Sean Augenstein', 'Brendan McMahan', 'Daniel Ramage', 'Swaroop Ramaswamy', 'Peter Kairouz', 'Mingqing Chen', 'Rajiv Mathews', 'Blaise Aguera-Arcas']",Google,Arxiv (2019),
Hiding Images Within Images,"We present a system to hide a full color image inside another of the
same size with minimal quality loss to either image.  Deep neural
networks are simultaneously trained to create the hiding and revealing
processes and are designed to specifically work as a pair.  The system
is trained on images drawn randomly from the ImageNet database, and
works well on natural images from a wide variety of sources.  Beyond
demonstrating the successful application of deep learning to hiding
images, we examine how the result is achieved and apply numerous
transformations to analyze if image quality in the host and hidden
image can be maintained.  These transformation range from simple image
manipulations to sophisticated machine learning-based adversaries.
Two extensions to the basic system are presented that mitigate the
possibility of discovering the content of the hidden image.  With
these extensions, not only can the hidden information be kept secure,
but the system can be used to hide even more than a single image.
Applications for this technology include image authentication,
digital watermarks, finding exact regions of image manipulation, and
storing meta-information about image rendering and content.",['Shumeet Baluja'],Google,IEEE Transactions on Pattern Analysis and Machine Intelligence (2019),
Human-Centered Tools for Coping with Imperfect Algorithms during Medical Decision-Making,"Machine learning (ML) is increasingly being used in image retrieval systems for medical decision making. One application of ML is to retrieve visually similar medical images from past patients (e.g. tissue from biopsies) to reference when making a medical decision with a new patient. However, no algorithm can perfectly capture an expert's ideal notion of similarity for every case: an image that is algorithmically determined to be similar may not be medically relevant to a doctor's specific diagnostic needs. In this paper, we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm, and developed tools that empower users to cope with the search algorithm on-the-fly, communicating what types of similarity are most important at different moments in time. In two evaluations with pathologists, we found that these refinement tools increased the diagnostic utility of images found and increased user trust in the algorithm. The tools were preferred over a traditional interface, without a loss in diagnostic accuracy. We also observed that users adopted new strategies when using refinement tools, re-purposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors. Taken together, these findings inform future human-ML collaborative systems for expert decision-making.","['Carrie Jun Cai', 'Emily Reif', 'Narayan G Hegde', 'Been Kim', 'Daniel Smilkov', 'Martin Wattenberg', 'Fernanda Viégas', 'Greg Corrado']",Google,Conference on Human Factors in Computing Systems (2019),
Identifying and Correcting Label Bias in Machine Learning,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained
on such datasets can inherit these biases. In this
paper, we provide a mathematical formulation of
how this bias can arise. We do so by assuming
the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who
intends to provide accurate labels but may have
biases against certain groups. Despite the fact that
we only observe the biased labels, we are able to
show that the bias may nevertheless be corrected
by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset
corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine
learning classifier. Our procedure is fast and robust and can be used with virtually any learning
algorithm. We evaluate on a number of standard
machine learning fairness datasets and a variety
of fairness notions, finding that our method outperforms standard approaches in achieving fair
classification.",['Ofir Nachum'],Google,arxiv (2019),
Interpreting Social Respect:  A Normative Lens for ML Models,"Machine learning is often viewed as an inherently value-neutral process:
statistical tendencies in the training inputs are simply''
used to generalize to new examples. However when models impact social
systems such as interactions between humans, these patterns learned by models
have normative implications. It is important that we ask not onlywhat
patterns exist in the data?'', but also ``how do we want our system 
to impact people?'' In particular, because minority and marginalized
members of society are often statistically underrepresented in data sets, models
may have undesirable disparate impact on such groups. As such, objectives of
social equity and distributive justice require that we develop tools for both
identifying and interpreting harms introduced by models.",['Ben Hutchinson'],Google,(2019) (2019),
Investigating the Effects of Gender Bias on GitHub,"Diversity, including gender diversity, is valued by many software development organizations, yet the field remains dominated by men. One reason for this lack of diversity is gender bias. In this paper, we study the effects of that bias by using an existing framework derived from the gender studies literature. We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub, then evaluate those hypotheses quantitatively. While our results show that effects of gender bias are largely invisible on the GitHub platform itself, there are still signals of women concentrating their work in fewer places and being more restrained in communication than men.",['Emerson Murphy-Hill'],Google,Proceedings of the 2019 International Conference on Software Engineering,
"Matroids, Matchings, and Fairness","The desire to use machine learning to assist in human decision making has spawned a large area of research in understanding the impact of such systems not only on the society as a whole, but also the specific impact on different subpopulations. Recent work has shown that  while there are several natural ways to quantify the fairness of a particular system, none of them are universal, and except for trivial cases, satisfying one means violating another~\citet&lbrace;Kleinberg, Goel, Kleinberg2&rbrace;.  ","['Ravi Kumar', 'Silvio Lattanzi', 'Sergei Vassilvitskii']",Google,AISTATS 2019 (2019),
Metric-Optimized Example Weights,"Real-world machine learning applications often have complex test metrics, and may have training and test data that are not identically distributed. Motivated by known connections between complex test metrics and cost-weighted learning, we propose addressing these issues by using a weighted loss function with a standard loss, where the weights on the training examples are learned to optimize the test metric on a validation set. These metric-optimized example weights can be learned for any test metric, including black box and customized ones for specific applications. We illustrate the performance of the proposed method on diverse public benchmark datasets and real-world applications. We also provide a generalization bound for the method.","['Sen Zhao', 'Harikrishna Narasimhan']",Google,Proceedings of the 36th International Conference on Machine Learning (2019),
Model Cards for Model Reporting,"Trained machine learning models are increasingly used to perform high impact tasks such as determining crime recidivism rates and predicting health risks. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts they are not well-suited for, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards (or M-cards) to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic subgroups (e.g., race, geographic location, sex, Fitzpatrick skin tone) and intersectional subgroups (e.g., age and race, or sex and Fitzpatrick skin tone) that are relevant to the intended application domains. Model cards also disclose the context under which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for models trained to detect smiling faces on the CelebA dataset (Liu et al., 2015) and models trained to detect toxicity in the Conversation AI dataset (Dixon et al., 2018). We propose this work as a step towards the responsible democratization of machine learning and related AI technology, providing context around machine learning models and increasing the transparency into how well such models work.  We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed documentation.","['Andrew Zaldivar', 'Ben Hutchinson', 'Lucy Vasserman']",Google,(2019) (2019),
Optimal Noise-Adding Mechanism in Additive Differential Privacy,"We derive the optimal $(0, \delta)$-differentially private query-output independent noise-adding mechanism for single real-valued query function under a general cost-minimization framework. Under a mild technical condition, we show that the optimal noise probability distribution is a uniform distribution with a probability mass at the origin. We explicitly derive the optimal noise distribution for general $\ell^p$ cost functions, including $\ell^1$ (for noise magnitude) and $\ell^2$ (for noise power) cost functions, and show that the probability concentration on the origin occurs when $\delta > \frac&lbrace;p&rbrace;&lbrace;p+1&rbrace;$. Our result demonstrates an improvement over the existing Gaussian mechanisms  by a factor of two and three for $(0,\delta)$-differential privacy in the high privacy regime in the context of minimizing the noise magnitude and noise power, and the gain is more pronounced in the low privacy regime. Our result is consistent with the existing result for $(0,\delta)$-differential privacy in the discrete setting, and identifies a probability concentration phenomenon in the continuous setting.","['Wei Ding', 'Ruiqi Guo', 'Sanjiv Kumar']",Google,Proceedings of the 22th International Conference on Artificial Intelligence and Statistics (AISTATS) (2019),
"Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals","Machine learning models often need to satisfy many real-world policy goals and capture some
kinds of side information. We show that many such goals can be mathematically expressed as
constraints on the modelâs predictions on the data, which we call rate constraints. In this paper, we
study the specific problem of training non-convex models subject to these rate constraints (which
are non-convex and non-differentiable), and the general problem of constrained optimization of
possibly non-convex objectives with possibly non-convex and non-differentiable constraints. In the
non-convex setting, the Lagrangian may not have an equilibrium to converge to, and thus using the
standard approach of Lagrange multipliers may fail as a deterministic solution may not even exist.
Furthermore, if the constraints are non-differentiable, then one cannot optimize the Lagrangian
with gradient-based methods by definition. To solve these issues, we present the proxy-Lagrangian,
which leads to an algorithm that produces a stochastic classifier with theoretical guarantees by
playing a two-player non-zero-sum game. The first player minimizes external regret in terms of a
differentiable relaxation of the constraints and the second player enforces the original constraints
by minimizing swap-regret: this leads to finding a solution concept which we call a semi-coarse
correlated equilibrium which we interestingly show corresponds to an approximately optimal and
feasible solution to the constrained optimization problem. We then give a procedure which shrinks
the randomized solution down to one that is a mixture of at most m + 1 deterministic solutions.
This culminates into end-to-end procedures which can provably solve non-convex constrained
optimization problems with possibly non-differentiable and non-convex constraints. We provide
extensive experimental results on benchmark and real-world problems, enforcing a broad range of
policy goals including different fairness metrics, and other goals on accuracy, coverage, recall, and
churn.","['Andrew Cotter', 'Serena Wang']",Google,Journal of Machine Learning Research (2019),
Pareto-Efficient Fairness for Skewed Subgroup Data,"As awareness of the potential for learned models to amplify existing societal biases increases,
the field of ML fairness has developed mitigation techniques. A prevalent method applies constraints, including equality of performance, with respect to subgroups defined over the intersection of sensitive attributes such as race and gender. Enforcing such constraints when the subgroup populations are considerably skewed with respect to a target can lead to unintentional degradation in performance, without benefiting any individual subgroup, counter to the United Nations Sustainable Development goals of reducing inequalities and promoting growth. In order to avoid such performance degradation while ensuring equitable treatment to all groups, we propose Pareto-Efficient Fairness (PEF), which identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane. Specifically, PEF finds a Pareto Optimal point which maximizes multiple subgroup
accuracy measures. The algorithm scalarizes using the adaptive weighted metric norm by iteratively searching the Pareto region of all models enforcing the fairness constraint. PEF is backed
by strong theoretical results on discoverability and provides domain practitioners finer control in
navigating both convex and non-convex accuracyfairness trade-offs. Empirically, we show that PEF
increases performance of all subgroups in skewed synthetic data and UCI datasets.","['Alyssa Whitlock Lees', 'Chris Welty']",Google,AISG (2019),
Perturbation Sensitivity Analysis to Detect Unintended Model Biases,"Data-driven statistical Natural Language Processing (NLP) techniques leverage large amounts of language data to build models that can understand language. However, most language data reflect the public discourse at the time the data was produced, and hence NLP models are susceptible to learning incidental associations around named referents at a particular point in time, in addition to general linguistic meaning. An NLP system designed to model notions such as sentiment and toxicity should ideally produce scores that are independent of the identity of such entities mentioned in text and their social associations. For example, in a general purpose sentiment analysis system, a phrase such as I hate Katy Perry should be interpreted as having the same sentiment as I hate Taylor Swift. Based on this idea, we propose a generic evaluation framework, Perturbation Sensitivity Analysis, which detects unintended model biases related to named entities, and requires no new annotations or corpora. We demonstrate the utility of this analysis by employing it on two different NLP models --- a sentiment model and a toxicity model --- applied on online comments in English language from four different genres.","['Vinodkumar Prabhakaran', 'Ben Hutchinson']",Google,EMNLP 2019 (2019),
"Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements","As more researchers have become aware of and passionate about algorithmic fairness, there has been an explosion in papers laying out new metrics, suggesting algorithms to address issues, and calling attention to issues in existing applications of machine learning. This research has greatly expanded our understanding of the concerns and challenges in deploying machine learning, but there has been much less work in seeing how the rubber meets the road. ","['Alex Beutel', 'Ed H. Chi']",Google,"AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) (2019)",
Shape Constraints for Set Functions,"Set functions predict a label from a permutation-invariant variable-size collection of feature vectors. We propose making set functions more understandable and regularized by capturing domain knowledge through shape constraints. We show how prior work in monotonic constraints can be adapted to set functions. Then we propose two new shape constraints designed to generalize the conditioning role of weights in a weighted mean. We show how one can train standard functions and set functions that satisfy these shape constraints with a deep lattice network. We propose a non-linear estimation strategy we call the semantic feature engine that uses set functions with the proposed shape constraints to estimate labels for compound sparse categorical features. Experiments on real-world data show the achieved accuracy is similar to deep sets or deep neural networks, but provides guarantees of the model behavior and is thus easier to explain and debug.","['Andrew Cotter', 'Serena Wang']",Google,International Conference on Machine Learning (2019),
The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks,"This paper describes a testing methodology for quantitatively assessing the risk of \emph&lbrace;unintended memorization&rbrace; of rare or unique sequences in generative sequence models---a common type of neural network. Such models are sometimes trained on sensitive data (e.g., the text of users' private messages); our methodology allows deep-learning to choose configurations that minimize memorization during training, thereby  benefiting privacy.",['Nicholas Carlini'],Google,USENIX Security (2019),
Tough Times at Transitional Homeless Shelters: Considering the Impact of Financial Insecurity on Digital Security and Privacy,"Addressing digital security and privacy issues can be particularly difficult for users who face challenging circumstances. We performed semi-structured interviews with residents and staff at 4 transitional homeless shelters in the U.S. San Francisco Bay Area (n=15 residents, 3 staff) to explore their digital security and privacy challenges. Based on these interviews, we outline four tough times themes -- challenges experienced by our financially insecure participants that impacted their digital security and privacy -- which included: (1) limited financial resources, (2) limited access to reliable devices and Internet, (3) untrusted relationships, and (4) ongoing stress. We provide examples of how each theme impacts digital security and privacy practices and needs. We then use these themes to provide a framework outlining opportunities for technology creators to better support users facing security and privacy challenges related to financial insecurity.","['Manya Sleeper', ""Kathleen O'Leary"", 'Anna Turner', 'Jill Palzkill Woelfer', 'Sunny Consolvo']",Google,Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,
Toward a better trade-off between performance and fairness with kernel-based distribution matching,"As recent literature has demonstrated how classifiers often carry unintended biases toward some subgroups, deploying machine learned models to users demands careful consideration of the social consequences. How should we address this problem in a real-world system? How should we balance core performance and fairness metrics? In this paper, we introduce a MinDiff framework for regularizing classifiers toward different fairness metrics and analyze a technique with kernel-based statistical dependency tests. We run a thorough study on an academic dataset to compare the Pareto frontier achieved by different regularization approaches, and apply our kernel-based method to two large-scale industrial systems demonstrating real-world improvements.","['Flavien Prost', 'Ed H. Chi', 'Alex Beutel']",Google,Neurips (2019),
Towards Automatic Concept-based Explanations,"Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are salient for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \emph&lbrace;concept&rbrace; based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent and salient for the neural network's predictions.","['James Wexler', 'Been Kim']",Google,NeurIPS (2019),
Towards Equitable AI for the Next Billion Users,"In this article, we present research provocations for AI in the Global South, to spur a conversation on the implicit beliefs, biases, and issues that may be normalized in AI. As much of AIâs functioning is still not well understood or fully developed, we believe these critical areas for research are crucial to shaping inclusive AI as it becomes more complex and powerful. We bring our perspectives as HCI researchers and social scientists that work closely with AI researchers. We have started to address some of these areas in our research and invite further explorations from the research community.","['Nithya Sambasivan', 'Jess Scon Holbrook']",Google,ACM interactions (2019),
Towards Federated Learning at Scale: System Design,"Federated Learning is a distributed machine learning approach which enables training on a large corpus of data which never needs to leave user devices. We have spent some effort over the last two years building a scalable production system for FL. In this paper, we report about the resulting high-level design, sketching the challenges and the solutions, as well as touching the open problems and future directions.","['K. A. Bonawitz', 'Alex Ingerman', 'Jakub Konečný', 'Stefano Mazzocchi', 'Brendan McMahan', 'Daniel Ramage']",Google,SysML 2019,
Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints,"Classifiers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other policy goals. We study the generalization performance for such constrained optimization problems, in terms of how well the constraints are satisfied at evaluation time, given that they are satisfied at training time. To improve generalization, we frame the problem as a two-player game where one player optimizes the model parameters on a training dataset, and the other player enforces the constraints on an independent validation dataset. We build on recent work in two-player constrained optimization to show that if one uses this two-dataset approach, then constraint generalization can be significantly improved. As we illustrate experimentally, this approach works not only in theory, but also in practice.","['Andrew Cotter', 'Serena Wang']",Google,International Conference on Machine Learning (2019),
Transfer of Machine Learning Fairness across Domains,"If our models are used in new or unexpected cases, do we know if they will make fair predictions? Previously, researchers developed ways to debias a model for a single problem domain. However, this is often not how models are trained and used in practice. For example, labels and demographics (sensitive attributes) are often hard to observe, resulting in auxiliary or synthetic data to be used for training, and proxies of the sensitive attribute to be used for evaluation of fairness. A model trained for one setting may be picked up and used in many others, particularly as is common with pre-training and cloud APIs. Despite the pervasiveness of these complexities, remarkably little work in the fairness literature has theoretically examined these issues. We frame all of these settings as domain adaptation problems: how can we use what we have learned in a source domain to debias in a new target domain, without directly debiasing on the target domain as if it is a completely new problem? We offer new theoretical guarantees of improving fairness across domains, and offer a modeling approach to transfer to data-sparse target domains. We give empirical results validating the theory and showing that these modeling approaches can improve fairness metrics with less data.","['Xuezhi Wang', 'Alex Beutel', 'Ed H. Chi']",Google,(2019) (2019),
"Transparent, Scrutable and Explainable User Models for Personalized Recommendation","Most recommender systems base their recommendations on implicit or explicit item-level feedback provided by users. These item ratings are combined into a complex user model, which then predicts the suitability of other items. While effective, such methods have limited scrutability and transparency. For instance, if a user's interests change, then many item ratings would usually need to be modified to significantly shift the user's recommendations. Similarly, explaining how the system characterizes the user is impossible, short of presenting the entire list of known item ratings. In this paper, we present a new set-based recommendation technique that permits the user model to be explicitly presented to users in natural language, empowering users to understand recommendations made and improve the recommendations dynamically. While performing comparably to traditional collaborative filtering techniques in a standard static setting, our approach allows users to efficiently improve recommendations. Further, it makes it easier for the model to be validated and adjusted, building user trust and understanding.",['Filip Radlinski'],Google,Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19) (2019),
Understanding and correcting pathologies in the training of learned optimizers,"Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned optimizers may similarly outperform current hand-designed optimizers, especially for specific problems. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process. The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance. This allows us to train neural networks to perform optimization of a specific task faster than tuned first-order methods. Moreover, by training the optimizer against validation loss (as opposed to training loss), we are able to learn optimizers that train networks to generalize better than first order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks faster in wall-clock time compared to tuned first-order methods and with an improvement in test loss.","['Luke Metz', 'Niru Maheswaranathan', 'Daniel Freeman', 'Jascha Sohl-dickstein']",Google,ICML (2019),
What is Fair?  Exploring Pareto-Efficiency for Fairness Constraint Classifiers,"The potential for learned models to amplify existing societal biases has been broadly recognized. Fairness-aware classifier constraints, which apply equality metrics of performance across subgroups defined on sensitive attributes such as race and gender, seek to rectify inequity but can yield non-uniform degradation in performance for skewed datasets.  In certain domains, imbalanced degradation of performance can yield another form of unintentional bias. In the spirit of constructing fairness-aware algorithms as societal imperative, we explore an alternative: Pareto-Efficient Fairness (PEF).  PEF identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane, maximizing multiple subgroup accuracies. Empirically we demonstrate that PEF increases performance of all subgroups in several UCI datasets.","['Alyssa Whitlock Lees', 'Chris Welty']",Google,arxiv (2019),
A General Approach to Adding Differential Privacy to Iterative Training Procedures,"In this work we address the practical challenges of training machine learning models on privacy-sensitive datasets by introducing a modular approach that minimizes changes to training algorithms, provides a variety of configuration strategies for the privacy mechanism, and then isolates and simplifies the critical logic that computes the final privacy guarantees. A key challenge is that training algorithms often require estimating many different quantities (vectors) from the same set of examples --- for example, gradients of different layers in a deep learning architecture, as well as metrics and batch normalization parameters. Each of these may have different properties like dimensionality, magnitude, and tolerance to noise. By extending previous work on the Moments Accountant for the subsampled Gaussian mechanism, we can provide privacy for such heterogeneous sets of vectors, while also structuring the approach to minimize software engineering challenges.","['Brendan McMahan', 'Nicolas Papernot', 'Peter Kairouz']",Google,NIPS (2018),
A Qualitative Exploration of Perceptions of Algorithmic Fairness,"Algorithmic systems increasingly shape information people are exposed to as well as influence decisions about employment, finances, and other opportunities. In some cases, algorithmic systems may be more or less favorable to certain groups or individuals, sparking substantial discussion of algorithmic fairness in public policy circles, academia, and the press. We broaden this discussion by exploring how members of potentially affected communities feel about algorithmic fairness. We conducted workshops and interviews with 44 participants from several populations traditionally marginalized by categories of race or class in the United States. While the concept of algorithmic fairness was largely unfamiliar, learning about algorithmic (un)fairness elicited negative feelings that connect to current national discussion about racial injustice and economic inequality. In addition to their concerns about potential harms to themselves and society, participants also indicated that algorithmic fairness (or lack thereof) could substantially affect their trust in a company or product.",[],Google,CHI 2018 (2018),
Adversarial Examples as an Input-Fault Tolerance Problem,"We analyze the adversarial examples problem in terms of a modelâs fault tolerance
with respect to its input. Whereas previous work focuses on arbitrarily strict threat
models, i.e., -perturbations, we consider arbitrary valid inputs and propose an
information-based characteristic for evaluating tolerance to diverse input faults.",[],Google,NeurIPS Workshop on Security in Machine Learning (2018),
Adversarial Spheres,"State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size O(1/sqrt(d)). Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.","['Luke Metz', 'Maithra Raghu', 'Martin Wattenberg']",Google,ICLR Workshop (2018),
Adversarially Robust Generalization Requires More Data,"Machine learning models are often susceptible to adversarial perturbations of their inputs.
Even small perturbations can cause state-of-the-art classifiers with high âstandardâ accuracy to
produce an incorrect prediction with high confidence. To better understand this phenomenon, we
study adversarially robust learning from the viewpoint of generalization. We show that already
in a simple natural data model, the sample complexity of robust learning can be significantly
larger than that of âstandardâ learning. This gap is information theoretic and holds irrespective
of the training algorithm or the model family. We complement our theoretical results with
experiments on popular image classification datasets and show that a similar gap exists here as
well. We postulate that the difficulty of training robust classifiers stems, at least partially, from
this inherently larger sample complexity.",[],Google,NeurIPS (Spotlight) (2018),
Anatomy of a Privacy-Safe Large-Scale Information Extraction System Over Email,"Extracting structured data from emails can enable several assistive experiences, such as reminding the user when a bill payment is due, answering queries about the departure time of a booked flight, or proactively surfacing an emailed discount coupon while the user is at that store.
This paper presents Juicer, a system for extracting information from email that is serving over a billion Gmail users daily. We describe how the design of the system was informed by three key principles: scaling to a planet-wide email service, isolating the complexity to provide a simple experience for the developer, and safeguarding the privacy of users (our team and the developers we support are not allowed to view any single email). We describe the design tradeoffs made in building this system, the challenges faced and the approaches used to tackle them. We present case studies of three extraction tasks implemented on this platformâbill reminders, commercial offers, and hotel reservationsâto illustrate the effectiveness of the platform despite challenges unique to each task. Finally, we outline several areas of ongoing research in large-scale machine-learned information extraction from email.","['Ying Sheng', 'Sandeep Tata', 'James B. Wendt', 'Jing Xie', 'Qi Zhao', 'Marc Najork']",Google,24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2018),
Diminishing Returns Shape Constraints for Interpretability and Regularization,"We investigate machine learning models that can provide diminishing returns
and accelerating returns guarantees to capture prior knowledge or policies
about how outputs should depend on inputs. We show that one can build
flexible, nonlinear, multi-dimensional models using lattice functions with any
combination of concavity/convexity and monotonicity constraints on any
subsets of features, and compare to new shape-constrained neural networks.
We demonstrate on real-world examples that these shape constrained models
can provide tuning-free regularization and improve model understandability.","['Dara Bahri', 'Andy Cotter', 'Kevin Canini']",Google,NIPS 2018 (2018),
Ensuring Fairness in Machine Learning to Advance Health Equity,"A central promise of machine learning (ML) is to use historical data to project the future trajectories of patients.  Will they have a good or bad outcome? What diagnoses will they have? What treatments should they be given?  But in many cases, we do not want the future to look like the past, especially when the past contains patterns of human or structural biases against vulnerable populations.","['Alvin Rishi Rajkomar', 'Greg Corrado', 'Michael Howell']",Google,Annals of Internal Medicine (2018),
Failure Modes of Variational Inference for Decision Making,"In this paper we highlight the risks of relying on
mean-field variational inference to learn models
that are used as simulators for decision making.
We study the role of accurate inference for latent
variable models in terms of cumulative reward
performance. We show how naive mean-field
variational inference at test time can lead to poor
decisions in basic but fundamental quadratic control problems with continuous actions, as relevant
correlations in the latent space are ignored. We
then extend these examples to a more complex
non-linear scenario with asymmetric costs, where
regret is even more significant.","['Carlos Riquelme', 'Matt Hoffman']",Google,ICML Workshop (2018),
Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy,"Purpose
Use adjudication to quantify errors in diabetic retinopathy (DR) grading based on individual graders and majority decision, and to train an improved automated algorithm for DR grading.","['Jonathan Krause', 'Varun Gulshan', 'Kasumi Widner', 'Greg Corrado', 'Lily Peng', 'Dale Webster']",Google,Ophthalmology (2018),
Human-in-the-Loop Interpretability Prior,"We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.",['Been Kim'],Google,NeurIPS (Spotlight) (2018),
Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV),"The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of âzebraâ is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.","['Been Kim', 'Martin Wattenberg', 'Carrie Jun Cai', 'James Wexler', 'Fernanda Viegas', 'Rory Abbott Sayres']",Google,ICML (2018),
Intriguing Properties of Adversarial Examples,"It is becoming increasingly clear that many machine learning classifiers are vulnerable
to adversarial examples. In attempting to explain the origin of adversarial
examples, previous studies have typically focused on the fact that neural networks
operate on high dimensional data, they overfit, or they are too linear. Here we
argue that the origin of adversarial examples is primarily due to an inherent uncertainty
that neural networks have about their predictions. We show that the functional
form of this uncertainty is independent of architecture, dataset, and training
protocol; and depends only on the statistics of the logit differences of the network,
which do not change significantly during training. This leads to adversarial error
having a universal scaling, as a power-law, with respect to the size of the adversarial
perturbation. We show that this universality holds for a broad range of datasets
(MNIST, CIFAR10, ImageNet, and random data), models (including state-of-theart
deep networks, linear models, adversarially trained networks, and networks
trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated
by these results, we study the effects of reducing prediction entropy on
adversarial robustness. Finally, we study the effect of network architectures on
adversarial sensitivity. To do this, we use neural architecture search with reinforcement
learning to find adversarially robust architectures on CIFAR10. Our
resulting architecture is more robust to white and black box attacks compared to
previous attempts.","['Ekin Dogus Cubuk', 'Quoc V. Le']",Google,ICLR (2018),
Learning Differentially Private Recurrent Language Models,"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes ""large step"" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","['Brendan McMahan', 'Daniel Ramage', 'Li Zhang']",Google,International Conference on Learning Representations (ICLR) (2018),
Learning how to explain neural networks: PatternNet and PatternAttribution,"DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of  simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.","['Pieter-jan Kindermans', 'Dumitru Erhan', 'Been Kim']",Google,ICLR (2018),
Learning to Attack: Adversarial Transformation Networks,"With the rapidly increasing popularity of deep neural networks
for image recognition tasks, a parallel interest in generating
adversarial examples to attack the trained models has
arisen. To date, these approaches have involved either directly
computing gradients with respect to the image pixels or directly
solving an optimization on the image pixels. We generalize
this pursuit in a novel direction: can a separate network
be trained to efficiently attack another fully trained network?
We demonstrate that it is possible, and that the generated
attacks yield startling insights into the weaknesses of
the target network. We call such a network an Adversarial
Transformation Network (ATN). ATNs transform any input
into an adversarial attack on the target network, while being
minimally perturbing to the original inputs and the target networkâs
outputs. Further, we show that ATNs are capable of
not only causing the target network to make an error, but can
be constructed to explicitly control the type of misclassification
made. We demonstrate ATNs on both simple MNIST digit
classifiers and state-of-the-art ImageNet classifiers deployed
by Google, Inc.: Inception ResNet-v2.","['Shumeet Baluja', 'Ian Fischer']",Google,Proceedings of AAAI-2018,
Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning,"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states.","['Shane Gu', 'Julian Ibarz', 'Sergey Levine']",Google,ICLR (2018),
Measuring and Mitigating Unintended Bias in Text Classification,"We introduce and illustrate a new approach to measuring and
mitigating unintended bias in machine learning models. Our
definition of unintended bias is parameterized by a test set
and a subset of input features. We illustrate how this can
be used to evaluate text classifiers using a synthetic test set
and a public corpus of comments annotated for toxicity from
Wikipedia Talk pages. We also demonstrate how imbalances
in training data can lead to unintended bias in the resulting
models, and therefore potentially unfair applications. We use
a set of common demographic identity terms as the subset of
input features on which we measure bias. This technique permits
analysis in the common scenario where demographic information
on authors and readers is unavailable, so that bias
mitigation must focus on the content of the text itself. The
mitigation method we introduce is an unsupervised approach
based on balancing the training dataset. We demonstrate that
this approach reduces the unintended bias without compromising
overall model quality","['Lucas Dixon', 'Jeffrey Sorensen', 'Nithum Thain', 'Lucy Vasserman']",Google,"AAAI/ACM Conference on AI, Ethics, and Society (2018)",
Mitigating Unwanted Biases with Adversarial Learning,"Machine learning can be used to train a model that accurately represents
the data on which it is trained. The most common loss functions
minimized by gradient descent involve accuracy. However,
modeling the training data optimally requires accurately modeling
any undesirable biases present in that training data. One task
which easily demonstrates this phenomenon is word embeddings
learned from standard corpora. When such word embeddings are
used to perform tasks like analogy completion, the bias in the word
embeddings propagates to the predicted analogy completions. Ideally
we would like to remove the biased information which might
impact task performance while retaining as much other semantic
information as possible. We present here a method for debiasing
networks using an adversary. First we formalize this problem by
describing the nature of the input to our network X, describing
the prediction which is desired Y and the protected variable Z. The
objective then becomes to maximize the primary networkâs ability
to predict Y while minimizing the adversaryâs ability to use that
prediction to predict Z. When applied to analogy completion this
method results in embeddings which are still quite useful for performing
analogy completion but without producing predictions
impacted by bias prediction. When applied to a categorization task
such as the one in the UCI Adult Dataset it results in a predictive
model that maintains accuracy while ensuring equality of odds.
This method is quite flexible and is applicable to any problem set
which is expressible as a model which predicts a label Y using an
input X while trying to be fair with respect to a protected variable
Z.",['Blake Lemoine'],Google,(2018) (2018),
Motivating the Rules of the Game for Adversarial Example Research,"Advances in machine learning have led to broad deployment of systems with impressive
performance on important problems. Nonetheless, these systems can be induced
to make errors on data that are surprisingly similar to examples the learned system
handles correctly. The existence of these errors raises a variety of questions about
out-of-sample generalization and whether bad actors might use such examples to abuse
deployed systems. As a result of these security concerns, there has been a flurry of
recent papers proposing algorithms to defend against such malicious perturbations of
correctly handled examples. It is unclear how such misclassifications represent a different
kind of security problem than other errors, or even other attacker-produced
examples that have no specific relationship to an uncorrupted input. In this paper,
we argue that adversarial example defense papers have, to date, mostly considered
abstract, toy games that do not relate to any specific security concern. Furthermore,
defense papers have not yet precisely described all the abilities and limitations of attackers
that would be relevant in practical security. Towards this end, we establish a
taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally,
we provide a series of recommendations outlining a path forward for future work
to more clearly articulate the threat model and perform more meaningful evaluation.",['George E. Dahl'],Google,arxiv (2018),
Multi-Task Learning for Personal Search Ranking with Query Clustering,"User needs vary significantly across different tasks, and therefore
their queries will also vary significantly in their expressiveness
and semantics. Many studies have been proposed
to model such query diversity by obtaining query types and
building query-dependent ranking models. To obtain query
types, these studies typically require either a labeled query
dataset or clicks from multiple users aggregated over the
same document. These techniques, however, are not applicable
when manual query labeling is not viable, and aggregated
clicks are unavailable due to the private nature of the document
collection, e.g., in personal search scenarios. Therefore,
in this paper, we study the problem of how to obtain query
type in an unsupervised fashion and how to leverage this information
using query-dependent ranking models in personal
search. We first develop a hierarchical clustering algorithm
based on truncated SVD and varimax rotation to obtain
coarse-to-fine query types. Then, we propose three query-dependent
ranking models, including two neural models that
leverage query type information as additional features, and
one novel multi-task neural model that is trained to simultaneously
rank documents and predict query types. We evaluate
our ranking models using the click data collected from one of
the worldâs largest personal search engines. The experiments
demonstrate that the proposed multi-task model can significantly
outperform the baseline neural models, which either
do not incorporate query type information or just simply
feed query type as an additional feature. To the best of our
knowledge, this is the first successful application of query-dependent
multi-task learning in personal search ranking.","['Maryam Karimzadehgan', 'Michael Bendersky', 'Zhen Qin', 'Don Metzler']",Google,Proceedings of ACM Conference on Information and Knowledge Management (CIKM) (2018),
Playing the Game of Universal Adversarial Perturbations,"We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.
By observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely \fp,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.","['Mateusz Malinowski', 'Olivier Pietquin']",Google,(2018) (2018),
Privacy Amplification by Iteration,"Most commonly used learning algorithms work by iteratively updating an intermediate solution using one or a few data points in each iteration.  Analysis of differential privacy for such algorithms often involves ensuring privacy of each step and then reasoning about the cumulative privacy cost of the algorithm. This is enabled by composition theorems for differential privacy that allow releasing of all the intermediate results.  In this work, we demonstrate that for contractive iterations, not releasing the intermediate results strongly amplifies the privacy guarantees.
",[],Google,2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS),
Privacy in Geospatial Applications and Location-Based Social Networks,"The use of location data has greatly benefited from the availability of location-based services, the popularity of social networks, and the accessibility of public location data sets. However, in addition to providing users with the ability to obtain accurate driving directions or the convenience of geo-tagging friends and pictures, location is also a very sensitive type of data, as attested by more than a decade of research on different aspects of privacy related to location data.",['Igor Bilogrevic'],Google,Handbook of Mobile Data Privacy (2018),
Privacy-preserving Prediction,"Ensuring differential privacy of models learned from sensitive user data is an important goal that has been studied extensively in recent years.
It is now known that for some basic learning problems, especially those involving high-dimensional data, producing an accurate private model requires much more data than learning without privacy. At the same time, in many applications it is not necessary to expose the model itself. Instead users may be allowed to query the prediction model on their inputs only through an appropriate interface. Here we formulate the problem of ensuring privacy of individual predictions and investigate the overheads required to achieve it in several standard models of classification and regression.",[],Google,Conference on Learning Theory (COLT) 2018,
Risk-Sensitive Generative Adversarial Imitation Learning,"We study risk-sensitive imitation learning where the agent's goal is to perform at least as well as the expert in terms of a risk profile. We first formulate our risk-sensitive imitation learning setting. We consider the generative adversarial approach to imitation learning (GAIL) and derive an optimization problem for our formulation, which we call it risk-sensitive GAIL (RS-GAIL). We then derive two different versions of our RS-GAIL optimization problem that aim at matching the risk profiles of the agent and the expert w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop risk-sensitive generative adversarial imitation learning algorithms based on these optimization problems. We evaluate the performance of our algorithms and compare them with GAIL and the risk-averse imitation learning (RAIL) algorithms in two MuJoCo and two OpenAI classical control tasks.",['Yinlam Chow'],Google,AISTATS (2018),
Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints,"Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation. We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against \textit&lbrace;any&rbrace; number of adversarial deletions. We extensively evaluate the performance of our algorithms against prior state-of-the-art on real-world applications, including (i) Uber-pick up locations with location privacy constraints; (ii) feature selection with fairness constraints for income prediction and crime rate prediction; and (iii) robust to deletion summarization of census data, consisting of 2,458,285 feature vectors.",['Morteza Zadimoghaddam'],Google,"Thirty-fifth International Conference on Machine Learning, ICML 2018",
Scalable Private Learning with PATE,"The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a ""student"" model the knowledge of an ensemble of ""teacher"" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachersâ answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets.","['Nicolas Papernot', 'Shuang Song']",Google,International Conference on Learning Representations (ICLR) (2018),
Text Embeddings Contain Bias. Here's Why That Matters.,"With the public release of embedding models, itâs important to understand the various biases that they contain. Developers who use them should be aware of the biases inherent in the models as well as how biases can manifest in downstream applications that use these models. In this post, we examine a few specific forms of bias and suggest tools for evaluating as well as mitigating bias.",['Mario Guajardo-Céspedes'],Google,(2018) (2018),
The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks,"This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization.",[],Google,ArXiv e-prints (2018),
To Trust Or Not To Trust A Classifier,"Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the &lbrace;\it trust score&rbrace;, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis.",['Been Kim'],Google,NeurIPS (2018),
Training On-Device Ranking Models from Cross-User Interactions in a Privacy-Preserving Fashion,(See the attached PDF -- a one-page abstract for the upcoming DESIRES 2018 workshop),['Marc Najork'],Google,Proc. of the First Biennial Conference on Design of Experimental Search & Information Retrieval Systems (DESIRES) (2018),
Towards A Rigorous Science of Interpretable Machine Learning,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",['Been Kim'],Google,arXiv (2017),
Designing Unbiased Surveys for HCI Research,"Surveys are a commonly used method within HCI research. While it initially appears easy and inexpensive to conduct surveys, overlooking key considerations in questionnaire design and the survey research process can yield skewed, biased, or entirely invalid survey results. Fortunately decades of academic research and analysis exist on optimizing the validity and reliability of survey data, from which this course will draw. To enable the creation of unbiased surveys, this course demonstrates questionnaire design biases and pitfalls, provides best practices for minimizing these, and reviews different uses of surveys within HCI.","['Hendrik Müller', 'Aaron Sedley']",Google,CHI '14 Extended Abstracts on Human Factors in Computing Systems (2014),
WCMP: Weighted Cost Multipathing for Improved Fairness in Data Centers,"Data Center topologies employ multiple paths among servers to deliver scalable, cost-effective network capacity. The simplest and the most widely deployed approach for load balancing among these paths, Equal Cost Multipath (ECMP), hashes flows among the shortest paths toward a destination. ECMP leverages uniform hashing of balanced flow sizes to achieve fairness and good load balancing in data centers. However, we show that ECMP further assumes a balanced, regular, and fault-free topology, which are invalid assumptions in practice that can lead to substantial performance degradation and, worse, variation in flow bandwidths even for same size flows.","['Junlan Zhou', 'Leon Poutievski', 'Amin Vahdat']",Google,EuroSys '14: Proceedings of the Ninth European Conference on Computer Systems (2014),
A practical algorithm for balancing the max-min fairness and throughput objectives in traffic engineering,"One of the goals of traffic engineering is to achieve a
flexible trade-off between fairness and throughput so that users
are satisfied with their bandwidth allocation and the network
operator is satisfied with the utilization of network resources. In
this paper, we propose a novel way to balance the throughput
and fairness objectives with linear programming. It allows the
network operator to precisely control the trade-off by bounding
the fairness degradation for each commodity compared to the
max-min fair solution or the throughput degradation compared
to the optimal throughput. We also present improvements to a
previous algorithm that achieves max-min fairness by solving a
series of linear programs. We significantly reduce the number
of steps needed when the access rate of commodities is limited.
We extend the algorithm to two important practical use cases:
importance weights and piece-wise linear utility functions for
commodities. Our experiments on synthetic and real networks
show that our algorithms achieve a significant speedup and
provide practical insights on the trade-off between fairness and
throughput.",['Subhasree Mandal'],Google,INFOCOM (2012),
Upward Max Min Fairness,No abstract available; check out the Download or Google Scholar links above for publications details.,"['Avinatan Hassidim', 'Haim Kaplan', 'Alok Kumar', 'Danny Raz', 'Michal Segalov']",Google,INFOCOM (2012),

