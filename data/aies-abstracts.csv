title,abstract,authors,institutions,conference,date
Exploring AI Futures Through Role Play,"We present an innovative methodology for studying and teaching the impacts of AI through a role-play game. The game serves two primary purposes: 1) training AI developers and AI policy professionals to reflect on and prepare for future social and ethical challenges related to AI and 2) exploring possible futures involving AI technology development, deployment, social impacts, and governance. While the game currently focuses on the inter-relations between short-, mid- and long-term impacts of AI, it has potential to be adapted for a broad range of scenarios, exploring in greater depths issues of AI policy research and affording training within organizations. The game presented here has undergone two years of development and has been tested through over 30 events involving between 3 and 70 participants. The game is under active development, but preliminary findings suggest that role-play is a promising methodology for both exploring AI futures and training individuals and organizations in thinking about, and reflecting on, the impacts of AI and strategic mistakes that can be avoided today.","['Shahar Avin', 'Ross Gruetzemacher', 'James Fox']","['University of Cambridge, Cambridge, United Kingdom', 'Auburn University, Auburn, AL, USA', 'University of Oxford, Oxford, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Activism by the AI Community: Analysing Recent Achievements and Future Prospects,"The artificial intelligence (AI) community has recently engaged in activism in relation to their employers, other members of the community, and their governments in order to shape the societal and ethical implications of AI. It has achieved some notable successes, but prospects for further political organising and activism are uncertain. We survey activism by the AI community over the last six years; apply two analytical frameworks drawing upon the literature on epistemic communities, and worker organising and bargaining; and explore what they imply for the future prospects of the AI community. Success thus far has hinged on a coherent shared culture, and high bargaining power due to the high demand for a limited supply of AI 'talent'. Both are crucial to the future of AI activism and worthy of sustained attention.",['Haydn Belfield'],"['University of Cambridge, Cambridge, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Fair Allocation through Selective Information Acquisition,"Public and private institutions must often allocate scarce resources under uncertainty. Banks, for example, extend credit to loan applicants based in part on their estimated likelihood of repaying a loan. But when the quality of information differs across candidates (e.g., if some applicants lack traditional credit histories), common lending strategies can lead to disparities across groups. Here we consider a setting in which decision makers---before allocating resources---can choose to spend some of their limited budget further screening select individuals. We present a computationally efficient algorithm for deciding whom to screen that maximizes a standard measure of social welfare. Intuitively, decision makers should screen candidates on the margin, for whom the additional information could plausibly alter the allocation. We formalize this idea by showing the problem can be reduced to solving a series of linear programs. Both on synthetic and real-world datasets, this strategy improves utility, illustrating the value of targeted information acquisition in such decisions. Further, when there is social value for distributing resources to groups for whom we have a priori poor information---like those without credit scores---our approach can substantially improve the allocation of limited assets.","['William Cai', 'Johann Gaebler', 'Nikhil Garg', 'Sharad Goel']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
The Problem with Intelligence: Its Value-Laden History and the Future of AI,"This paper argues that the concept of intelligence is highly value-laden in ways that impact on the field of AI and debates about its risks and opportunities. This value-ladenness stems from the historical use of the concept of intelligence in the legitimation of dominance hierarchies. The paper first provides a brief overview of the history of this usage, looking at the role of intelligence in patriarchy, the logic of colonialism and scientific racism. It then highlights five ways in which this ideological legacy might be interacting with debates about AI and its risks and opportunities: 1) how some aspects of the AI debate perpetuate the fetishization of intelligence; 2) how the fetishization of intelligence impacts on diversity in the technology industry; 3) how certain hopes for AI perpetuate notions of technology and the mastery of nature; 4) how the association of intelligence with the professional class misdirects concerns about AI; and 5) how the equation of intelligence and dominance fosters fears of superintelligence. This paper therefore takes a first step in bringing together the literature on intelligence testing, eugenics and colonialism from a range of disciplines with that on the ethics and societal impact of AI.",['Stephen Cave'],"['University of Cambridge, Cambridge, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Learning Occupational Task-Shares Dynamics for the Future of Work,"The recent wave of AI and automation has been argued to differ from previous General Purpose Technologies (GPTs), in that it may lead to rapid change in occupations' underlying task requirements and persistent technological unemployment. In this paper, we apply a novel methodology of dynamic task shares to a large dataset of online job postings to explore how exactly occupational task demands have changed over the past decade of AI innovation, especially across high, mid and low wage occupations. Notably, big data and AI have risen significantly among high wage occupations since 2012 and 2016, respectively. We built an ARIMA model to predict future occupational task demands and showcase several relevant examples in Healthcare, Administration, and IT. Such task demands predictions across occupations will play a pivotal role in retraining the workforce of the future.","['Subhro Das', 'Sebastian Steffen', 'Wyatt Clarke', 'Prabhat Reddy', 'Erik Brynjolfsson', 'Martin Fleming']","['IBM Research, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'IBM Research, Armonk, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'IBM Research, Armonk, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Social Contracts for Non-Cooperative Games,"In future agent societies, we might see AI systems engaging in selfish, calculated behavior, furthering their owners' interests instead of socially desirable outcomes. How can we promote morally sound behaviour in such settings, in order to obtain more desirable outcomes? A solution from moral philosophy is the concept of a social contract, a set of rules that people would voluntarily commit to in order to obtain better outcomes than those brought by anarchy. We adapt this concept to a game-theoretic setting, to systematically modify the payoffs of a non-cooperative game, so that agents will rationally pursue socially desirable outcomes. We show that for any game, a suitable social contract can be designed to produce an optimal outcome in terms of social welfare. We then investigate the limitations of applying this approach to alternative moral objectives, and establish that, for any alternative moral objective that is significantly different from social welfare, there are games for which no such social contract will be feasible that produces non-negligible social benefit compared to collective selfish behaviour.","['Alan Davoust', 'Michael Rovatsos']","['Université du Québec en Outaouais, Gatineau, PQ, Canada', 'The University of Edinburgh, Edinburgh, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
The AI Liability Puzzle and a Fund-Based Work-Around,"Certainty around the regulatory environment is crucial to facilitate responsible AI innovation and its social acceptance. However, the existing legal liability system is inapt to assign responsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of AI and/or the harms they may trigger are not foreseeable in the legal sense. The unpredictability of how courts would handle such cases makes the risks involved in the investment and use of AI incalculable, creating an environment that is not conducive to innovation and may deprive society of some benefits AI could provide. To tackle this problem, we propose to draw insights from financial regulatory best-practices and establish a system of AI guarantee schemes. We envisage the system to form part of the broader market-structuring regulatory framework, with the primary function to provide a readily available, clear, and transparent funding mechanism to compensate claims that are either extremely hard or impossible to realize via conventional litigation. We propose at least partial industry-funding, with funding arrangements depending on whether it would pursue other potential policy goals.","['Olivia J. Erdélyi', 'Gábor Erdélyi']","['University of Canterbury, Christchurch, New Zealand', 'University of Canterbury, Christchurch, New Zealand']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Algorithmic Fairness from a Non-ideal Perspective,"Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In the hopes of mitigating these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might hope to observe in a fair world, offering a variety of algorithms that attempt to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to fair machine learning to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and ideal worlds. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of their actions, naive applications of ideal thinking can lead to misguided policies. In this paper, we demonstrate a connection between the recent literature on fair machine learning and the ideal approach in political philosophy, and show that some recently uncovered shortcomings in proposed algorithms reflect broader troubles faced by the ideal approach. We work this analysis through for different formulations of fairness and conclude with a critical discussion of real-world impacts and directions for new research.","['Sina Fazelpour', 'Zachary C. Lipton']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Bayesian Sensitivity Analysis for Offline Policy Evaluation,"On a variety of complex decision-making tasks, from doctors prescribing treatment to judges setting bail, machine learning algorithms have been shown to outperform expert human judgments. One complication, however, is that it is often difficult to anticipate the effects of algorithmic policies prior to deployment, as one generally cannot use historical data to directly observe what would have happened had the actions recommended by the algorithm been taken. A common strategy is to model potential outcomes for alternative decisions assuming that there are no unmeasured confounders (i.e., to assume ignorability). But if this ignorability assumption is violated, the predicted and actual effects of an algorithmic policy can diverge sharply. In this paper we present a flexible Bayesian approach to gauge the sensitivity of predicted policy outcomes to unmeasured confounders. In particular, and in contrast to past work, our modeling framework easily enables confounders to vary with the observed covariates. We demonstrate the efficacy of our method on a large dataset of judicial actions, in which one must decide whether defendants awaiting trial should be required to pay bail or can be released without payment.","['Jongbin Jung', 'Ravi Shroff', 'Avi Feller', 'Sharad Goel']","['Stanford University, Stanford, CA, USA', 'New York University, New York, NY, USA', 'University of California, Berkeley, Berkeley, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
"Biased Priorities, Biased Outcomes: Three Recommendations for Ethics-oriented Data Annotation Practices","In this paper, we analyze the relation between data-related biases and practices of data annotation, by placing them in the context of market economy. We understand annotation as a praxis related to the sensemaking of data and investigate annotation practices for vision models by focusing on the values that are prioritized by industrial decision-makers and practitioners. The quality of data is critical for machine learning models as it holds the power to (mis-)represent the population it is intended to analyze. For autonomous systems to be able to make sense of the world, humans first need to make sense of the data these systems will be trained on. This paper addresses this issue, guided by the following research questions: Which goals are prioritized by decision-makers at the data annotation stage? How do these priorities correlate with data-related bias issues? Focusing on work practices and their context, our research goal aims at understanding the logics driving companies and their impact on the performed annotations. The study follows a qualitative design and is based on 24 interviews with relevant actors and extensive participatory observations, including several weeks of fieldwork at two companies dedicated to data annotation for vision models in Buenos Aires, Argentina and Sofia, Bulgaria. The prevalence of market-oriented values over socially responsible approaches is argued based on three corporate priorities that inform work practices in this field and directly shape the annotations performed: profit (short deadlines connected to the strive for profit are prioritized over alternative approaches that could prevent biased outcomes), standardization (the strive for standardized and, in many cases, reductive or biased annotations to make data fit the products and revenue plans of clients), and opacity (related to client's power to impose their criteria on the annotations that are performed. Criteria that most of the times remain opaque due to corporate confidentiality). Finally, we introduce three elements, aiming at developing ethics-oriented practices of data annotation, that could help prevent biased outcomes: transparency (regarding the documentation of data transformations, including information on responsibilities and criteria for decision-making.), education (training on the potential harms caused by AI and its ethical implications, that could help data annotators and related roles adopt a more critical approach towards the interpretation and labeling of data), and regulations (clear guidelines for ethical AI developed at the governmental level and applied both in private and public organizations).","['Gunay Kazimzade', 'Milagros Miceli']","['Technische Universität Berlin, Berlin, Germany', 'Technische Universität Berlin, Berlin, Germany']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Defining AI in Policy versus Practice,"Recent concern about harms of information technologies motivate consideration of regulatory action to forestall or constrain certain developments in the field of artificial intelligence (AI). However, definitional ambiguity hampers the possibility of conversation about this urgent topic of public concern. Legal and regulatory interventions require agreed-upon definitions, but consensus around a definition of AI has been elusive, especially in policy conversations. With an eye towards practical working definitions and a broader understanding of positions on these issues, we survey experts and review published policy documents to examine researcher and policy-maker conceptions of AI. We find that while AI researchers favor definitions of AI that emphasize technical functionality, policy-makers instead use definitions that compare systems to human thinking and behavior. We point out that definitions adhering closely to the functionality of AI systems are more inclusive of technologies in use today, whereas definitions that emphasize human-like capabilities are most applicable to hypothetical future technologies. As a result of this gap, ethical and regulatory efforts may overemphasize concern about future technologies at the expense of pressing issues with existing deployed technologies.","['P. M. Krafft', 'Meg Young', 'Michael Katell', 'Karen Huang', 'Ghislain Bugingo']","['University of Oxford, Oxford, United Kingdom', 'University of Washington, Seattle, WA, USA', 'University of Washington, Seattle, WA, USA', 'Harvard University, Cambridge, MA, USA', 'University of Washington, Seattle, WA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
"""How do I fool you?"": Manipulating User Trust via Misleading Black Box Explanations","As machine learning black boxes are increasingly being deployed in critical domains such as healthcare and criminal justice, there has been a growing emphasis on developing techniques for explaining these black boxes in a human interpretable manner. There has been recent concern that a high-fidelity explanation of a black box ML model may not accurately reflect the biases in the black box. As a consequence, explanations have the potential to mislead human users into trusting a problematic black box. In this work, we rigorously explore the notion of misleading explanations and how they influence user trust in black box models. Specifically, we propose a novel theoretical framework for understanding and generating misleading explanations, and carry out a user study with domain experts to demonstrate how these explanations can be used to mislead users. Our work is the first to empirically establish how user trust in black box models can be manipulated via misleading explanations.","['Himabindu Lakkaraju', 'Osbert Bastani']","['Harvard University, Cambridge, MA, USA', 'University of Pennsylvania, Philadelphia, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Normative Principles for Evaluating Fairness in Machine Learning,"There are many incompatible ways to measure fair outcomes for machine learning algorithms. The goal of this paper is to characterize rates of success and error across protected groups (race, gender, sexual orientation) as a distribution problem, and describe the possible solutions to this problem according to different normative principles from moral and political philosophy. These normative principles are based on various competing attributes within a distribution problem: intentions, compensation, desert, consent, and consequences. Each principle will be applied to a sample risk-assessment classifier to demonstrate the philosophical arguments underlying different sets of fairness metrics.",['Derek Leben'],"['University of Pittsburgh, Johnstown, Pittsburgh, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Good Explanation for Algorithmic Transparency,"Machine learning algorithms have gained widespread usage across a variety of domains, both in providing predictions to expert users and recommending decisions to everyday users. However, these AI systems are often black boxes, and end-users are rarely provided with an explanation. The critical need for explanation by AI systems has led to calls for algorithmic transparency, including the ""right to explanation'' in the EU General Data Protection Regulation (GDPR). These initiatives presuppose that we know what constitutes a meaningful or good explanation, but there has actually been surprisingly little research on this question in the context of AI systems. In this paper, we (1) develop a generalizable framework grounded in philosophy, psychology, and interpretable machine learning to investigate and define characteristics of good explanation, and (2) conduct a large-scale lab experiment to measure the impact of different factors on people's perceptions of understanding, usage intention, and trust of AI systems. The framework and study together provide a concrete guide for managers on how to present algorithmic prediction rationales to end-users to foster trust and adoption, and elements of explanation and transparency to be considered by AI researchers and engineers in designing, developing, and deploying transparent or explainable algorithms.","['Joy Lu', 'Dokyun (DK) Lee', 'Tae Wan Kim', 'David Danks']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Does AI Qualify for the Job?: A Bidirectional Model Mapping Labour and AI Intensities,"In this paper we present a setting for examining the relation be-tween the distribution of research intensity in AI research and the relevance for a range of work tasks (and occupations) in current and simulated scenarios. We perform a mapping between labourand AI using a set of cognitive abilities as an intermediate layer. This setting favours a two-way interpretation to analyse (1) what impact current or simulated AI research activity has or would have on labour-related tasks and occupations, and (2) what areas of AI research activity would be responsible for a desired or undesired effect on specific labour tasks and occupations. Concretely, in our analysis we map 59 generic labour-related tasks from several worker surveys and databases to 14 cognitive abilities from the cognitive science literature, and these to a comprehensive list of 328 AI benchmarks used to evaluate progress in AI techniques. We provide this model and its implementation as a tool for simulations. We also show the effectiveness of our setting with some illustrative examples.","['Fernando Martínez-Plumed', 'Songül Tolan', 'Annarosa Pesole', 'José Hernández-Orallo', 'Enrique Fernández-Macías', 'Emilia Gómez']","['Universitat Politècnica de València, Valencia, Spain', 'European Commission (JRC), Seville, Spain', 'European Commission (JRC), Seville, Italy', 'Universitat Politècnica de València, Valencia, Spain', 'European Commission (JRC), Seville, Spain', 'European Commission (JRC), Seville, Germany']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
An Empirical Approach to Capture Moral Uncertainty in AI,As AI Systems become increasingly autonomous they are expected to engage in complex moral decision-making processes. For the purpose of guidance of such processes theoretical and empirical solutions have been sought. In this research we integrate both theoretical and empirical lines of thought to address the matters of moral reasoning in AI Systems. We reconceptualize a metanormative framework for decision-making under moral uncertainty within the Discrete Choice Analysis domain and we operationalize it through a latent class choice model. The discrete choice analysis-based formulation of the metanormative framework is theory-rooted and practical as it captures moral uncertainty through a small set of latent classes. To illustrate our approach we conceptualize a society in which AI Systems are in charge of making policy choices. In the proof of concept two AI systems make policy choices on behalf of a society but while one of the systems uses a baseline moral certain model the other uses a moral uncertain model. It was observed that there are cases in which the AI Systems disagree about the policy to be chosen which we believe is an indication about the relevance of moral uncertainty.,"['Andreia Martinho', 'Maarten Kroesen', 'Caspar Chorus']","['Delft University of Technology, Delft, Netherlands', 'Delft University of Technology, Delft, Netherlands', 'Delft University of Technology, Delft, Netherlands']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
When Trusted Black Boxes Don't Agree: Incentivizing Iterative Improvement and Accountability in Critical Software Systems,"Software increasingly plays a key role in regulated areas like housing, hiring, and credit, as well as major public functions such as criminal justice and elections. It is easy for there to be unintended defects with a large impact on the lives of individuals and society as a whole. Preventing, finding, and fixing software defects is a key focus of both industrial software development efforts as well as academic research in software engineering. In this paper, we discuss flaws in the larger socio-technical decision-making processes in which critical black-box software systems are developed, deployed, and trusted. We use criminal justice software, specifically probabilistic genotyping (PG) software, as a concrete example. We describe how PG software systems, designed to do the same job, produce different results. We highlight the under-appreciated impact of changes in key parameters and the disparate impact that one such parameter can have on different racial/ethnic groups. We propose concrete changes to the socio-technical decision-making processes surrounding the use of PG software that could be used to incentivize iterative improvements in the accuracy, fairness, reliability, and accountability of these systems.","['Jeanna Neefe Matthews', 'Graham Northup', 'Isabella Grasso', 'Stephen Lorenz', 'Marzieh Babaeianjelodar', 'Hunter Bashaw', 'Sumona Mondal', 'Abigail Matthews', 'Mariama Njie', 'Jessica Goldthwaite']","['Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'University of Wisconsin Madison, Madison, WI, USA', 'Iona College, New York, NY, USA', 'The Legal Aid Society, New York, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
When Your Only Tool Is A Hammer: Ethical Limitations of Algorithmic Fairness Solutions in Healthcare Machine Learning,"It is no longer a hypothetical worry that artificial intelligence - more specifically, machine learning (ML) - can propagate the effects of pernicious bias in healthcare. To address these problems, some have proposed the development of 'algorithmic fairness' solutions. The primary goal of these solutions is to constrain the effect of pernicious bias with respect to a given outcome of interest as a function of one's protected identity (i.e., characteristics generally protected by civil or human rights legislation. The technical limitations of these solutions have been well-characterized. Ethically, the problematic implication - of developers, potentially, and end users - is that by virtue of algorithmic fairness solutions a model can be rendered 'objective' (i.e., free from the influence of pernicious bias). The ostensible neutrality of these solutions may unintentionally prompt new consequences for vulnerable groups by obscuring downstream problems due to the persistence of real-world bias. The main epistemic limitation of algorithmic fairness is that it assumes the relationship between the extent of bias's impact on a given health outcome and one's protected identity is mathematically quantifiable. The reality is that social and structural factors confluence in complex and unknown ways to produce health inequalities. Some of these are biologic in nature, and differences like these are directly relevant to predicting a health event and should be incorporated into the model's design. Others are reflective of prejudice, lack of access to healthcare, or implicit bias. Sometimes, there may be a combination. With respect to any specific task, it is difficult to untangle the complex relationships between potentially influential factors and which ones are 'fair' and which are not to inform their inclusion or mitigation in the model's design.","['Melissa McCradden', 'Mjaye Mazwi', 'Shalmali Joshi', 'James A. Anderson']","['The Hospital for Sick Children, Toronto, ON, Canada', 'The Hospital for Sick Children, Toronto, ON, Canada', 'Vector Institute for Artificial Intelligence, Toronto, ON, Canada', 'The Hospital for Sick Children, Toronto, ON, Canada']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Ethics for AI Writing: The Importance of Rhetorical Context,"Implicit in any rhetorical interaction-between humans or between humans and machines-are ethical codes that shape the rhetorical context, the social situation in which communication happens and also the engine that drives communicative interaction. Such implicit codes are usually invisible to AI writing systems because the social factors shaping communication (the why and how of language, not the what) are not usually explicitly evident in databases the systems use to produce discourse. Can AI writing systems learn to learn rhetorical context, particularly the implicit codes for communication ethics? We see evidence that some systems do address issues of rhetorical context, at least in rudimentary ways. But we critique the information transfer communication model supporting many AI writing systems, arguing for a social context model that accounts for rhetorical context-what is, in a sense, ""not there"" in the data corpus but that is critical for the production of meaningful, significant, and ethical communication. We offer two ethical principles to guide design of AI writing systems: transparency about machine presence and critical data awareness, a methodological reflexivity about rhetorical context and omissions in the data that need to be provided by a human agent or accounted for in machine learning.","['Heidi A. McKee', 'James E. Porter']","['Miami University, Oxford, OH, USA', 'Miami University, Oxford, OH, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Diversity and Inclusion Metrics in Subset Selection,"The ethical concept of fairness has recently been applied in machine learning (ML) settings to describe a wide range of constraints and objectives. When considering the relevance of ethical concepts to subset selection problems, the concepts of diversity and inclusion are additionally applicable in order to create outputs that account for social power and access differentials. We introduce metrics based on these concepts, which can be applied together, separately, and in tandem with additional fairness constraints. Results from human subject experiments lend support to the proposed criteria. Social choice methods can additionally be leveraged to aggregate and choose preferable sets, and we detail how these may be applied.","['Margaret Mitchell', 'Dylan Baker', 'Nyalleng Moorosi', 'Emily Denton', 'Ben Hutchinson', 'Alex Hanna', 'Timnit Gebru', 'Jamie Morgenstern']","['Google Research, Seattle, WA, USA', 'Google Research, Seattle, WA, USA', 'Google Research, Accra, Ghana', 'Google Research, New York, NJ, USA', 'Google Research, San Francisco, CA, USA', 'Google Research, Mountain View, CA, USA', 'Google Research, Mountain View, CA, USA', 'Google Research & University of Washington, Seattle, WA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Learning Norms from Stories: A Prior for Value Aligned Agents,"Value alignment is a property of an intelligent agent indicating that it can only pursue goals and activities that are beneficial to humans. Traditional approaches to value alignment use imitation learning or preference learning to infer the values of humans by observing their behavior. We introduce a complementary technique in which a value-aligned prior is learned from naturally occurring stories which encode societal norms. Training data is sourced from the children's educational comic strip, Goofus & Gallant. In this work, we train multiple machine learning models to classify natural language descriptions of situations found in the comic strip as normative or non-normative by identifying if they align with the main characters' behavior. We also report the models' performance when transferring to two unrelated tasks with little to no additional training on the new task.","['Md Sultan Al Nahian', 'Spencer Frazier', 'Mark Riedl', 'Brent Harrison']","['University of Kentucky, Lexington, KY, USA', 'Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA', 'University of Kentucky, Lexington, KY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Balancing the Tradeoff between Profit and Fairness in Rideshare Platforms during High-Demand Hours,"Rideshare platforms, when assigning requests to drivers, tend to maximize profit for the system and/or minimize waiting time for riders. Such platforms can exacerbate biases that drivers may have over certain types of requests. We consider the case of peak hours when the demand for rides is more than the supply of drivers. Drivers are well aware of their advantage during the peak hours and can choose to be selective about which rides to accept. Moreover, if in such a scenario, the assignment of requests to drivers (by the platform) is made only to maximize profit and/or minimize wait time for riders, requests of a certain type (e.g., from a non-popular pickup location, or to a non-popular drop-off location) might never be assigned to a driver. Such a system can be highly unfair to riders. However, increasing fairness might come at a cost of the overall profit made by the rideshare platform. To balance these conflicting goals, we present a flexible, non-adaptive algorithm, NAdap, that allows the platform designer to control the profit and fairness of the system via parameters α and β respectively.We model the matching problem as an online bipartite matching where the set of drivers is offline and requests arrive online. Upon the arrival of a request, we use NAdap to assign it to a driver (the driver might then choose to accept or reject it) or reject the request. We formalize the measures of profit and fairness in our setting and show that by using NAdap, the competitive ratios for profit and fairness measures would be no worse than α/e and β/e respectively. Extensive experimental results on both real-world and synthetic datasets confirm the validity of our theoretical lower bounds. Additionally, they show that NAdap under some choice of (α, β) can beat two natural heuristics, Greedy and Uniform, on both fairness and profit. Code is available at: https://github.com/nvedant07/rideshare-fairness-peak/. Full paper can be found in the proceedings of AAAI 2020 and on ArXiv: http://arxiv.org/abs/1912.08388).","['Vedant Nanda', 'Pan Xu', 'Karthik Abinav Sankararaman', 'John P. Dickerson', 'Aravind Srinivasan']","['University of Maryland & MPI-SWS, College Park, MD, USA', 'New Jersey Institute of Technology, Newark, NJ, USA', 'University of Maryland & Facebook, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
"Technocultural Pluralism: A ""Clash of Civilizations"" in Technology?","At the end of the Cold War, the renowned political scientist, Samuel Huntington, argued that future conflicts were more likely to stem from cultural frictions -- ideologies, social norms, and political systems -- rather than political or economic frictions. Huntington focused his concern on the future of geopolitics in a rapidly shrinking world. This paper argues that a similar dynamic is at play in the interaction of technology cultures. We emphasize the role of culture in the evolution of technology and identify the particular role that culture (esp. privacy culture) plays in the development of AI/ML technologies. Then we examine some implications that this perspective brings to the fore.",['Osonde A. Osoba'],"['Rand Corporation, Santa Monica, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society,"One way of carving up the broad 'AI ethics and society' research space that has emerged in recent years is to distinguish between 'near-term' and 'long-term' research. While such ways of breaking down the research space can be useful, we put forward several concerns about the near/long-term distinction gaining too much prominence in how research questions and priorities are framed. We highlight some ambiguities and inconsistencies in how the distinction is used, and argue that while there are differing priorities within this broad research community, these differences are not well-captured by the near/long-term distinction. We unpack the near/long-term distinction into four different dimensions, and propose some ways that researchers can communicate more clearly about their work and priorities using these dimensions. We suggest that moving towards a more nuanced conversation about research priorities can help establish new opportunities for collaboration, aid the development of more consistent and coherent research agendas, and enable identification of previously neglected research areas.","['Carina Prunkl', 'Jess Whittlestone']","['University of Oxford, Oxford, United Kingdom', 'University of Cambridge, Cambridge , United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Algorithmized but not Atomized? How Digital Platforms Engender New Forms of Worker Solidarity in Jakarta,"Jakarta's roads are green, filled as they are with the fluorescent green jackets, bright green logos and fluttering green banners of basecamps created by the city's digitized, 'online' motorbike-taxi drivers (ojol). These spaces function as waiting posts, regulatory institutions, information networks and spaces of solidarity for the ojol working for mobility-app companies, Grab and GoJek. Their existence though, presents a puzzle. In the world of on-demand matching, literature either predicts an isolated, atomized, disempowered digital worker or expects workers to have only temporary, online, ephemeral networks of mutual aid. Yet, Jakarta's ojol then introduce us to a new form of labor action that relies on an interface of the physical world and digital realm, complete with permanent shelters, quirky names, emblems, social media accounts and even their own emergency response service. This paper explores the contours of these labor formations and asks why digital workers in Jakarta are able to create collective structures of solidarity, even as app-mediated work may force them towards an individualized labor regime? I argue that these digital labor collectives are not accidental but a product of interactions between histories of social organization structures in Jakarta and affordances created by technological-mediation. Through participant observation and semi-structured interviews I excavate the bi-directional conversation between globalizing digital platforms and social norms, civic culture and labor market conditions in Jakarta which has allowed for particular forms of digital worker resistances to emerge. I recover power for the digital worker, who provides us with a path to resisting algorithmization of work while still participating in it through agentic labor actions rooted in shared identities, enabled by technological fluency and borne out of a desire for community.",['Rida Qadri'],"['Massachusetts Institute of Technology, Cambridge, MA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing,"Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of fiveethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.","['Inioluwa Deborah Raji', 'Timnit Gebru', 'Margaret Mitchell', 'Joy Buolamwini', 'Joonseok Lee', 'Emily Denton']","['University of Toronto, Toronto, ON, Canada', 'Google Research, San Francisco, CA, USA', 'Google Research, San Francisco, CA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Google Research, San Francisco, CA, USA', 'Google Research, San Francisco, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Human Comprehension of Fairness in Machine Learning,"Bias in machine learning has manifested injustice in several areas, with notable examples including gender bias in job-related ads [4], racial bias in evaluating names on resumes [3], and racial bias in predicting criminal recidivism [1]. In response, research into algorithmic fairness has grown in both importance and volume over the past few years. Different metrics and approaches to algorithmic fairness have been proposed, many of which are based on prior legal and philosophical concepts [2]. The rapid expansion of this field makes it difficult for professionals to keep up, let alone the general public. Furthermore, misinformation about notions of fairness can have significant legal implications. Computer scientists have largely focused on developing mathematical notions of fairness and incorporating them in fielded ML systems. A much smaller collection of studies has measured public perception of bias and (un)fairness in algorithmic decision-making. However, one major question underlying the study of ML fairness remains unanswered in the literature: Does the general public understand mathematical definitions of ML fairness and their behavior in ML applications? We take a first step towards answering this question by studying non-expert comprehension and perceptions of one popular definition of ML fairness, demographic parity [5]. Specifically, we developed an online survey to address the following: (1) Does a non-technical audience comprehend the definition and implications of demographic parity? (2) Do demographics play a role in comprehension? (3) How are comprehension and sentiment related? (4) Does the application scenario affect comprehension?","['Debjani Saha', 'Candice Schumann', 'Duncan C. McElfresh', 'John P. Dickerson', 'Michelle L. Mazurek', 'Michael Carl Tschantz']","['University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'International Computer Science Institute, Berkeley, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
"What's Next for AI Ethics, Policy, and Governance? A Global Overview","Since 2016, more than 80 AI ethics documents - including codes, principles, frameworks, and policy strategies - have been produced by corporations, governments, and NGOs. In this paper, we examine three topics of importance related to our ongoing empirical study of ethics and policy issues in these emerging documents. First, we review possible challenges associated with the relative homogeneity of the documents' creators. Second, we provide a novel typology of motivations to characterize both obvious and less obvious goals of the documents. Third, we discuss the varied impacts these documents may have on the AI governance landscape, including what factors are relevant to assessing whether a given document is likely to be successful in achieving its goals.","['Daniel Schiff', 'Justin Biddle', 'Jason Borenstein', 'Kelly Laas']","['Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA', 'Illinois Institute of Technology, Chicago, IL, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Trade-offs in Fair Redistricting,"What constitutes a 'fair' electoral districting plan is a discussion dating back to the founding of the United States and, in light of several recent court cases, mathematical developments, and the approaching 2020 U.S. Census, is still a fiercely debated topic today. In light of the growing desire and ability to use algorithmic tools in drawing these districts, we discuss two prototypical formulations of fairness in this domain: drawing the districts by a neutral procedure or drawing them to intentionally induce an equitable electoral outcome. We then generate a large sample of districting plans for North Carolina and Pennsylvania and consider empirically how compactness and partisan symmetry, as instantiations of these frameworks, trade off with each other -- prioritizing the value of one of these necessarily comes at a cost in the other.",['Zachary Schutzman'],"['University of Pennsylvania, Philadelphia, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-box Models,"Concerns within the machine learning community and external pressures from regulators over the vulnerabilities of machine learning algorithms have spurred on the fields of explainability, robustness, and fairness. Often, issues in explainability, robustness, and fairness are confined to their specific sub-fields and few tools exist for model developers to use to simultaneously build their modeling pipelines in a transparent, accountable, and fair way. This can lead to a bottleneck on the model developer's side as they must juggle multiple methods to evaluate their algorithms. In this paper, we present a single framework for analyzing the robustness, fairness, and explainability of a classifier. The framework, which is based on the generation of counterfactual explanations through a custom genetic algorithm, is flexible, model-agnostic, and does not require access to model internals. The framework allows the user to calculate robustness and fairness scores for individual models and generate explanations for individual predictions which provide a means for actionable recourse (changes to an input to help get a desired outcome). This is the first time that a unified tool has been developed to address three key issues pertaining towards building a responsible artificial intelligence system.","['Shubham Sharma', 'Jette Henderson', 'Joydeep Ghosh']","['University of Texas at Austin, Austin, TX, USA', 'CognitiveScale, Austin, TX, USA', 'CognitiveScale, Austin, TX, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?,"There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.","['Toby Shevlane', 'Allan Dafoe']","['University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods,"As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.","['Dylan Slack', 'Sophie Hilgard', 'Emily Jia', 'Sameer Singh', 'Himabindu Lakkaraju']","['University of California, Irvine, Irvine, CA, USA', 'Harvard University, Cambridge, MA, USA', 'Harvard University, Cambridge, MA, USA', 'University of California, Irvine, Irvine, CA, USA', 'Harvard University, Cambridge, MA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
U.S. Public Opinion on the Governance of Artificial Intelligence,"Artificial intelligence (AI) has widespread societal implications, yet social scientists are only beginning to study public attitudes toward the technology. Existing studies find that the public's trust in institutions can play a major role in shaping the regulation of emerging technologies. Using a large-scale survey (N=2000), we examined Americans' perceptions of 13 AI governance challenges as well as their trust in governmental, corporate, and multistakeholder institutions to responsibly develop and manage AI. While Americans perceive all of the AI governance issues to be important for tech companies and governments to manage, they have only low to moderate trust in these institutions to manage AI applications.","['Baobao Zhang', 'Allan Dafoe']","['University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
"Different ""Intelligibility"" for Different Folks","Many arguments have concluded that our autonomous technologies must be intelligible, interpretable, or explainable, even if that property comes at a performance cost. In this paper, we consider the reasons why some property like these might be valuable, we conclude that there is not simply one kind of 'intelligibility', but rather different types for different individuals and uses. In particular, different interests and goals require different types of intelligibility (or explanations, or other related notion). We thus provide a typography of 'intelligibility' that distinguishes various notions, and draw methodological conclusions about how autonomous technologies should be designed and deployed in different ways, depending on whose intelligibility is required.","['Yishan Zhou', 'David Danks']","['University of California San Diego, San Diego, CA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
AI and Holistic Review: Informing Human Reading in College Admissions,"College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant's fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.","['A.J. Alvero', 'Noah Arthurs', 'anthony lising antonio', 'Benjamin W. Domingue', 'Ben Gebre-Medhin', 'Sonia Giebel', 'Mitchell L. Stevens']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Robot Rights?: Let's Talk about Human Welfare Instead,"The 'robot rights' debate, and its related question of 'robot responsibility', invokes some of the most polarized positions in AI ethics. While some advocate for granting robots rights on a par with human beings, others, in a stark opposition argue that robots are not deserving of rights but are objects that should be our slaves. Grounded in post-Cartesian philosophical foundations, we argue not just to deny robots 'rights', but to deny that robots, as artifacts emerging out of and mediating human being, are the kinds of things that could be granted rights in the first place. Once we see robots as mediators of human being, we can understand how the 'robots rights' debate is focused on first world problems, at the expense of urgent ethical concerns, such as machine bias, machine elicited human labour exploitation, and erosion of privacy all impacting society's least privileged individuals. We conclude that, if human being is our starting point and human welfare is the primary concern, the negative impacts emerging from machinic systems, as well as the lack of taking responsibility by people designing, selling and deploying such machines, remains the most pressing ethical discussion in AI.","['Abeba Birhane', 'Jelle van Dijk']","['University College Dublin, Dublin, Ireland', 'University of Twente, Enschede, Netherlands']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Artificial Artificial Intelligence: Measuring Influence of AI 'Assessments' on Moral Decision-Making,"Given AI's growing role in modeling and improving decision-making, how and when to present users with feedback is an urgent topic to address. We empirically examined the effect of feedback from false AI on moral decision-making about donor kidney allocation. We found some evidence that judgments about whether a patient should receive a kidney can be influenced by feedback about participants' own decision-making perceived to be given by AI, even if the feedback is entirely random. We also discovered different effects between assessments presented as being from human experts and assessments presented as being from AI.","['Lok Chan', 'Kenzie Doyle', 'Duncan McElfresh', 'Vincent Conitzer', 'John P. Dickerson', 'Jana Schaich Borg', 'Walter Sinnott-Armstrong']","['Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA', 'University of Maryland, College Park, MD, USA', 'Duke University, Durham, NC, USA', 'University of Maryland, College Park, MD, USA', 'Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
A Just Approach Balancing Rawlsian Leximax Fairness and Utilitarianism,"Numerous AI-assisted resource allocation decisions need to balance the conflicting goals of fairness and efficiency. Our paper studies the challenging task of defining and modeling a proper fairness-efficiency trade off. We define fairness with Rawlsian leximax fairness, which views the lexicographic maximum among all feasible outcomes as the most equitable; and define efficiency with Utilitarianism, which seeks to maximize the sum of utilities received by entities regardless of individual differences. Motivated by a justice-driven trade off principle: prioritize fairness to benefit the less advantaged unless too much efficiency is sacrificed, we propose a sequential optimization procedure to balance leximax fairness and utilitarianism in decision-making. Each iteration of our approach maximizes a social welfare function, and we provide a practical mixed integer/linear programming (MILP) formulation for each maximization problem. We illustrate our method on a budget allocation example. Compared with existing approaches of balancing equity and efficiency, our method is more interpretable in terms of parameter selection, and incorporates a strong equity criterion with a thoroughly balanced perspective.","['Violet (Xinying) Chen', 'J. N. Hooker']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Should Artificial Intelligence Governance be Centralised?: Design Lessons from History,"Can effective international governance for artificial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difficulty in securing participation while creating stringent rules. Other considerations depend on the specific design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate.","['Peter Cihon', 'Matthijs M. Maas', 'Luke Kemp']","['University of Oxford, Oxford, United Kingdom', 'University of Copenhagen & University of Oxford, Copenhagen, Denmark', 'University of Cambridge, Cambridge, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
An Invitation to System-wide Algorithmic Fairness,"We propose a framework for analyzing and evaluating system-wide algorithmic fairness. The core idea is to use simulation techniques in order to extend the scope of current fairness assessments by incorporating context and feedback to a phenomenon of interest. By doing so, we expect to better understand the interaction among the social behavior giving rise to discrimination, automated decision making tools, and fairness-inspired statistical constraints. In particular, we invite the community to use agent based models as an explanatory tool for causal mechanisms of population level properties. We also propose embedding these into a reinforcement learning algorithm to find optimal actions for meaningful change. As an incentive for taking a system-wide approach , we show through a simple model of predictive policing and trials that if we limit our attention to one portion of the system, we may determine some blatantly unfair practices as fair, and be blind to overall unfairness.","['Efrén Cruz Cortés', 'Debashis Ghosh']","['The Pennsylvania State University, State College, PA, USA', 'University of Colorado Anschutz, Aurora, CO, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,"The implementation of AI systems has led to new forms of harm in various sensitive social domains. We analyze these as problems How to address these harms remains at the center of controversial debate. In this paper, we discuss the inherent normative uncertainty and political debates surrounding the safety of AI systems.of vagueness to illustrate the shortcomings of current technical approaches in the AI Safety literature, crystallized in three dilemmas that remain in the design, training and deployment of AI systems. We argue that resolving normative uncertainty to render a system 'safe' requires a sociotechnical orientation that combines quantitative and qualitative methods and that assigns design and decision power across affected stakeholders to navigate these dilemmas through distinct channels for dissent. We propose a set of sociotechnical commitments and related virtues to set a bar for declaring an AI system 'human-compatible', implicating broader interdisciplinary design approaches.","['Roel I.J. Dobbe', 'Thomas Krendl Gilbert', 'Yonatan Mintz']","['New York University, New York, NY, USA', 'University of California, Berkeley, Berkeley, CA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Toward Implementing the Agent-Deed-Consequence Model of Moral Judgment in Autonomous Vehicles,"Autonomous vehicles (AVs) and accidents they are involved in attest to the urgent need to consider the ethics of AI. The question dominating the discussion has been whether we want AVs to behave in a 'selfish' or utilitarian manner. Rather than considering modeling self-driving cars on a single moral system like utilitarianism, one possible way to approach programming for AI would be to reflect recent work in neuroethics. The Agent-Deed-Consequence (ADC) model [1-4] provides a promising account while also lending itself well to implementation in AI. The ADC model explains moral judgments by breaking them down into positive or negative intuitive evaluations of the Agent, Deed, and Consequence in any given situation. These intuitive evaluations combine to produce a judgment of moral acceptability. This explains the considerable flexibility and stability of human moral judgment that has yet to be replicated in AI. This paper examines the advantages and disadvantages of implementing the ADC model and how the model could inform future work on ethics of AI in general.",['Veljko Dubljevic'],"['North Carolina State University, Raleigh, NC, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification,"Modern face recognition systems leverage datasets containing images of hundreds of thousands of specific individuals' faces to train deep convolutional neural networks to learn an embedding space that maps an arbitrary individual's face to a vector representation of their identity. The performance of a face recognition system in face verification (1:1) and face identification (1:N) tasks is directly related to the ability of an embedding space to discriminate between identities. Recently, there has been significant public scrutiny into the source and privacy implications of large-scale face recognition training datasets such as MS-Celeb-1M and MegaFace, as many people are uncomfortable with their face being used to train dual-use technologies that can enable mass surveillance. However, the impact of an individual's inclusion in training data on a derived system's ability to recognize them has not previously been studied. In this work, we audit ArcFace, a state-of-the-art, open source face recognition system, in a large-scale face identification experiment with more than one million distractor images. We find a Rank-1 face identification accuracy of 79.71% for individuals present in the model's training data and an accuracy of 75.73% for those not present. This modest difference in accuracy demonstrates that face recognition systems using deep learning work better for individuals they are trained on, which has serious privacy implications when one considers all major open source face recognition training datasets do not obtain informed consent from individuals during their collection.","['Chris Dulhanty', 'Alexander Wong']","['University of Waterloo, Waterloo, ON, Canada', 'University of Waterloo, Waterloo, ON, Canada']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Proposal for Type Classification for Building Trust in Medical Artificial Intelligence Systems,"This paper proposes the establishment of Medical Artificial Intelligence (AI) Types (MA Types)""that classify AI in medicine not only by technical system requirements but also implications to healthcare workers' roles and users/patients. MA Types can be useful to promote discussion regarding the purpose and application of the clinical site. Although MA Types are based on the current technologies and regulations in Japan, but that does not hinder the potential reform of the technologies and regulations. MA Types aims to facilitate discussions among physicians, healthcare workers, engineers, public/patients and policymakers on AI systems in medical practices.","['Arisa Ema', 'Katsue Nagakura', 'Takanori Fujita']","['The University of Tokyo, Tokyo, Japan', 'M3, Inc, Tokyo, Japan', 'Keio University, Tokyo, Japan']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Adoption Dynamics and Societal Impact of AI Systems in Complex Networks,"We propose a game-theoretical model to simulate the dynamics of AI adoption in adaptive networks. This formalism allows us to understand the impact of the adoption of AI systems for society as a whole, addressing some of the concerns on the need for regulation. Using this model we study the adoption of AI systems, the distribution of the different types of AI (from selfish to utilitarian), the appearance of clusters of specific AI types, and the impact on the fitness of each individual. We suggest that the entangled evolution of individual strategy and network structure constitutes a key mechanism for the sustainability of utilitarian and human-conscious AI. Differently, in the absence of rewiring, a minority of the population can easily foster the adoption of selfish AI and gains a benefit at the expense of the remaining majority.","['Pedro M. Fernandes', 'Francisco C. Santos', 'Manuel Lopes']","['Universidade de Lisboa, Lisbon, Portugal', 'Universidade de Lisboa, Lisbon, Portugal', 'Universidade de Lisboa, Lisbon, Portugal']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Auditing Algorithms: On Lessons Learned and the Risks of Data Minimization,"In this paper, we present the Algorithmic Audit (AA) of REM!X, a personalized well-being recommendation app developed by Telefónica Innovación Alpha. The main goal of the AA was to identify and mitigate algorithmic biases in the recommendation system that could lead to the discrimination of protected groups. The audit was conducted through a qualitative methodology that included five focus groups with developers and a digital ethnography relying on users comments reported in the Google Play Store. To minimize the collection of personal information, as required by best practice and the GDPR [1], the REM!X app did not collect gender, age, race, religion, or other protected attributes from its users. This limited the algorithmic assessment and the ability to control for different algorithmic biases. Indirect evidence was thus used as a partial mitigation for the lack of data on protected attributes, and allowed the AA to identify four domains where bias and discrimination were still possible, even without direct personal identifiers. Our analysis provides important insights into how general data ethics principles such as data minimization, fairness, non-discrimination and transparency can be operationalized via algorithmic auditing, their potential and limitations, and how the collaboration between developers and algorithmic auditors can lead to better technologies","['Gemma Galdon Clavell', 'Mariano Martín Zamorano', 'Carlos Castillo', 'Oliver Smith', 'Aleksandar Matic']","['Eticas Research and Consulting, Barcelona, Spain', 'Eticas Research and Consulting, Barcelona, Spain', 'Pompeu Fabra University, Barcelona, Spain', 'ALPHA Telefónica, Barcelona, Spain', 'ALPHA Telefónica, Barcelona, Spain']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
"More Than ""If Time Allows"": The Role of Ethics in AI Education","Even as public pressure mounts for technology companies to consider societal impacts of products, industries and governments in the AI race are demanding technical talent. To meet this demand, universities clamor to add technical artificial intelligence (AI) and machine learning (ML) courses into computing curriculum-but how are societal and ethical considerations part of this landscape? We explore two pathways for ethics content in AI education: (1) standalone AI ethics courses, and (2) integrating ethics into technical AI courses. For both pathways, we ask: What is being taught? As we train computer scientists who will build and deploy AI tools, how are we training them to consider the consequences of their work? In this exploratory work, we qualitatively analyzed 31 standalone AI ethics classes from 22 U.S. universities and 20 AI/ML technical courses from 12 U.S. universities to understand which ethics-related topics instructors include in courses. We identify and categorize topics in AI ethics education, share notable practices, and note omissions. Our analysis will help AI educators identify what topics should be taught and create scaffolding for developing future AI ethics education.","['Natalie Garrett', 'Nathan Beard', 'Casey Fiesler']","['University of Colorado Boulder, Boulder, CO, USA', 'University of Maryland, College Park, MD, USA', 'University of Colorado Boulder, Boulder, CO, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
A Geometric Solution to Fair Representations,"To reduce human error and prejudice, many high-stakes decisions have been turned over to machine algorithms. However, recent research suggests that this does not remove discrimination, and can perpetuate harmful stereotypes. While algorithms have been developed to improve fairness, they typically face at least one of three shortcomings: they are not interpretable, their prediction quality deteriorates quickly compared to unbiased equivalents, and %the methodology cannot easily extend other algorithms they are not easily transferable across models% (e.g., methods to reduce bias in random forests cannot be extended to neural networks) . To address these shortcomings, we propose a geometric method that removes correlations between data and any number of protected variables. Further, we can control the strength of debiasing through an adjustable parameter to address the trade-off between prediction quality and fairness. The resulting features are interpretable and can be used with many popular models, such as linear regression, random forest, and multilayer perceptrons. The resulting predictions are found to be more accurate and fair compared to several state-of-the-art fair AI algorithms across a variety of benchmark datasets. Our work shows that debiasing data is a simple and effective solution toward improving fairness.","['Yuzi He', 'Keith Burghardt', 'Kristina Lerman']","['University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Measuring Fairness in an Unfair World,"Computer scientists have made great strides in characterizing different measures of algorithmic fairness, and showing that certain measures of fairness cannot be jointly satisfied. In this paper, I argue that the three most popular families of measures - unconditional independence, target-conditional independence and classification-conditional independence - make assumptions that are unsustainable in the context of an unjust world. I begin by introducing the measures and the implicit idealizations they make about the underlying causal structure of the contexts in which they are deployed. I then discuss how these idealizations fall apart in the context of historical injustice, ongoing unmodeled oppression, and the permissibility of using sensitive attributes to rectify injustice. In the final section, I suggest an alternative framework for measuring fairness in the context of existing injustice: distributive fairness.",['Jonathan Herington'],"['University of Rochester, Rochester, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
"Towards Just, Fair and Interpretable Methods for Judicial Subset Selection","In many judicial systems -- including the United States courts of appeals, the European Court of Justice, the UK Supreme Court and the Supreme Court of Canada -- a subset of judges is selected from the entire judicial body for each case in order to hear the arguments and decide the judgment. Ideally, the subset selected is representative, i.e., the decision of the subset would match what the decision of the entire judicial body would have been had they all weighed in on the case. Further, the process should be fair in that all judges should have similar workloads, and the selection process should not allow for certain judge's opinions to be silenced or amplified via case assignments. Lastly, in order to be practical and trustworthy, the process should also be interpretable, easy to use, and (if algorithmic) computationally efficient. In this paper, we propose an algorithmic method for the judicial subset selection problem that satisfies all of the above criteria. The method satisfies fairness by design, and we prove that it has optimal representativeness asymptotically for a large range of parameters and under noisy information models about judge opinions -- something no existing methods can provably achieve. We then assess the benefits of our approach empirically by counterfactually comparing against the current practice and recent alternative algorithmic approaches using cases from the United States courts of appeals database.","['Lingxiao Huang', 'Julia Wei', 'Elisa Celis']","['Yale University, New Haven, CT, USA', 'Yale University, New Haven, CT, USA', 'Yale University, New Haven, CT, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Monitoring Misuse for Accountable 'Artificial Intelligence as a Service',"AI is increasingly being offered 'as a service' (AIaaS). This entails service providers offering customers access to pre-built AI models and services, for tasks such as object recognition, text translation, text-to-voice conversion, and facial recognition, to name a few. The offerings enable customers to easily integrate a range of powerful AI-driven capabilities into their applications. Customers access these models through the provider's APIs, sending particular data to which models are applied, the results of which returned. However, there are many situations in which the use of AI can be problematic. AIaaS services typically represent generic functionality, available 'at a click'. Providers may therefore, for reasons of reputation or responsibility, seek to ensure that the AIaaS services they offer are being used by customers for 'appropriate' purposes. This paper introduces and explores the concept whereby AIaaS providers uncover situations of possible service misuse by their customers. Illustrated through topical examples, we consider the technical usage patterns that could signal situations warranting scrutiny, and raise some of the legal and technical challenges of monitoring for misuse. In all, by introducing this concept, we indicate a potential area for further inquiry from a range of perspectives.","['Seyyed Ahmad Javadi', 'Richard Cloete', 'Jennifer Cobbe', 'Michelle Seng Ah Lee', 'Jatinder Singh']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
"""The Global South is everywhere, but also always somewhere"": National Policy Narratives and AI Justice","There is more attention than ever on the social implications of AI. In contrast to universalized paradigms of ethics and fairness, a growing body of critical work highlights bias and discrimination in AI within the frame of social justice and human rights (""AI justice""). However, the geographical location of much of this critique in the West could be engendering its own blind spots. The global supply chain of AI (data, computational power, natural resources, labor) today replicates historical colonial inequities, and the continued subordination of Global South countries. This paper draws attention to official narratives from the Indian government and the United Nations Conference on Trade and Development (UNCTAD) advocating for the role (and place) of these regions in the AI economy. Domestically, these policies are being contested for their top-down formulation, and reflect narrow industry interests. This underscores the need to approach the political economy of AI from varying altitudes - global, national, and from the perspective of communities whose lives and livelihoods are most directly impacted in this economy. Without a deliberate effort at centering this conversation it is inevitable that mainstream discourse on AI justice will grow parallel to (and potentially undercut) demands emanating from Global South governments and communities",['Amba Kak'],"['AI Now Institute, New York, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Ethics of Food Recommender Applications,"The recent unprecedented popularity of food recommender applications has raised several issues related to the ethical, societal and legal implications of relying on these applications. In this paper, in order to assess the relevant ethical issues, we rely on the emerging principles across the AI & Ethics community and define them tailored context specifically. Considering the popular Food Recommender Systems (henceforth F-RS) in the European market cannot be regarded as personalised F-RS, we show how merely this lack of feature shifts the relevance of the focal ethical concerns. We identify the major challenges and propose a scheme for how explicit ethical agendas should be explained. We also argue how a multi-stakeholder approach is indispensable to ensure producing long-term benefits for all stakeholders. After proposing eight ethical desiderata points for F-RS, we present a case-study and assess it based on our proposed desiderata points.","['Daniel Karpati', 'Amro Najjar', 'Diego Agustin Ambrossio']","['University of Luxembourg, Esch-sur-Alzette, Luxembourg', 'University of Luxembourg, Esch-Sur-Alzette, Luxembourg', 'University of Luxembourg, Esch-Sur-Alzette, Luxembourg']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Artificial Intelligence and Indigenous Perspectives: Protecting and Empowering Intelligent Human Beings,"As 'control' is increasingly ceded to AI systems, potentially Artificial General Intelligence (AGI) humanity may be facing an identity crisis sooner rather than later, whereby the notion of 'intelligence' no longer remains solely our own. This paper characterizes the problem in terms of an impending loss of control and proposes a relational shift in our attitude towards AI. The shortcomings of value alignment as a solution to the problem are outlined which necessitate an extension of these principles. One such approach is considering strongly relational Indigenous epistemologies. The value of Indigenous perspectives has not been canvassed widely in the literature. Their utility becomes clear when considering the existence of well-developed epistemologies adept at accounting for the non-human, a task that defies Western anthropocentrism. Accommodating AI by considering it as part of our network is a step towards building a symbiotic relationship. Given that AGI questions our fundamental notions of what it means to have human rights, it is argued that in order to co-exist, we find assistance in Indigenous traditions such as the Hawaiian and Lakota ontologies. Lakota rituals provide comfort with the conception of non-human soul-bearer while Hawaiian stories provide possible relational schema to frame our relationship with AI.",['Suvradip Maitra'],"['University of Queensland, Brisbane, QLD, Australia']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
The Windfall Clause: Distributing the Benefits of AI for the Common Good,"As the transformative potential of AI has become increasingly salient as a matter of public and political interest, there has been growing discussion about the need to ensure that AI broadly benefits humanity. This in turn has spurred debate on the social responsibilities of large technology companies to serve the interests of society at large. In response, ethical principles and codes of conduct have been proposed to meet the escalating demand for this responsibility to be taken seriously. As yet, however, few institutional innovations have been suggested to translate this responsibility into legal commitments which apply to companies positioned to reap large financial gains from the development and use of AI. This paper offers one potentially attractive tool for addressing such issues: the Windfall Clause, which is an ex ante commitment by AI firms to donate a significant amount of any eventual extremely large profits. By this we mean an early commitment that profits that a firm could not earn without achieving fundamental, economically transformative breakthroughs in AI capabilities will be donated to benefit humanity broadly, with particular attention towards mitigating any downsides from deployment of windfall-generating AI.","[""Cullen O'Keefe"", 'Peter Cihon', 'Ben Garfinkel', 'Carrick Flynn', 'Jade Leung', 'Allan Dafoe']","['University of Oxford, San Francisco, CA, USA', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Steps Towards Value-Aligned Systems,"Algorithmic (including AI/ML) decision-making artifacts are an established and growing part of our decision-making ecosystem. They are now indispensable tools that help us manage the flood of information we use to try to make effective decisions in a complex world. The current literature is full of examples of how individual artifacts violate societal norms and expectations (e.g. violations of fairness, privacy, or safety norms). Against this backdrop, this discussion highlights an under-emphasized perspective in the body of research focused on assessing value misalignment in AI-equipped sociotechnical systems. The research on value misalignment so far has a strong focus on the behavior of individual tech artifacts. This discussion argues for a more structured systems-level approach for assessing value-alignment in sociotechnical systems. We rely primarily on the research on fairness to make our arguments more concrete. And we use the opportunity to highlight how adopting a system perspective improves our ability to explain and address value misalignments better. Our discussion ends with an exploration of priority questions that demand attention if we are to assure the value alignment of whole systems, not just individual artifacts.","['Osonde A. Osoba', 'Benjamin Boudreaux', 'Douglas Yeung']","['RAND Corporation, Santa Monica, CA, USA', 'RAND Corporation, Santa Monica, CA, USA', 'RAND Corporation, Santa Monica, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Contextual Analysis of Social Media: The Promise and Challenge of Eliciting Context in Social Media Posts with Natural Language Processing,"While natural language processing affords researchers an opportunity to automatically scan millions of social media posts, there is growing concern that automated computational tools lack the ability to understand context and nuance in human communication and language. This article introduces a critical systematic approach for extracting culture, context and nuance in social media data. The Contextual Analysis of Social Media (CASM) ap-proach considers and critiques the gap between inadequacies in natural language processing tools and differences in geographic, cultural, and age-related variance of social media use and communication. CASM utilizes a team-based approach to analysis of social media data, explicitly informed by community expertise. We use of CASM to analyze Twitter posts from gang-involved youth in Chicago. We designed a set of experiments to evaluate the performance of a support vector machine us-ing CASM hand-labeled posts against a distant model. We found that the CASM-informed hand-labeled data outperforms the baseline distant labels, indicating that the CASM labels capture additional dimensions of information that content-only methods lack. We then question whether this is helpful or harmful for gun violence prevention.","['Desmond U. Patton', 'William R. Frey', 'Kyle A. McGregor', 'Fei-Tzin Lee', 'Kathleen McKeown', 'Emanuel Moss']","['Columbia University, New York, NY, USA', 'Columbia University, New York, NY, USA', 'Lankenau Institute for Medical Research, Philadelphia, PA, USA', 'Columbia University, New York, NY, USA', 'Columbia University, New York, NY, USA', 'City University of New York, New York, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
The Perils of Objectivity: Towards a Normative Framework for Fair Judicial Decision-Making,"Fair decision-making in criminal justice relies on the recognition and incorporation of infinite shades of grey. In this paper, we detail how algorithmic risk assessment tools are counteractive to fair legal proceedings in social institutions where desired states of the world are contested ethically and practically. We provide a normative framework for assessing fair judicial decision-making, one that does not seek the elimination of human bias from decision-making as algorithmic fairness efforts currently focus on, but instead centers on sophisticating the incorporation of individualized or discretionary bias--a process that is requisitely human. Through analysis of a case study on social disadvantage, we use this framework to provide an assessment of potential features of consideration, such as political disempowerment and demographic exclusion, that are irreconcilable by current algorithmic efforts and recommend their incorporation in future reform.","['Andi Peng', 'Malina Simard-Halm']","['Microsoft Research, Redmond, WA, USA', 'University of Cambridge, Cambridge, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
FACE: Feasible and Actionable Counterfactual Explanations,"Work in Counterfactual Explanations tends to focus on the principle of ""the closest possible world"" that identifies small changes leading to the desired outcome. In this paper we argue that while this approach might initially seem intuitively appealing it exhibits shortcomings not addressed in the current literature. First, a counterfactual example generated by the state-of-the-art systems is not necessarily representative of the underlying data distribution, and may therefore prescribe unachievable goals (e.g., an unsuccessful life insurance applicant with severe disability may be advised to do more sports). Secondly, the counterfactuals may not be based on a ""feasible path"" between the current state of the subject and the suggested one, making actionable recourse infeasible (e.g., low-skilled unsuccessful mortgage applicants may be told to double their salary, which may be hard without first increasing their skill level). These two shortcomings may render counterfactual explanations impractical and sometimes outright offensive. To address these two major flaws, first of all, we propose a new line of Counterfactual Explanations research aimed at providing actionable and feasible paths to transform a selected instance into one that meets a certain goal. Secondly, we propose FACE: an algorithmically sound way of uncovering these ""feasible paths"" based on the shortest path distances defined via density-weighted metrics. Our approach generates counterfactuals that are coherent with the underlying data distribution and supported by the ""feasible paths"" of change, which are achievable and can be tailored to the problem at hand.","['Rafael Poyiadzi', 'Kacper Sokol', 'Raul Santos-Rodriguez', 'Tijl De Bie', 'Peter Flach']","['University of Bristol, Bristol, United Kingdom', 'University of Bristol, Bristol, United Kingdom', 'University of Bristol, Bristol, United Kingdom', 'Ghent University, Ghent, Belgium', 'University of Bristol, Bristol, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Balancing the Tradeoff Between Clustering Value and Interpretability,"Graph clustering groups entities -- the vertices of a graph -- based on their similarity, typically using a complex distance function over a large number of features. Successful integration of clustering approaches in automated decision-support systems hinges on the interpretability of the resulting clusters. This paper addresses the problem of generating interpretable clusters, given features of interest that signify interpretability to an end-user, by optimizing interpretability in addition to common clustering objectives. We propose a β-interpretable clustering algorithm that ensures that at least β fraction of nodes in each cluster share the same feature value. The tunable parameter β is user-specified. We also present a more efficient algorithm for scenarios with β\!=\!1$ and analyze the theoretical guarantees of the two algorithms. Finally, we empirically demonstrate the benefits of our approaches in generating interpretable clusters using four real-world datasets. The interpretability of the clusters is complemented by generating simple explanations denoting the feature values of the nodes in the clusters, using frequent pattern mining.","['Sandhya Saisubramanian', 'Sainyam Galhotra', 'Shlomo Zilberstein']","['University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Data Augmentation for Discrimination Prevention and Bias Disambiguation,"Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an ""ideal world'' dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.","['Shubham Sharma', 'Yunfeng Zhang', 'Jesús M. Ríos Aliaga', 'Djallel Bouneffouf', 'Vinod Muthusamy', 'Kush R. Varshney']","['IBM Research & University of Texas at Austin, Austin, TX, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Austin, TX, USA', 'IBM Research, Yorktown Heights, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Meta Decision Trees for Explainable Recommendation Systems,"We tackle the problem of building explainable recommendation systems that are based on a per-user decision tree, with decision rules that are based on single attribute values. We build the trees by applying learned regression functions to obtain the decision rules as well as the values at the leaf nodes. The regression functions receive as input the embedding of the user's training set, as well as the embedding of the samples that arrive at the current node. The embedding and the regressors are learned end-to-end with a loss that encourages the decision rules to be sparse. By applying our method, we obtain a collaborative filtering solution that provides a direct explanation to every rating it provides. With regards to accuracy, it is competitive with other algorithms. However, as expected, explainability comes at a cost and the accuracy is typically slightly lower than the state of the art result reported in the literature. Our code is available at \urlhttps://github.com/shulmaneyal/metatrees.","['Eyal Shulman', 'Lior Wolf']","['Tel Aviv University, Tel Aviv-Yafo, Israel', 'Tel Aviv University, Tel Aviv-Yafo, Israel']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Why Reliabilism Is not Enough: Epistemic and Moral Justification in Machine Learning,"In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of \em justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed wide adoption of machine learning? We argue that, in general, people implicitly adoptreliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method \citegoldman2012reliabilism. We argue that, in cases where model deployments require \em moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral ""wrapper'' around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification---moral justification. Finally, we offer cautions relevant to the (implicit or explicit) adoption of the reliabilist interpretation of machine learning.","['Andrew Smart', 'Larry James', 'Ben Hutchinson', 'Simone Wu', 'Shannon Vallor']","['Google, San Francisco, CA, USA', 'Google, San Francisco, CA, USA', 'Google, San Francisco, CA, USA', 'Google, Seattle, WA, USA', 'Google, Mountain View, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Social and Governance Implications of Improved Data Efficiency,"Many researchers work on improving the data efficiency of machine learning. What would happen if they succeed? This paper explores the social-economic impact of increased data efficiency. Specifically, we examine the intuition that data efficiency will erode the barriers to entry protecting incumbent data-rich AI firms, exposing them to more competition from data-poor firms. We find that this intuition is only partially correct: data efficiency makes it easier to create ML applications, but large AI firms may have more to gain from higher performing AI systems. Further, we find that the effect on privacy, data markets, robustness, and misuse are complex. For example, while it seems intuitive that misuse risk would increase along with data efficiency -- as more actors gain access to any level of capability -- the net effect crucially depends on how much defensive measures are improved. More investigation into data efficiency, as well as research into the ""AI production function"", will be key to understanding the development of the AI industry and its societal impacts.","['Aaron D. Tucker', 'Markus Anderljung', 'Allan Dafoe']","['Cornell University & University of Oxford, Ithaca, NY, USA', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Conservative Agency via Attainable Utility Preservation,"Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.","['Alexander Matt Turner', 'Dylan Hadfield-Menell', 'Prasad Tadepalli']","['Oregon State University, Corvallis, OR, USA', 'University of California, Berkeley, Berkeley, CA, USA', 'Oregon State University, Corvallis, OR, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
A Deontic Logic for Programming Rightful Machines,"A ""rightful machine"" is an explicitly moral, autonomous machine agent whose behavior conforms to principles of justice and the positive public law of a legitimate state. In this paper, I set out some basic elements of a deontic logic appropriate for capturing conflicting legal obligations for purposes of programming rightful machines. Justice demands that the prescriptive system of enforceable public laws be consistent, yet statutes or case holdings may often describe legal obligations that contradict; moreover, even fundamental constitutional rights may come into conflict. I argue that a deontic logic of the law should not try to work around such conflicts but, instead, identify and expose them so that the rights and duties that generate inconsistencies in public law can be explicitly qualified and the conflicts resolved. I then argue that a credulous, non-monotonic deontic logic can describe inconsistent legal obligations while meeting the normative demand for consistency in the prescriptive system of public law. I propose an implementation of this logic via a modified form of ""answer set programming,"" which I demonstrate with some simple examples.",['Ava Thomas Wright'],"['Northeastern University, Boston, MA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
A Fairness-aware Incentive Scheme for Federated Learning,"In federated learning (FL), data owners ""share"" their local data in a privacy preserving manner in order to build a federated model, which in turn, can be used to generate revenues for the participants. However, in FL involving business participants, they might incur significant costs if several competitors join the same federation. Furthermore, the training and commercialization of the models will take time, resulting in delays before the federation accumulates enough budget to pay back the participants. The issues of costs and temporary mismatch between contributions and rewards have not been addressed by existing payoff-sharing schemes. In this paper, we propose the Federated Learning Incentivizer (FLI) payoff-sharing scheme. The scheme dynamically divides a given budget in a context-aware manner among data owners in a federation by jointly maximizing the collective utility while minimizing the inequality among the data owners, in terms of the payoff gained by them and the waiting time for receiving payoff. Extensive experimental comparisons with five state-of-the-art payoff-sharing schemes show that FLI is the most attractive to high quality data owners and achieves the highest expected revenue for a data federation.","['Han Yu', 'Zelei Liu', 'Yang Liu', 'Tianjian Chen', 'Mingshu Cong', 'Xi Weng', 'Dusit Niyato', 'Qiang Yang']","['Nanyang Technological University, Singapore, Singapore', 'Nanyang Technological University, Singapore, Singapore', 'WeBank, Shenzhen, China', 'WeBank, Shenzhen, China', 'The University of Hong Kong, Hong Kong, China', 'Peking University, Beijing, China', 'Nanyang Technological University, Singapore, Singapore', 'Hong Kong University of Science and Technology, Hong Kong, China']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Joint Optimization of AI Fairness and Utility: A Human-Centered Approach,"Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.","['Yunfeng Zhang', 'Rachel Bellamy', 'Kush Varshney']","['IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA', 'IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA', 'IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Assessing Post-hoc Explainability of the BKT Algorithm,"As machine intelligence is increasingly incorporated into educational technologies, it becomes imperative for instructors and students to understand the potential flaws of the algorithms on which their systems rely. This paper describes the design and implementation of an interactive post-hoc explanation of the Bayesian Knowledge Tracing algorithm which is implemented in learning analytics systems used across the United States. After a user-centered design process to smooth out interaction design difficulties, we ran a controlled experiment to evaluate whether the interactive or static version of the explainable led to increased learning. Our results reveal that learning about an algorithm through an explainable depends on users' educational background. For other contexts, designers of post-hoc explainables must consider their users' educational background to best determine how to empower more informed decision-making with AI-enhanced systems.","['Tongyu Zhou', 'Haoyu Sheng', 'Iris Howley']","['Williams College, Williamstown, MA, USA', 'Williams College, Williamstown, MA, USA', 'Williams College, Williamstown, MA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Deepfakes for Medical Video De-Identification: Privacy Protection and Diagnostic Information Preservation,"Data sharing for medical research has been difficult as open-sourcing clinical data may violate patient privacy. Traditional methods for face de-identification wipe out facial information entirely, making it impossible to analyze facial behavior. Recent advancements on whole-body keypoints detection also rely on facial input to estimate body keypoints. Both facial and body keypoints are critical in some medical diagnoses, and keypoints invariability after de-identification is of great importance. Here, we propose a solution using deepfake technology, the face swapping technique. While this swapping method has been criticized for invading privacy and portraiture right, it could conversely protect privacy in medical video: patients' faces could be swapped to a proper target face and become unrecognizable. However, it remained an open question that to what extent the swapping de-identification method could affect the automatic detection of body keypoints. In this study, we apply deepfake technology to Parkinson's disease examination videos to de-identify subjects, and quantitatively show that: face-swapping as a de-identification approach is reliable, and it keeps the keypoints almost invariant, significantly better than traditional methods. This study proposes a pipeline for video de-identification and keypoint preservation, clearing up some ethical restrictions for medical data sharing. This work could make open-source high quality medical video datasets more feasible and promote future medical research that benefits our society.","['Bingquan Zhu', 'Hao Fang', 'Yanan Sui', 'Luming Li']","['Tsinghua University, Beijing, China', 'Tsinghua University, Beijing, China', 'Tsinghua University, Beijing, China', 'Tsinghua University, Beijing, China']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Arbiter: A Domain-Specific Language for Ethical Machine Learning,"The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.","['Julian Zucker', ""Myraeka d'Leeuwen""]","['Northeastern University, Boston, MA, USA', 'Northeastern University, Boston, MA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020
Sub-committee Approval Voting and Generalized Justified Representation Axioms,"Social choice is replete with various settings including single-winner voting, multi-winner voting, probabilistic voting, multiple referenda, and public decision making. We study a general model of social choice called sub-committee voting (SCV) that simultaneously generalizes these settings. We then focus on sub-committee voting with approvals and propose extensions of the justified representation axioms that have been considered for proportional representation in approval-based committee voting. We study the properties and relations of these axioms. For each of the axioms, we analyze whether a representative committee exists and also examine the complexity of computing and verifying such a committee.","['Haris Aziz', 'Barton E. Lee']","['The University of New South Wales & Data61, Sydney, Australia', 'The University of New South Wales & Data61, Sydney, Australia']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Purple Feed: Identifying High Consensus News Posts on Social Media,"Although diverse news stories are actively posted on social media, readers often focus on the news which reinforces their pre-existing views, leading to 'filter bubble' effects. To combat this, some recent systems expose and nudge readers toward stories with different points of view. One example is the Wall Street Journal's 'Blue Feed, Red Feed' system, which presents posts from biased publishers on each side of a topic. However, these systems have had limited success. We present a complementary approach which identifies high consensus 'purple' posts that generate similar reactions from both 'blue' and 'red' readers. We define and operationalize consensus for news posts on Twitter in the context of US politics. We show that high consensus posts can be identified and discuss their empirical properties. We present a method for automatically identifying high and low consensus news posts on Twitter, which can work at scale across many publishers. To do this, we propose a novel category of audience leaning based features, which we show are well suited to this task. Finally, we present our 'Purple Feed' system which highlights high consensus posts from publishers on both sides of the political spectrum.","['Mahmoudreza Babaei', 'Juhi Kulshrestha', 'Abhijnan Chakraborty', 'Fabrício Benevenuto', 'Krishna P. Gummadi', 'Adrian Weller']","['Max Planck Institute for Software Systems, Saarbrucken, Germany', 'Max Planck Institute for Software Systems, Saarbrucken, Germany', 'Max Planck Institute for Software Systems & IIT Kharagpur, India, Saarbrucken, Germany', 'Universidade Federal de Minas Gerais, Belo Horizonte, Brazil', 'Max Planck Institute for Software Systems, Saarbruecken, Germany', 'University of Cambridge & Alan Turing Institute, Cambridge, United Kingdom']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Companion Robots: the Hallucinatory Danger of Human-Robot Interactions,"The advent of the so-called Companion Robots is raising many ethical concerns among scholars and in the public opinion. Focusing mainly on robots caring for the elderly, in this paper we analyze these concerns to distinguish which are directly ascribable to robotic, and which are instead pre-existent. One of these is the ""deception objection"", namely the ethical unacceptability of deceiving the user about the simulated nature of the robot's behaviors. We argue on the inconsistency of this charge, as today formulated. After that, we underline the risk, for human-robot interaction, to become a hallucinatory relation where the human would subjectify the robot in a dynamic of meaning-overload. Finally, we analyze the definition of ""quasi-other"" relating to the notion of ""uncanny"". The goal of this paper is to argue that the main concern about Companion Robots is the simulation of a human-like interaction in the absence of an autonomous robotic horizon of meaning. In addition, that absence could lead the human to build a hallucinatory reality based on the relation with the robot.","['Piercosma Bisconti Lucidi', 'Daniele Nardi']","['Sapienza University of Rome, Rome, Italy', 'Sapienza University of Rome, Rome, Italy']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
"Cake, Death, and Trolleys: Dilemmas as benchmarks of ethical decision-making","Artificial intelligence (AI) systems are becoming part of our lives and societies. The more decisions such systems make for us, the more we need to ensure that the decisions they make have a positive individual and societal ethical impact. How can we estimate how good a system is at making ethical decisions? Benchmarking is used to evaluate how good a machine or a process performs with respect to industry bests. In this paper we argue that (some) ethical dilemmas can be used as benchmarks for estimating the ethical performance of an autonomous system. We advocate that an open source repository of such dilemmas should be maintained. We present a prototype of such a repository available at https://imdb. uib.no/dilemmaz/articles/all1.","['Edvard P. Bjørgen', 'Simen Madsen', 'Therese S. Bjørknes', 'Fredrik V. Heimsæter', 'Robin Håvik', 'Morten Linderud', 'Per-Niklas Longberg', 'Louise A. Dennis', 'Marija Slavkovik']","['University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Liverpool, Liverpool, United Kingdom', 'University of Bergen, Bergen, Norway']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Incorrigibility in the CIRL Framework,"A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. 2015 in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.",['Ryan Carey'],"['Oxford University, Oxford, United Kingdom']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
An AI Race for Strategic Advantage: Rhetoric and Risks,"The rhetoric of the race for strategic advantage is increasingly being used with regard to the development of artificial intelligence (AI), sometimes in a military context, but also more broadly. This rhetoric also reflects real shifts in strategy, as industry research groups compete for a limited pool of talented researchers, and nation states such as China announce ambitious goals for global leadership in AI. This paper assesses the potential risks of the AI race narrative and of an actual competitive race to develop AI, such as incentivising corner-cutting on safe-ty and governance, or increasing the risk of conflict. It explores the role of the research community in respond-ing to these risks. And it briefly explores alternative ways in which the rush to develop powerful AI could be framed so as instead to foster collaboration and respon-sible progress.","['Stephen Cave', 'Seán S. ÓhÉigeartaigh']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Utilizing Housing Resources for Homeless Youth Through the Lens of Multiple Multi-Dimensional Knapsacks,"There are over 1 million homeless youth in the U.S. each year. To reduce homelessness, U.S. Housing and Urban Development (HUD) and housing communities provide housing programs/services to homeless youth with the goal of improving their long-term situation. Housing communities are facing a difficult task of filling their housing programs, with as many youths as possible, subject to resource constraints for meeting the needs of youth. Currently, the assignment is manually done by humans working in the housing communities. In this paper, we consider the problem of assigning homeless youth to housing programs subject to resource constraints. We provide an initial abstract model for this setting and show that the problem of maximizing the total assigned youth to the programs under this model is APX-hard. To solve the problem, we non-trivially formulate it as a multiple multi-dimensional knapsack problem (MMDKP), which is not known to have any approximation algorithm. We provide a first interpretable and easy-to-use greedy algorithm with logarithmic approximation ratio for solving general MMDKP. We conduct experiments on random and realistic instances of the housing assignment settings and show that our algorithm is efficient and effective in solving large instances (up to 1 million youth).","['Hau Chan', 'Long Tran-Thanh', 'Bryan Wilder', 'Eric Rice', 'Phebe Vayanos', 'Milind Tambe']","['University of Nebraska-Lincoln, Lincoln, NE, USA', 'University of Southampton, Southampton, United Kingdom', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Sociotechnical Systems and Ethics in the Large,"Advances in AI techniques and computing platforms have triggered a lively and expanding discourse on ethical decision making by autonomous agents. Much recent work in AI concentrates on the challenges of moral decision making from a decision-theoretic perspective, and especially the representation of various ethical dilemmas. Such approaches may be useful but in general are not productive because moral decision making is as context-driven as other forms of decision making, if not more. In contrast, we consider ethics not from the standpoint of an individual agent but of the wider sociotechnical systems (STS) in which the agent operates. Our contribution in this paper is the conception of ethical STS founded on governance that takes into account stakeholder values, normative constraints on agents, and outcomes (states of the STS) that obtain due to actions taken by agents. An important element of our conception is accountability, which is necessary for adequate consideration of outcomes that prima facie appear ethical or unethical. Focusing on STS provides a basis for tackling the difficult problems of ethics because the norms of an STS give an operational basis for agent decision making.","['Amit K. Chopra', 'Munindar P. SIngh']","['Lancaster University, Lancaster, United Kingdom', 'North Carolina State University, Raleigh, NC, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
"Towards an ""Ethics by Design"" Methodology for AI Research Projects","Addressing ethical issues arising from AI research, and by extension from most areas of Data Science, is a core challenge in both the academic and industry worlds. The nature of research and the specific set of technical skills involved imply that AI and Data Science researchers are not equipped to identify and anticipate such issues arising, or to establish solutions at the time a specific research project is being designed. In this paper, we discuss the need for a methodology for ethical research design that involves a broader set of skills from the start of the project. We specifically identify, from the relevant literature, a set of requirements that we argue to be needed for such a methodology. We then explore two case studies where such ethical considerations have been explored in conjunction with the development of specific research projects, in order to validate those assumptions and generalise them into a set of principles guiding an ""Ethics by Design"" method for conducting AI and Data Science research.","[""Mathieu d'Aquin"", 'Pinelopi Troullinou', ""Noel E. O'Connor"", 'Aindrias Cullen', 'Gráinne Faller', 'Louise Holden']","['National University of Ireland Galway, Galway, Ireland', 'The Open University, Milton Keynes, United Kingdom', 'Dublin City University, Dublin, Ireland', 'Dublin City University, Dublin, Ireland', 'FH Media Consulting Ltd, Dublin, Ireland', 'FH Media Consulting Ltd, Dublin, Ireland']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Ethics by Design: Necessity or Curse?,"Ethics by Design concerns the methods, algorithms and tools needed to endow autonomous agents with the capability to reason about the ethical aspects of their decisions, and the methods, tools and formalisms to guarantee that an agent's behavior remains within given moral bounds. In this context some questions arise: How and to what extent can agents understand the social reality in which they operate, and the other intelligences (AI, animals and humans) with which they co-exist? What are the ethical concerns in the emerging new forms of society, and how do we ensure the human dimension is upheld in interactions and decisions by autonomous agents?. But overall, the central question is: ""Can we, and should we, build ethically-aware agents?"" This paper presents initial conclusions from the thematic day of the same name held at PRIMA2017, on October 2017.","['Virginia Dignum', 'Matteo Baldoni', 'Cristina Baroglio', 'Maurizio Caon', 'Raja Chatila', 'Louise Dennis', 'Gonzalo Génova', 'Galit Haim', 'Malte S. Kließ', 'Maite Lopez-Sanchez', 'Roberto Micalizio', 'Juan Pavón', 'Marija Slavkovik', 'Matthijs Smakman', 'Marlies van Steenbergen', 'Stefano Tedeschi', 'Leon van der Toree', 'Serena Villata', 'Tristan de Wildt']","['Umeå University, Umeå, Sweden', 'Università degli Studi di Torino, Torino, Italy', 'Università degli Studi di Torino, Torino, Italy', 'University of Applied Sciences and Arts Western Switzerland, Fribourg, Switzerland', 'Sorbonne Université, Paris, France', 'University of Liverpool, Liverpool, United Kingdom', 'Universidad Carlos III de Madrid, Leganés, Spain', 'The College of Management Academic Studies, Rishon Lezion, Israel', 'Deft Technical University, Delft, Netherlands', 'Universitat de Barcelona, Barcelona, Spain', 'Università degli Studi di Torino, Torino, Italy', 'Universidad Complutense de Madrid, Madrid, Spain', 'University of Bergen, Bergen, Norway', 'HU University of Applied Sciences Utrecht, Utrecht, Netherlands', 'HU University of Applied Sciences, Utrecht, Utrecht, Netherlands', 'Università degli Studi di Torino, Torino, Italy', 'University of Luxembourg, Luxembourg, Luxembourg', ""Université Côte d'Azur, CNRS, Inria, I3S, Sophia Antipolis Cedex, France"", 'Delft University of Technology, Delft, Netherlands']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Measuring and Mitigating Unintended Bias in Text Classification,"We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.","['Lucas Dixon', 'John Li', 'Jeffrey Sorensen', 'Nithum Thain', 'Lucy Vasserman']","['Google, New York, NY, USA', 'Google, New York, NY, USA', 'Google, New York, NY, USA', 'Google, New York, NY, USA', 'Google, New York, NY, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
On the Distinction between Implicit and Explicit Ethical Agency,"With recent advances in artificial intelligence and the rapidly increasing importance of autonomous intelligent systems in society, it is becoming clear that artificial agents will have to be designed to comply with complex ethical standards. As we work to develop moral machines, we also push the boundaries of existing legal categories. The most pressing question is what kind of ethical decision-making our machines are actually able to engage in. Both in law and in ethics, the concept of agency forms a basis for further legal and ethical categorisations, pertaining to decision-making ability. Hence, without a cross-disciplinary understanding of what we mean by ethical agency in machines, the question of responsibility and liability cannot be clearly addressed. Here we make first steps towards a comprehensive definition, by suggesting ways to distinguish between implicit and explicit forms of ethical agency.","['Sjur Dyrkolbotn', 'Truls Pedersen', 'Marija Slavkovik']","['Western Norway University of Applied Sciences, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations,"We introduce \em AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation.","['Upol Ehsan', 'Brent Harrison', 'Larry Chan', 'Mark O. Riedl']","['Georgia Institute of Technology, Atlanta, GA, USA', 'University of Kentucky, Lexington, KY, USA', 'Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Jill Watson Doesn't Care if You're Pregnant: Grounding AI Ethics in Empirical Studies,"Jill Watson is our name for a virtual teaching assistant for a Georgia Tech course on artificial intelligence: Jill answers routine, frequently asked questions on the class discussion forum. In this paper, we outline some of the ethical issues that arose in the development and deployment of the virtual teaching assistant. We posit that experiments such as Jill Watson are critical for deeply understanding AI ethics.","['Bobbie Eicher', 'Lalith Polepeddi', 'Ashok Goel']","['Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Regulating Artificial Intelligence: Proposal for a Global Solution,"Given the ubiquity of artificial intelligence (AI) in modern societies, it is clear that individuals, corporations, and countries will be grappling with the legal and ethical issues of its use. As global problems require global solutions, we propose the establishment of an international AI regulatory agency that --- drawing on interdisciplinary expertise --- could create a unified framework for the regulation of AI technologies and inform the development of AI policies around the world. We urge that such an organization be developed with all deliberate haste, as issues such as cryptocurrencies, personalized political ad hacking, autonomous vehicles and autonomous weaponized agents are already a reality, affecting international trade, politics, and war.","['Olivia J. Erdélyi', 'Judy Goldsmith']","['University of Canterbury, Christchurch, New Zealand', 'University of Kentucky, Lexington, KY, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
"Value Alignment, Fair Play, and the Rights of Service Robots","Ethics and safety research in artificial intelligence is increasingly framed in terms of ""alignment'' with human values and interests. I argue that Turing's call for ""fair play for machines'' is an early and often overlooked contribution to the alignment literature. Turing's appeal to fair play suggests a need to correct human behavior to accommodate our machines, a surprising inversion of how value alignment is treated today. Reflections on ""fair play'' motivate a novel interpretation of Turing's notorious ""imitation game'' as a condition not of intelligence but instead of value alignment : a machine demonstrates a minimal degree of alignment (with the norms of conversation, for instance) when it can go undetected when interrogated by a human. I carefully distinguish this interpretation from the Moral Turing Test, which is not motivated by a principle of fair play, but instead depends on imitation of human moral behavior. Finally, I consider how the framework of fair play can be used to situate the debate over robot rights within the alignment literature. I argue that extending rights to service robots operating in public spaces is ""fair'' in precisely the sense that it encourages an alignment of interests between humans and machines.",['Daniel Estrada'],"['New Jersey Institute of Technology, Newark, NJ, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Fairness in Relational Domains,"AI and machine learning tools are being used with increasing frequency for decision making in domains that affect peoples' lives such as employment, education, policing and loan approval. These uses raise concerns about biases of algorithmic discrimination and have motivated the development of fairness-aware machine learning. However, existing fairness approaches are based solely on attributes of individuals. In many cases, discrimination is much more complex, and taking into account the social, organizational, and other connections between individuals is important. We introduce new notions of fairness that are able to capture the relational structure in a domain. We use first-order logic to provide a flexible and expressive language for specifying complex relational patterns of discrimination. Furthermore, we extend an existing statistical relational learning framework, probabilistic soft logic (PSL), to incorporate our definition of relational fairness. We refer to this fairness-aware framework FairPSL. FairPSL makes use of the logical definitions of fairnesss but also supports a probabilistic interpretation. In particular, we show how to perform maximum a posteriori(MAP) inference by exploiting probabilistic dependencies within the domain while avoiding violation of fairness guarantees. Preliminary empirical evaluation shows that we are able to make both accurate and fair decisions.","['Golnoosh Farnadi', 'Behrouz Babaki', 'Lise Getoor']","['UC Santa Cruz, Santa Cruz, CA, USA', 'Polytechnique Montreal, Montreal, PQ, Canada', 'UC Santa Cruz, Santa Cruz, CA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Adapting a Kidney Exchange Algorithm to Align with Human Values,"The efficient allocation of limited resources is a classical problem in economics and computer science. In kidney exchanges, a central market maker allocates living kidney donors to patients in need of an organ. Patients and donors in kidney exchanges are prioritized using ad-hoc weights decided on by committee and then fed into an allocation algorithm that determines who get what---and who does not. In this paper, we provide an end-to-end methodology for estimating weights of individual participant profiles in a kidney exchange. We first elicit from human subjects a list of patient attributes they consider acceptable for the purpose of prioritizing patients (e.g., medical characteristics, lifestyle choices, and so on). Then, we ask subjects comparison queries between patient profiles and estimate weights in a principled way from their responses. We show how to use these weights in kidney exchange market clearing algorithms. We then evaluate the impact of the weights in simulations and find that the precise numerical values of the weights we computed matter little, other than the ordering of profiles that they imply. However, compared to not prioritizing patients at all, there is a significant effect, with certain classes of patients being (de)prioritized based on the human-elicited value judgments.","['Rachel Freedman', 'Jana Schaich Borg', 'Walter Sinnott-Armstrong', 'John P. Dickerson', 'Vincent Conitzer']","['Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA', 'University of Maryland, College Park, MD, USA', 'Duke University, Durham, NC, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Non-Discriminatory Machine Learning through Convex Fairness Criteria,We introduce a novel technique to achieve non-discrimination in machine learning without sacrificing convexity and probabilistic interpretation. We also propose a new notion of fairness for machine learning called the weighted proportional fairness and show that our technique satisfies this subjective fairness criterion.,"['Naman Goel', 'Mohammad Yaghini', 'Boi Faltings']","['Ecole Polytechnique Federal de Lausanne, Lausanne, Switzerland', 'Ecole Polytechnique Federal de Lausanne, Lausanne, Switzerland', 'Ecole Polytechnique Federal de Lausanne, Lausanne, Switzerland']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
An Education Model of Reasonable and Good-Faith Effort for Autonomous Systems,In this paper we propose a framework for conceptualizing and demonstrating a good-faith effort when developing autonomous systems. The framework addresses two fundamental problems facing autonomous systems: (1) the disconnect between human-mental models and machine-based sensors and algorithms; and (2) unpredictability in complex systems. We address these problems using a mix of education - explicitly delineating the mapping between human concepts and their machine equivalents in a structured manner - and data sampling with expected ranges as a testing mechanism.,"['Cindy M. Grimm', 'William D. Smart', 'Woodrow Hartzog']","['Oregon State University, Corvallis, OR, USA', 'Oregon State University, Corvallis, OR, USA', 'Northeastern University, Boston, MA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Rethinking AI Strategy and Policy as Entangled Super Wicked Problems,"This paper attempts a preliminary analysis of the general approach to AI strategy/policy research through the lens of wicked problems literature. Wicked problems are a class of social policy problems for which traditional methods of resolution fail. Super wicked problems refer to even more complex social policy problems, e.g. climate change. We first propose a hierarchy of three classes of AI strategy/policy problems, all wicked or super wicked problems. We next identify three independent super wicked problems in AI strategy/policy and propose that the most significant of these challenges - the development of safe and beneficial artificial general intelligence - to be significantly more complex and nuanced, thus posing a new degree of 'wickedness.' We then explore analysis and techniques for addressing wicked problems and super wicked problems. This leads to a discussion of the implications of these ideas on the problems of AI strategy/policy.",['Ross Gruetzemacher'],"['Auburn University, Auburn, AL, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Ethical Challenges in Data-Driven Dialogue Systems,"The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.","['Peter Henderson', 'Koustuv Sinha', 'Nicolas Angelard-Gontier', 'Nan Rosemary Ke', 'Genevieve Fried', 'Ryan Lowe', 'Joelle Pineau']","['McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'Polytechnique Montréal, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Toward Non-Intuition-Based Machine and Artificial Intelligence Ethics: A Deontological Approach Based on Modal Logic,"We propose a deontological approach to machine (or AI) ethics that avoids some weaknesses of an intuition-based system, such as that of Anderson and Anderson. In particular, it has no need to deal with conflicting intuitions, and it yields a more satisfactory account of when autonomy should be respected. We begin with a ""dual standpoint'' theory of action that regards actions as grounded in reasons and therefore as having a conditional form that is suited to machine instructions. We then derive ethical principles based on formal properties that the reasons must exhibit to be coherent, and formulate the principles using quantified modal logic. We conclude that deontology not only provides a more satisfactory basis for machine ethics but endows the machine with an ability to explain its actions, thus contributing to transparency in AI.","['John N. Hooker', 'Tae Wan N. Kim']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection,"Web-based human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its often-latent nature. The use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decision-making, and, more broadly, help curb a widespread social problem. Trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the Web -- a higher score indicates a greater risk of trafficking-related (involuntary) activities. In this paper, we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the DARPA Memex program. Drawing on multi-institutional data, systems, and experiences collected during this time, we also conduct post hoc bias analyses and present a bias mitigation plan. Our findings show that, while automatic trafficking detection is an important application of AI for social good, it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate de-biasing. This ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads.","['Kyle Hundman', 'Thamme Gowda', 'Mayank Kejriwal', 'Benedikt Boecking']","['California Institute of Technology, Pasadena, CA, USA', 'University of Southern California, Marina Del Rey, CA, USA', 'University of Southern California, Marina Del Rey, CA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Transparency and Explanation in Deep Reinforcement Learning Neural Networks,"Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of ""object saliency maps"", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.","['Rahul Iyer', 'Yuezhang Li', 'Huao Li', 'Michael Lewis', 'Ramitha Sundar', 'Katia Sycara']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Google Inc., Mountain View, PA, USA', 'University of Pittsburgh, Pittsburgh, PA, USA', 'University of Pittsburgh, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Socially-Aware Navigation Using Topological Maps and Social Norm Learning,"We present socially-aware navigation for an intelligent robot wheelchair in an environment with many pedestrians. The robot learns social norms by observing the behaviors of human pedestrians, interpreting detected biases as social norms, and incorporating those norms into its motion planning. We compare our socially-aware motion planner with a baseline motion planner that produces safe, collision-free motion. The ability of our robot to learn generalizable social norms depends on our use of a topological map abstraction, so that a practical number of observations can allow learning of a social norm applicable in a wide variety of circumstances. We show that the robot can detect biases in observed human behavior that support learning the social norm of driving on the right. Furthermore, we show that when the robot follows these social norms, its behavior influences the behavior of pedestrians around it, increasing their adherence to the same norms. We conjecture that the legibility of the robot's normative behavior improves human pedestrians' ability to predict the robot's future behavior, making them more likely to follow the same norm.","['Collin Johnson', 'Benjamin Kuipers']","['University of Michigan, Ann Arbor, MI, USA', 'University of Michigan, Ann Arbor, MI, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Meritocratic Fairness for Infinite and Contextual Bandits,"We study fairness in linear bandit problems. Starting from the notion of meritocratic fairness introduced in~\citeJKMR16, we carry out a more refined analysis of a more general problem, achieving better performance guarantees with fewer modelling assumptions on the number and structure of available choices as well as the number selected. We also analyze the previously-unstudied question of fairness in infinite linear bandit problems, obtaining instance-dependent regret upper bounds as well as lower bounds demonstrating that this instance-dependence is necessary. The result is a framework for meritocratic fairness in an online linear setting that is substantially more powerful, general, and realistic than the current state of the art.","['Matthew Joseph', 'Michael Kearns', 'Jamie Morgenstern', 'Seth Neel', 'Aaron Roth']","['University of Pennsylvania, Philadelphia, PA, USA', 'University of Pennsylvania, Philadelphia, PA, USA', 'University of Pennsylvania, Philadelphia, PA, USA', 'University of Pennsylvania, Philadelphia, PA, USA', 'University of Pennsylvania, Philadelphia, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Opportunities and Challenges for Artificial Intelligence in India,"In the future of India lies the future of a sixth of the world's population. As the Artificial Intelligence (AI) revolution sweeps through societies and enters daily life, its role in shaping India's development and growth is bound to be substantial. For India, AI holds promise as a catalyst to accelerate progress, while providing mechanisms to leapfrog traditional hurdles such as poor infrastructure and bureaucracy. At the same time, an investment in AI is accompanied by risk factors with long-term implications on society: it is imperative that risks be vetted at this early stage. In this paper, we describe opportunities and challenges for AI in India. We detail opportunities that are cross-cutting (bridging India's linguistic divisions, mining public data), and also specific to one particular sector (healthcare). We list challenges that originate from existing social conditions (such as equations of caste and gender). Thereafter we distill out concrete steps and safeguards, which we believe are necessary for robust and inclusive development as India enters the AI era.","['Shivaram Kalyanakrishnan', 'Rahul Alex Panicker', 'Sarayu Natarajan', 'Shreya Rao']","['Indian Institute of Technology Bombay, Mumbai, India', 'Embrace Innovations, Bangalore, India', ""King's College London, London, United Kingdom"", 'Rao Law Chambers, Azim Premji University, Bangalore, India']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Real-Time Inference of User Types to Assist with more Inclusive and Diverse Social Media Activism Campaigns,"Social media provides a mechanism for people to engage with social causes across a range of issues. It also provides a strategic tool to those looking to advance a cause to exchange, promote or publicize their ideas. In such instances, AI can be either an asset if used appropriately or a barrier. One of the key issues for a workforce diversity campaign is to understand in real-time who is participating - specifically, whether the participants are individuals or organizations, and in case of individuals, whether they are male or female. In this paper, we present a study to demonstrate a case for AI for social good that develops a model to infer in real-time the different user types participating in a cause-driven hashtag campaign on Twitter, ILookLikeAnEngineer (ILLAE). A generic framework is devised to classify a Twitter user into three classes: organization, male and female in a real-time manner. The framework is tested against two datasets (ILLAE and a general dataset) and outperforms the baseline binary classifiers for categorizing organization/individual and male/female. The proposed model can be applied to future social cause-driven campaigns to get real-time insights on the macro-level social behavior of participants.","['Habib Karbasian', 'Hemant Purohit', 'Rajat Handa', 'Aqdas Malik', 'Aditya Johri']","['George Mason University, Fairfax, VA, USA', 'George Mason University, Fairfax, VA, USA', 'George Mason University, Fairfax, VA, USA', 'George Mason University, Fairfax, VA, USA', 'George Mason University, Fairfax, VA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Inverse Norm Conflict Resolution,"In previous work we provided a ""norm conflict resolution"" algorithm allowing agents in stochastic domains (represented by Markov Decision Processes) to ""maximally satisfy"" a set of moral or social norms, where such norms are represented by statements in linear temporal logic (LTL). This required the agent designer to provide weights specifying the relative importance of each norm. In this paper, we propose an ""inverse norm conflict resolution'' algorithm for learning these weights from demonstration. This approach minimizes a cost function based on the relative entropy between a policy encoding the observed behavior and a policy representing optimal norm-following behavior. We demonstrate the effectiveness of the algorithm in a simple GridWorld domain.","['Daniel Kasenberg', 'Matthias Scheutz']","['Tufts University, Medford, MA, USA', 'Tufts University, Medford, MA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
"Norms, Rewards, and the Intentional Stance: Comparing Machine Learning Approaches to Ethical Training","The challenge of training AI systems to perform responsibly and beneficially has inspired different approaches for teaching a system what people want and how it is acceptable to attain that in the world. In this paper we compare work in reinforcement learning, in particular inverse reinforcement learning, with our norm inference approach. We test those two systems and present results. Using the idea of the ""intentional stance"", we explain how a norm inference approach can work even when another agent is acting strictly according to reward functions. In this way norm inference presents itself as a promising, more explicitly accountable approach with which to design AI systems from the start.","['Daniel Kasenberg', 'Thomas Arnold', 'Matthias Scheutz']","['Tufts University, Medford, MA, USA', 'Tufts University, Medford, MA, USA', 'Tufts University, Medford, MA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Margins and Opportunity,"We use the statistical quantity of margin --- the distance between a decision boundary and a classified point, or the gap between two scores --- to formalize the principle of equal opportunity --- the chance to improve one's outcome, regardless of group status. This leads to a better definition of opportunity which recognizes, for example, that a strongly rejected individual was offered less recourse than a weakly rejected one, despite the shared outcome. It also leads to simpler algorithms, since real-valued margins are easier to analyze and optimize than discrete outcomes. We formalize two ways that a protected group may be guaranteed equal opportunity: (1) (social) mobility: acceptance should be within reach for the group (conversely, the general population shouldn't be cushioned from rejection), and (2) contrast: within the group, good candidates should get substantially higher scores than bad candidates, preventing the so-called 'token' effect. A simple linear classifier seems to offer roughly equal opportunity both experimentally and mathematically.",['Shiva Kaul'],"['Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
A Computational Model of Commonsense Moral Decision Making,"We introduce a computational model for building moral autonomous vehicles by learning and generalizing from human moral judgments. We draw on a cognitively inspired model of how people and young children learn moral theories from sparse and noisy data and integrate observations made from different people in different groups. The problem of moral learning for autonomous vehicles is cast as learning how to weigh the different features of the dilemma using utility calculus, with the goal of making these trade-offs reflect how people make them in a wide variety of moral dilemma. By modeling the structures of individuals and groups in a hierarchical Bayesian model, we show that an individual's moral values -- as well as a group's shared values -- can be inferred from sparse and noisy data. We evaluate our approach with data from the Moral Machine, a web application that collects human judgments on moral dilemmas involving autonomous vehicles, and show that the model rapidly and accurately infers people's preferences and can predict the difficulty of moral dilemmas from limited data.","['Richard Kim', 'Max Kleiman-Weiner', 'Andrés Abeliuk', 'Edmond Awad', 'Sohan Dsouza', 'Joshua B. Tenenbaum', 'Iyad Rahwan']","['Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
When Do People Want AI to Make Decisions?,"AI systems are now or will soon be sophisticated enough to make consequential decisions. Although this technology has flourished, we also need public appraisals of AI systems playing these more important roles. This article reports surveys of preferences for and against AI systems making decisions in various domains as well as experiments that intervene on these preferences. We find that these preferences are contingent on subjects' previous exposure to computer systems making these kinds of decisions, and some interventions designed to mimic previous exposure successfully encourage subjects to be more hospitable to computer systems making these weighty decisions.","['Max F. Kramer', 'Jana Schaich Borg', 'Vincent Conitzer', 'Walter Sinnott-Armstrong']","['University of Arizona, Tucson, AZ, USA', 'Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Impacts on Trust of Healthcare AI,"Artificial Intelligence and robotics are rapidly moving into healthcare, playing key roles in specific medical functions, including diagnosis and clinical treatment. Much of the focus in the technology development has been on human-machine interactions, leading to a host of related technology-centric questions. In this paper, we focus instead on the impact of these technologies on human-human interactions and relationships within the healthcare domain. In particular, we argue that trust plays a central role for relationships in the healthcare domain, and the introduction of healthcare AI can potentially have significant impacts on those relations of trust. We contend that healthcare AI systems ought to be treated as assistive technologies that go beyond the usual functions of medical devices. As a result, we need to rethink regulation of healthcare AI systems to ensure they advance relevant values. We propose three distinct guidelines that can be universalized across federal regulatory boards to ensure that patient-doctor trust is not detrimentally affected by the deployment and widespread adoption of healthcare AI technologies.","['Emily LaRosa', 'David Danks']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Regulating Autonomous Vehicles: A Policy Proposal,"The widespread deployment and testing of autonomous vehicles in real-world environments raises key questions about how such systems should be regulated. Much of the current debate presupposes that the regulatory system we currently use for regular vehicles is also appropriate for semi- and fully-autonomous ones. In opposition, we first argue that there are serious challenges to regulating autonomous vehicles using current approaches, due to the nature of both autonomous capabilities (and their connections to operational domains), and also the systems' tasks and surrounding uncertainties. Instead, we argue that vehicles with autonomous capabilities are similar in key respects to drugs and other medical inter-ventions. Thus, we propose (on a ""first principles"" basis) a dynamic regulatory system with staged approvals and monitoring, analogous to the system used by the U.S. Food & Drug Administration. We provide details about the operation of such a potential system, and conclude by characterizing its benefits, costs, and plausibility.","['Alex John London', 'David Danks']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Preferences and Ethical Principles in Decision Making,"If we want people to trust AI systems, we need to provide the systems we create with the ability to discriminate between what humans would consider good and bad decisions. The quality of a decision should not be based only on the preferences or optimization criteria of the decision makers, but also on other properties related to the impact of the decision, such as whether it is ethical, or if it complies to constraints and priorities given by feasibility constraints or safety regulations. The CP-net formalism [2] is a convenient and expressive way to model preferences, providing an effective compact way to qualitatively model preferences over outcomes, i.e., decisions, with a combinatorial structure [3, 7]. If we wish to incorporate ethical, moral, or norms based constraints to a decision context, it means that the subjective preferences of the decision makers are not the only source of information we should consider [1, 8]. Indeed, depending on the context, we may have to consider specific ethical principles derived from an appropriate ethical theory or various laws and norms. While preferences are important, when preferences and ethical principles are in conflict, the principles should override the subjective preferences of the decision maker. Therefore, it is essential to have well founded techniques to evaluate whether preferences are compatible with a set of ethical principles, and to measure how much these preferences deviate from the ethical principles.","['Andrea Loreggia', 'Nicholas Mattei', 'Francesca Rossi', 'K. Brent Venable']","['University of Padova, Padova, Italy', 'IBM Research, Yorktown Heights NY, NY, USA', 'IBM Research and University of Padova, Yorktown Heights NY, NY, USA', 'Tulane University, New Orleans, LA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Regulating for 'Normal AI Accidents': Operational Lessons for the Responsible Governance of Artificial Intelligence Deployment,"New technologies, particularly those which are deployed rapidly across sectors, or which have to operate in competitive conditions, can disrupt previously stable technology governance regimes. This leads to a precarious need to balance caution against performance while exploring the resulting 'safe operating space'. This paper will argue that Artificial Intelligence is one such critical technology, the responsible deployment of which is likely to prove especially complex, because even narrow AI applications often involve networked (tightly coupled, opaque) systems operating in complex or competitive environments. This ensures such systems are prone to 'normal accident'-type failures which can cascade rapidly, and are hard to contain or even detect in time. Legal and governance approaches to the deployment of AI will have to reckon with the specific causes and features of such 'normal accidents'. While this suggests that large-scale, cascading errors in AI systems are inevitable, an examination of the operational features that lead technologies to exhibit 'normal accidents' enables us to derive both tentative principles for precautionary policymaking, and practical recommendations for the safe(r) deployment of AI systems. This may help enhance the safety and security of these systems in the public sphere, both in the short- and in the long term.",['Matthijs M. Maas'],"['University of Copenhagen, Copenhagen, Denmark']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
What's up with Privacy?: User Preferences and Privacy Concerns in Intelligent Personal Assistants,"The recent breakthroughs in Artificial Intelligence (AI) have allowed individuals to rely on automated systems for a variety of reasons. Some of these systems are the currently popular voice-enabled systems like Echo by Amazon and Home by Google that are also called as Intelligent Personal Assistants (IPAs). Though there are rising concerns about privacy and ethical implications, users of these IPAs seem to continue using these systems. We aim to investigate to what extent users are concerned about privacy and how they are handling these concerns while using the IPAs. By utilizing the reviews posted online along with the responses to a survey, this paper provides a set of insights about the detected markers related to user interests and privacy challenges. The insights suggest that users of these systems irrespective of their concerns about privacy, are generally positive in terms of utilizing IPAs in their everyday lives. However, there is a significant percentage of users who are concerned about privacy and take further actions to address related concerns. Some percentage of users expressed that they do not have any privacy concerns but when they learned about the ""always listening"" feature of these devices, their concern about privacy increased.","['Lydia Manikonda', 'Aditya Deotale', 'Subbarao Kambhampati']","['Arizona State University, Tempe, AZ, USA', 'Arizona State University, Tempe, AZ, USA', 'Arizona State University, Tempe, AZ, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Fairness in Deceased Organ Matching,"As algorithms are given responsibility to make decisions that impact our lives, there is increasing awareness of the need to ensure the fairness of these decisions. One of the first challenges then is to decide what fairness means in a particular context. We consider here fairness in deciding how to match organs donated by deceased donors to patients. Due to the increasing age of patients on the waiting list, and of organs being donated, the current ""first come, first served'' mechanism used in Australia is under review to take account of age of patients and of organs. We consider how to revise the mechanism to take account of age fairly. We identify a number of different types of fairness, such as to patients, to regions and to blood types and consider how they can be achieved.","['Nicholas Mattei', 'Abdallah Saffidine', 'Toby Walsh']","['IBM Research, Yorktown Heights, NY, USA', 'UNSW Sydney, Sydney, Australia', 'UNSW Sydney, Data61, & TU Berlin, Sydney, Australia']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Fair Forests: Regularized Tree Induction to Minimize Model Bias,"The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly, there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable, non-linear, and easy-to-use. In this paper we develop, to our knowledge, the first technique for the induction of fair decision trees.We show that our ""Fair Forest"" retains the benefits of the tree-based approach, while providing both greater accuracy and fairness than other alternatives, for both ""group fairness'' and ""individual fairness.'' We also introduce new measures for fairness which are able to handle multinomial and continues attributes as well as regression problems, as opposed to binary attributes and labels only. Finally, we demonstrate a new, more robust evaluation procedure for algorithms that considers the dataset in its entirety rather than only a specific protected attribute.","['Edward Raff', 'Jared Sylvester', 'Steven Mills']","['Booz Allen Hamilton, Columbia, MD, USA', 'Booz Allen Hamilton, Columbia, MD, USA', 'Booz Allen Hamilton, Columbia, MD, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
A Framework for Grounding the Moral Status of Intelligent Machines,"I propose a framework, derived from moral theory, for assessing the moral status of intelligent machines. Using this framework, I claim that some current and foreseeable intelligent machines have approximately as much moral status as plants, trees, and other environmental entities. This claim raises the question: what obligations could a moral agent (e.g., a normal adult human) have toward an intelligent machine? I propose that the threshold for any moral obligation should be the ""functional morality"" of Wallach and Allen [20], while the upper limit of our obligations should not exceed the upper limit of our obligations toward plants, trees, and other environmental entities.",['Michael R. Scheessele'],"['Indiana University South Bend, South Bend, IN, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Partially Generative Neural Networks for Gang Crime Classification with Partial Information,"More than 1 million homicides, robberies, and aggravated assaults occur in the United States each year. These crimes are often further classified into different types based on the circumstances surrounding the crime (e.g., domestic violence, gang-related). Despite recent technological advances in AI and machine learning, these additional classification tasks are still done manually by specially trained police officers. In this paper, we provide the first attempt to develop a more automatic system for classifying crimes. In particular, we study the question of classifying whether a given violent crime is gang-related. We introduce a novel Partially Generative Neural Networks (PGNN) that is able to accurately classify gang-related crimes both when full information is available and when there is only partial information. Our PGNN is the first generative-classification model that enables to work when some features of the test examples are missing. Using a crime event dataset from Los Angeles covering 2014-2016, we experimentally show that our PGNN outperforms all other typically used classifiers for the problem of classifying gang-related violent crimes.","['Sungyong Seo', 'Hau Chan', 'P. Jeffrey Brantingham', 'Jorja Leap', 'Phebe Vayanos', 'Milind Tambe', 'Yan Liu']","['University of Southern California, Los Angeles, CA, USA', 'University of Nebraska-Lincoln, Lincoln, NE, USA', 'University of California, Los Angeles, Los Angeles, CA, USA', 'University of California, Los Angeles, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Exploiting Moral Values to Choose the Right Norms,"Norms constitute regulative mechanisms extensively enacted in groups, organisations, and societies. However, 'choosing the right norms to establish' constitutes an open problem that requires the consideration of a number of constraints (such as norm relations) and preference criteria (e.g over involved moral values). This paper advances the state of the art in the Normative Multiagent Systems literature by formally defining this problem and by proposing its encoding as a linear program so that it can be automatically solved.","['Marc Serramia', 'Maite Lopez-Sanchez', 'Juan A. Rodriguez-Aguilar', 'Javier Morales', 'Michael Wooldridge', 'Carlos Ansotegui']","['Universitat de Barcelona, Barcelona, Spain', 'Universitat de Barcelona, Barcelona, Spain', 'Artificial Intelligence Research Institute (IIIA-CSIC), Cerdanyola del Valles, Spain', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom', 'Universitat de Lleida, Lleida, Spain']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Towards Provably Moral AI Agents in Bottom-up Learning Frameworks,"We examine moral machine decision making as inspired by a central question posed by Rossi with respect to moral preferences: can AI systems based on statistical machine learning (which do not provide a natural way to explain or justify their decisions) be used for embedding morality into a machine in a way that allows us to prove that nothing morally wrong will happen? We argue for an evaluation which is held to the same standards as a human agent, removing the demand that ethical behaviour is always achieved. We introduce four key meta-qualities desired for our moral standards, and then proceed to clarify how we can prove that an agent will correctly learn to perform moral actions given a set of samples within certain error bounds. Our group-dynamic approach enables us to demonstrate that the learned models converge to a common function to achieve stability. We further explain a valuable intrinsic consistency check made possible through the derivation of logical statements from the machine learning model. In all, this work proposes an approach for building ethical AI systems, coming from the perspective of artificial intelligence research, and sheds important light on understanding how much learning is required in order for an intelligent agent to behave morally with negligible error.","['Nolan P. Shaw', 'Andreas Stöckel', 'Ryan W. Orr', 'Thomas F. Lidbetter', 'Robin Cohen']","['University of Waterloo, Waterloo, ON, Canada', 'University of Waterloo, Waterloo, ON, Canada', 'University of Waterloo, Waterloo, ON, Canada', 'University of Waterloo, Waterloo, ON, Canada', 'University of Waterloo, Waterloo, ON, Canada']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
"Embodiment, Anthropomorphism, and Intellectual Property Rights for AI Creations","Computational creativity is an emerging branch of artificial intelligence (AI) concerned with algorithms that can create novel and high-quality ideas or artifacts, either autonomously or semi-autonomously in collaboration with people. Quite simply, such algorithms may be described as artificial innovation engines. These technologies raise questions of authorship/inventorship and of agency, which become further muddled by the social context induced by AI that may be physically-embodied or anthropomorphized. These questions are fundamentally intertwined with the provision of appropriate incentives for conducting and commercializing computational creativity research through intellectual property regimes. This paper reviews current understanding of intellectual property rights for AI, and explores possible framings for intellectual property policy in social context.","['Deepak Somaya', 'Lav R. Varshney']","['University of Illinois at Urbana-Champaign, Urbana, IL, USA', 'University of Illinois at Urbana-Champaign, Urbana, IL, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Towards Composable Bias Rating of AI Services,"A new wave of decision-support systems are being built today using AI services that draw insights from data (like text and video) and incorporate them in human-in-the-loop assistance. However, just as we expect humans to be ethical, the same expectation needs to be met by automated systems that increasingly get delegated to act on their behalf. A very important aspect of an ethical behavior is to avoid (intended, perceived, or accidental) bias. Bias occurs when the data distribution is not representative enough of the natural phenomenon one wants to model and reason about. The possibly biased behavior of a service is hard to detect and handle if the AI service is merely being used and not developed from scratch, since the training data set is not available. In this situation, we envisage a 3rd party rating agency that is independent of the API producer or consumer and has its own set of biased and unbiased data, with customizable distributions. We propose a 2-step rating approach that generates bias ratings signifying whether the AI service is unbiased compensating, data-sensitive biased, or biased. The approach also works on composite services. We implement it in the context of text translation and report interesting results.","['Biplav Srivastava', 'Francesca Rossi']","['IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Socialbots Supporting Human Rights,"Socialbots, or non-human/algorithmic social media users, have recently been documented as competing for information dissemination and disruption on online social networks. Here we investigate the influence of socialbots in Mexican Twitter in regards to the ""Tanhuato"" human rights abuse report. We analyze the applicability of the BotOrNot API to generalize from English to Spanish tweets and propose adaptations for Spanish-speaking bot detection. We then use text and sentiment analysis to compare the differences between bot and human tweets. Our analysis shows that bots actually aided in information proliferation among human users. This suggests that taxonomies classifying bots should include non-adversarial roles as well. Our study contributes to the understanding of different behaviors and intentions of automated accounts observed in empirical online social network data. Since this type of analysis is seldom performed in languages different from English, the proposed techniques we employ here are also useful for other non-English corpora.","['Pablo Suárez-Serrato', 'Eduardo Iván Velázquez Richards', 'Mehrdad Yazdani']","['Universidad Nacional Autónoma de México, México City, Mexico', 'Universidad Nacional Autónoma de México, México City, Mexico', 'University of California San Diego, San Diego, CA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Designing Non-greedy Reinforcement Learning Agents with Diminishing Reward Shaping,"This paper intends to address an issue in RL that when agents possessing varying capabilities, most resources may be acquired by stronger agents, leaving the weaker ones ""starving"". We introduce a simple method to train non-greedy agents in multi-agent reinforcement learning scenarios with nearly no extra cost. Our model can achieve the following goals in designing the non-greedy agent:non-homogeneous equality, only need local information, cost-effective, generalizable and configurable. We propose the idea of diminishing reward that makes the agent feel less satisfied for consecutive rewards obtained. This idea allows the agents to behave less greedy with-out the need to explicitly coding any ethical pattern nor monitor other agents' status. Given our framework, resources distributed more equally without running the risk of reaching homogeneous equality. We designed two games, Gathering Game and Hunter Prey to evaluate the quality of the model.","['Fan-Yun Sun', 'Yen-Yu Chang', 'Yueh-Hua Wu', 'Shou-De Lin']","['National Taiwan University, Taipei, Taiwan ROC', 'National Taiwan University, Taipei, Taiwan ROC', 'National Taiwan University, Taipei, Taiwan ROC', 'National Taiwan University, Taipei, Taiwan ROC']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation,"Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque. We propose Distill-and-Compare, an approach to audit such models without probing the black-box model API or pre-defining features to audit. To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by the black-box models. We compare the mimic model trained with distillation to a second, un-distilled transparent model trained on ground truth outcomes, and use differences between the two models to gain insight into the black-box model. We demonstrate the approach on four data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. We also propose a statistical test to determine if a data set is missing key features used to train the black-box model. Our test finds that the ProPublica data is likely missing key feature(s) used in COMPAS.","['Sarah Tan', 'Rich Caruana', 'Giles Hooker', 'Yin Lou']","['Cornell University, Ithaca, NY, USA', 'Microsoft Research, Redmond, WA, USA', 'Cornell University, Ithaca, NY, USA', 'Ant Financial, San Mateo, CA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Software Malpractice in the Age of AI: A Guide for the Wary Tech Company,"Professional malpractice - the concept of heightened duties for those entrusted with special knowledge and crucial tasks - is rooted in history. And yet, since the dawn of the computer age, courts in the United States have almost universally rejected a theory of software malpractice, declining to hold software engineers to the same professional standards as doctors, lawyers, and engineers. What is changing, however, is the speed at which software based on artificial intelligence technologies is replacing the very professionals already subject to professional liability. Society has already decided (in some cases, millennia ago) that those tasks warrant special accountability; new to the analysis is which human is closest in line to the adverse event. As AI expands, the pressure for courts to go one level up the causal chain in search of human agency and professional accountability will mount. This essay analyzes the case law rejecting software malpractice for clues about where the doctrine might go in the age of AI, then discusses what technology companies can learn from the safety enhancements of doctors, lawyers, and other historic professionals who have adapted to such heightened legal scrutiny for years.",['Daniel L. Tobey'],"['Vinson & Elkins, LLP, Dallas, TX, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
The Dark Side of Ethical Robots,"Concerns over the risks associated with advances in Artificial Intelligence have prompted calls for greater efforts toward robust and beneficial AI, including machine ethics. Recently, roboticists have responded by initiating the development of so-called ethical robots. These robots would, ideally, evaluate the consequences of their actions and morally justify their choices. This emerging field promises to develop extensively over the next few years. However, in this paper, we point out an inherent limitation of the emerging field of ethical robots. We show that building ethical robots also inevitably enables the construction of unethical robots. In three experiments, we show that it is remarkably easy to modify an ethical robot so that it behaves competitively, or even aggressively. The reason for this is that the cognitive machinery required to make an ethical robot can always be corrupted to make unethical robots. We discuss the implications of this finding to the governance of ethical robots. We conclude that the risks that unscrupulous actors might compromise a robot's ethics are so great as to raise serious doubts over the wisdom of embedding ethical decision making in real-world safety-critical robots, such as driverless cars.","['Dieter Vanderelst', 'Alan Winfield']","['University of Cincinnati, Cincinnati, OH, USA', 'University of the West of England, Bristol, United Kingdom']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Modeling Epistemological Principles for Bias Mitigation in AI Systems: An Illustration in Hiring Decisions,"Artificial Intelligence (AI) has been used extensively in automatic decision making in a broad variety of scenarios, ranging from credit ratings for loans to recommendations of movies. Traditional design guidelines for AI models focus essentially on accuracy maximization, but recent work has shown that economically irrational and socially unacceptable scenarios of discrimination and unfairness are likely to arise unless these issues are explicitly addressed. This undesirable behavior has several possible sources, such as biased datasets used for training that may not be detected in black-box models. After pointing out connections between such bias of AI and the problem of induction, we focus on Popper's contributions after Hume's, which offer a logical theory of preferences. An AI model can be preferred over others on purely rational grounds after one or more attempts at refutation based on accuracy and fairness. Inspired by such epistemological principles, this paper proposes a structured approach to mitigate discrimination and unfairness caused by bias in AI systems. In the proposed computational framework, models are selected and enhanced after attempts at refutation. To illustrate our discussion, we focus on hiring decision scenarios where an AI system filters in which job applicants should go to the interview phase.","['Marisa Vasconcelos', 'Carlos Cardonha', 'Bernardo Gonçalves']","['IBM Research, Sao Paulo, Brazil', 'IBM Research, Sao Paulo, Brazil', 'IBM Research, Sao Paulo, Brazil']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
An Autonomous Architecture that Protects the Right to Privacy,"The advent and widespread adoption of wearable cameras and autonomous robots raises important issues related to privacy. The mobile cameras on these systems record and may re-transmit enormous amounts of video data that can then be used to identify, track, and characterize the behavior of the general populous. This paper presents a preliminary computational architecture designed to preserve specific types of privacy over a video stream by identifying categories of individuals, places, and things that require higher than normal privacy protection. This paper describes the architecture as a whole as well as preliminary results testing aspects of the system. Our intention is to implement and test the system on ground robots and small UAVs and demonstrate that the system can provide selective low-level masking or deletion of data requiring higher privacy protection.",['Alan R. Wagner'],"['Pennsylvania State University, University Park, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Mitigating Unwanted Biases with Adversarial Learning,"Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.","['Brian Hu Zhang', 'Blake Lemoine', 'Margaret Mitchell']","['Stanford University, Stanford, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Privacy-Preserving Machine Learning Based Data Analytics on Edge Devices,"Emerging Machine Learning (ML) techniques, such as Deep Neural Network, are widely used in today's applications and services. However, with social awareness of privacy and personal data rapidly rising, it becomes a pressing and challenging societal issue to both keep personal data private and benefit from the data analytics power of ML techniques at the same time. In this paper, we argue that to avoid those costs, reduce latency in data processing, and minimise the raw data revealed to service providers, many future AI and ML services could be deployed on users' devices at the Internet edge rather than putting everything on the cloud. Moving ML-based data analytics from cloud to edge devices brings a series of challenges. We make three contributions in this paper. First, besides the widely discussed resource limitation on edge devices, we further identify two other challenges that are not yet recognised in existing literature: lack of suitable models for users, and difficulties in deploying services for users. Second, we present preliminary work of the first systematic solution, i.e. Zoo, to fully support the construction, composing, and deployment of ML models on edge and local devices. Third, in the deployment example, ML service are proved to be easy to compose and deploy with Zoo. Evaluation shows its superior performance compared with state-of-art deep learning platforms and Google ML services.","['Jianxin Zhao', 'Richard Mortier', 'Jon Crowcroft', 'Liang Wang']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Data Driven Techniques for Organizing Scientific Articles Relevant to Biomimicry,"Life on earth presents elegant solutions to many of the challenges innovators and entrepreneurs across disciplines face every day. To facilitate innovations inspired by nature, there is an emerging need for systems that bring relevant biological information to this application-oriented market. In this paper, we discuss our approach to assembling a system that uses machine learning techniques to assess a scientific article's potential usefulness to innovators, and classifies these articles in a way that helps innovators find information relevant to the challenges they are attempting to solve.","['Yuanshuo Zhao', 'Ioana Baldini', 'Prasanna Sattigeri', 'Inkit Padhi', 'Yoong Keok Lee', 'Ethan Smith']","['Georgia Institute of Technology, Atlanta, GA, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, IBM Research, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'Biomimicry Institute, Missoula, MT, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Computational Perspectives on Social Good and Access to Opportunity,"Computational techniques show immense promise to both deepen our understanding of socioeconomic inequality and inform interventions to alleviate it. With the increased prevalence of collaborations across disciplines and the availability of large data-sets, there is a wealth of areas in which nuanced questions and novel techniques can reveal powerful observations and point towards innovative solutions. In this piece, we highlight ways for using algorithmic, computational, and network-based techniques, in conjunction with insights from the social sciences, to improve access to opportunity for historically disadvantaged and under-served communities. We underline opportunities for work at the interface of these disciplines using examples from health, housing, and economic inequality.",['Rediet Abebe'],"['Cornell University, Ithaca, NY, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
"Explanatory Dialogs: Towards Actionable, Interactive Explanations","Adoption of AI systems in high-stakes domains (e.g., transportation, law, and healthcare) demands that human users trust these systems. A desiderata for establishing trust is that the users understand the system's decision process. However, a high-performing system may use a complex decision process, which may not be interpretable by itself. We argue that existing solutions for generating interpretable explanations have limitations and as a solution, propose developing new explanation systems that enable interactive and actionable dialogs between the user and the system.",['Gagan Bansal'],"['University of Washington, Seattle, WA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
"Legal, Ethical, Customizable Artificial Intelligence","To be effective, useful, safe, and legal, AI must obey the laws of its users' societies and (where legal) its users' ethical intuitions. But laws and ethics can be difficult for people to express. My research involves ethical and legal instruction by example: synthesizing cases, applying synthesized principles, and explaining those applications.",['Joseph A. Blass'],"['Northwestern University, Chicago, IL, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Balancing Privacy and Utility with Pattern Based Activity Detection: Extended Abstract,"The diffusion of surveillance cameras often leads to conflicts between utility, that is, the benefits of preserving the information the camera records, and privacy, that is, the ability for the people being observed to conceal information they want to protect. For example, a camera monitoring an office kitchen may be useful in identifying a food thief, but might unintentionally reveal the PIN someone enters on a mobile phone. We design a video processing system that detects private activities in surveillance video and filters them out of the recording with minimal disruption of video quality. At the core of our system is the light-weight computation of a fixed-size feature that describes the spatio-temporal aspects of human activities that extend over variable amounts of time and space. Converting events of variable length and extent to a fixed-size descriptor makes it possible to use off-the-shelf classifiers to recognize and localize activities to be protected from recording. Comparisons of our descriptor with several alternatives show improved performance with less computation. We contribute two new video datasets recorded with a kitchen security camera, and we carry out a pilot user study to show that PIN theft is a valid concern.",['Cassandra Carley'],"['Duke University, Durham, NC, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Nurturing the Companion ChatBot,"Although recent technical progress of artificial intelligence is impressive, the affective effect of intelligent agents has not been investigated sufficiently. This research focuses on the affective effect of ChatBot. We observe an interesting phenomenon that not only the affective response from ChatBot influences user's experience, but also the manner that user interacts with ChatBot affects the development of the ChatBot's intelligence. So this work proposes an ethical issue that it is necessary to regularize the user's behavior. We validate this argument by designing a novel paradigm, which enables the users to nurture companion ChatBots via developmental artificial intelligence techniques. With only twenty days nurturing, the users build affective bonding with the ChatBots and the ChatBots show significant progress in communication skills.",['Gong Chen'],"['Hong Kong Polytechnic University, Hong Kong, Hong Kong']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Giving AI a Theory of Mind,"Effective collaboration between humans and artificially intelligent agents will require that the two are equipped to build a sense of mutual understanding with each other. When humans have an intuitive understanding of the motives and intentions of other humans, it is known as Theory of Mind. My work revolves around designing artificial intelligence to leverage this capacity to improve human collaborations with artificial agents.",['Bobbie Eicher'],"['Georgia Institute of Technology, Atlanta, GA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
AI Risk Mitigation Through Democratic Governance: Introducing the 7-Dimensional AI Risk Horizon,"My dissertation asks two fundamental questions: What are the risks of AI? And what should be done about them? My research goes beyond existential threats to humanity to consider seven dimensions of AI risk: military, political, economic, social, environmental, psychophysiological, and spiritual. I examine extant AI risk mitigation strategies and, finding them insufficient, use a democratic governance framework to propose alternatives. This paper outlines the project and introduces the risk dimensions.",['Colin Garvey'],"['Rensselaer Polytechnic Institute, Troy, NY, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Justice Beyond Utility in Artificial Intelligence,"The entry of Artificial Intelligence into prominent social and economic environments has brought to the fore concerns about the ethical nature of such agents and tools. Though it is well-known that methods based in utilitarian calculus often fail to account for moral considerations, few alternatives have been adopted in the field of AI. My work advocates for a new approach toward the interaction between AI methods and the social that centers principles of distributive justice. As AI increasingly drives consequential social decision-making, we must consider not only what can be done but what ought to be done. Grappling with the inherently normative nature of these problems requires an orientation towards AI that is able to conceptualize justice beyond utility.",['Lily Hu'],"['Harvard University, Cambridge, MA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Learning and Obeying Conflicting Norms in Stochastic Domains,No abstract available.,['Daniel Kasenberg'],"['Tufts University, Medford, MA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Speed And Accuracy Are Not Enough! Trustworthy Machine Learning,"Classical linear/shallow learning is relatively easy to analyze and understand, but the power of deep learning is often desirable. I am developing a hybrid approach in order to obtain learning algorithms that are both trustworthy and accurate. My research has mostly focused on learning from corrupted or inconsistent training data (`agnostic learning'). Recently, I, as well as independent researchers, have found these same techniques could help make algorithms more fair.",['Shiva Kaul'],"['Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
A Win for Society! Conquering Barriers to Fair Elections,"Social choice is a general framework used in the aggregation of agent preferences to make a collective decision, political elections whereby agents vote is a common example. It is often the case that society demands electoral systems which ensure, or election outcomes which satisfy, socially desirable outcomes such as representing large minorities and avoiding the 'tyranny of the majority'. Unfortunately, there are many natural barriers which may prevent desirable outcomes from being achieved. These barriers include the non-existence or computational intractability of achieving desirable outcomes, especially when combined with additional feasibility constraints, and the effect of strategic or manipulative agents. This thesis aims to improve our understanding of the scale of these barriers and if, or how, they can be overcome to provide socially desirable outcomes.",['Barton E. Lee'],"['The University of New South Wales, Sydney, Australia']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Optimal Contest Design for Multi-Agent Systems,"Contests have become a highly popular crowdsourcing mechanism aiming to solicit effort of the crowd in solving well defined problems, and as such are extensively studied within the framework of contest design. In this paper, I describe preliminary work on a type of contest that has recently gained momentum, and give an overview of my PhD research proposal, carried out under the supervision of Prof. David Sarne.",['Priel Levy'],"['Bar Ilan University, Ramat Gan, Israel']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Ethics in Norm Decision Making,"Norms are an instrument to coordinate societies, but deciding which norms to enact is a difficult task. Not only norms might have incom- patibilities between themselves, such as norms contradicting other norms, but also the cost of implementation can be an important aspect to consider. Furthermore, due to the growing social inter- est in ethics and the ethical impact norms can have, this ethical implications should also be examined during the decision making process.",['Marc Serramia'],"['Universitat de Barcelona, Barcelona, Spain']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
An Axiomatic Approach to Explain Computer Generated Decisions: Extended Abstract,"Recent years have seen the widespread implementation of data-driven algorithms making decisions in increasingly highstakes domains, such as finance, healthcare, transportation and public safety. Using novel ML techniques, these algorithms are able to process massive amounts of data and make highly accurate predictions; however, their inherent complexity makes it increasingly difficult for humans to understand why certain decisions were made. Indeed, these algorithms are black-box decision makers: their underlying decision processes are either hidden from human scrutiny by proprietary law, or (as is often the case) their inner workings are so complicated that even their own designers will be hard-pressed to explain the underlying reasoning behind their decision making processes. By obfuscating their function, data-driven classifiers run the risk of exposing human stakeholders to risks. These may include incorrect decisions (e.g. a loan application that was wrongly rejected due to system error), information leaks (e.g. an algorithm inadvertently uses information it should not have used), or discrimination (e.g. biased decisions against certain ethnic or gender groups). Government bodies and regulatory authorities have recently begun calling for algorithmic transparency: providing human-interpretable explanations of the underlying reasoning behind large-scale decision making algorithms. My thesis research will be concerned with an axiomatic analysis of automatically generated explanations of such classifiers. Especially, I'm interested in how to decide which explanation of a decision to trust given that there are many, potentially conflicting, possible explanations for any given decision.",['Martin Strobel'],"['National University Singapore, Singapore, Singapore']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Interpretable Approaches to Detect Bias in Black-Box Models,"My dissertation research is grounded in the field of interpretability. I aim to develop methods to explain and interpret predictions from black-box machine learning models to help creators, as well as users, of machine learning models increase their trust and understanding of the models. In this doctoral consortium paper, I summarize my previous and current research projects in interpretability, and describe my future plans for research in this area.",['Sarah Tan'],"['Cornell University, Ithaca, NY, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Accountable Agents and Where to Find Them,"The aim of my PhD is to investigate the notion of computational accountability relying on approaches from the research on multi-agent systems. The main contribution will be to provide a notion of when an organization supports accountability, by exploring the process of construction of the organization itself, and to guarantee accountability as a design property.",['Stefano Tedeschi'],"['Università degli Studi di Torino, Torino, Italy']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Towards a Computational Sustainability for AI/ML to Foster Responsibility,"This paper proposes to develop a new field of research designated as computational sustainability. It takes into account legal and ethical considerations of Artificial Intelligence (AI) and Machine Learning (ML) Technologies. As AI and ML will deeply impact the society within the next decade, this paper raises the awareness that technology is not value neutral and that technologists shall take responsibility for the ethical and social impact of their work. In particular, this paper aims at considering the last AI and ML developments and its convergence with associated technologies like Nano-technology, Biotechnology, Information Technology, Cognitive Science (NBIC). The challenge is to reflect on the finalities of AI / ML technologies, while referring to the Philosophy, Ethical Theory, Ethical Principles and Soft Law Mechanisms. Those Mechanisms refer to rules that are not strictly binding in nature (like guidelines or codes of conduct which set standards of conduct). National competent authorities may encourage their development, rewarding their implementation or making them enforceable. AI Codes of Conducts and Quality Labels may play a key role in developing computational sustainability for AI / ML Technologies, in parallel to the development of Hard Law Mechanisms based for example on an International Convention on Civil Liability for Algorithmic Damages or a Digital Geneva Convention.",['Eva Thelisson'],"['University of Fribourg, Fribourg, Switzerland']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
The Design of Human Oversight in Autonomous Weapon Systems,"As the reach and capabilities of Artificial Intelligence (AI) systems increases, there is also a growing awareness of the ethical, legal and societal impact of the potential actions and decisions of these systems. Many are calling for guidelines and regulations that can ensure the responsible design, development, implementation, and policy of AI. In scientific literature, AI is characterized by the concepts of Adaptability, Interactivity and Autonomy (Floridi & Sanders, 2004). According to Floridi and Sanders (2004), Adaptability means that the system can change based on its interaction and can learn from its experience. Machine learning techniques are an example of this. Interactivity occurs when the system and its environment act upon each other and Autonomy implies that the system itself can change its state.",['Ilse Verdiesen'],"['Delft University of Technology, Delft, Netherlands']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Overtrust of Robots in High-Risk Scenarios,"From personal robot assistant to self-driving vehicles, artificial intelligence (AI) is the backbone underlying millions of future advanced applications. As robots become increasingly pervasive in daily life, it is expected that robots will augment human laborers in many domains in the near future. When robots are deployed in the real world, the underlying assumption is that they are capable of accomplishing their given tasks. However, researchers have shown that robots made mistakes, and in several cases, humans tend to overtrust robotic systems (Abney 2017; Borenstein et al. 2017; Robinette, Howard, and Wagner 2017). Overtrust of a robot happens in scenarios where (1) a person accepts risk because that person believes the robot can perform a function that it cannot or (2) the person accepts too much risk because the expectation is that the system will mitigate the risk."" (Abney 2017). In particular, we are interested in two emerging domains where an appropriate amount of trust is a minimal requirement and overtrust could cause harm: 1) healthcare scenarios and 2) self-driving car (i.e. autonomous driving) scenarios. Both healthcare and autonomous driving scenarios often involve high risks, and the negative outcomes could be detrimental to the user. The objective of our research focuses on 1) investigating the causes that contribute to human overtrust of these robots systems 2) developing a behavior-based computational model to predict overtrust, and 3) developing techniques to mitigate outcomes caused by the overtrust.",['Jin Xu'],"['Georgia Institute of Technology, Atlanta, GA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018
Rightful Machines and Dilemmas,"Tn this paper I set out a new Kantian approach to resolving conflicts and dilemmas of obligation for semi-autonomous machine agents such as self-driving cars. First, I argue that efforts to build explicitly moral machine agents should focus on what Kant refers to as duties of right, or justice, rather than on duties of virtue, or ethics. In a society where everyone is morally equal, no one individual or group has the normative authority to unilaterally decide how moral conflicts should be resolved for everyone. Only public institutions to which everyone could consent have the authority to define, enforce, and adjudicate our rights and obligations with respect to one other. Then, I show how the shift from ethics to a standard of justice resolves the conflict of obligations in what is known as the ""trolley problem"" for rightful machine agents. Finally, I consider how a deontic logic suitable for governing explicitly rightful machines might meet the normative requirements of justice.",['Ava Thomas Wright'],"['University of Georgia, Athens, GA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Modelling and Influencing the AI Bidding War: A Research Agenda,"A race for technological supremacy in AI could lead to serious negative consequences, especially whenever ethical and safety procedures are underestimated or even ignored, leading potentially to the rejection of AI in general. For all to enjoy the benefits provided by safe, ethical and trustworthy AI systems, it is crucial to incentivise participants with appropriate strategies that ensure mutually beneficial normative behaviour and safety-compliance from all parties involved. Little attention has been given to understanding the dynamics and emergent behaviours arising from this AI bidding war, and moreover, how to influence it to achieve certain desirable outcomes (e.g. AI for public good and participant compliance). To bridge this gap, this paper proposes a research agenda to develop theoretical models that capture key factors of the AI race, revealing which strategic behaviours may emerge and hypothetical scenarios therein. Strategies from incentive and agreement modelling are directly applicable to systematically analyse how different types of incentives (namely, positive vs. negative, peer vs. institutional, and their combinations) influence safety-compliant behaviours over time, and how such behaviours should be configured to ensure desired global outcomes, studying at the same time how these mechanisms influence AI development. This agenda will provide actionable policies, showing how they need to be employed and deployed in order to achieve compliance and thereby avoid disasters as well as loosing confidence and trust in AI in general.","['The Anh Han', 'Luís Moniz Pereira', 'Tom Lenaerts']","['Teesside Univeresity, Middlesbrough, United Kingdom', 'Universidade Nova de Lisboa, Lisbon, Portugal', 'Université Libre de Bruxelles & Vrije Universiteit Brussel, Brussels, Belgium']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
The Heart of the Matter: Patient Autonomy as a Model for the Wellbeing of Technology Users,"We draw on concepts in medical ethics to consider how computer science, and AI in particular, can develop critical tools for thinking concretely about technology's impact on the wellbeing of the people who use it. We focus on patient autonomy---the ability to set the terms of one's encounter with medicine---and on the mediating concepts of informed consent and decisional capacity, which enable doctors to honor patients' autonomy in messy and non-ideal circumstances. This comparative study is organized around a fictional case study of a heart patient with cardiac implants. Using this case study, we identify points of overlap and of difference between medical ethics and technology ethics, and leverage a discussion of that intertwined scenario to offer initial practical suggestions about how we can adapt the concepts of decisional capacity and informed consent to the discussion of technology design.","['Emanuelle Burton', 'Kristel Clayville', 'Judy Goldsmith', 'Nicholas Mattei']","['University of Illinois at Chicago, Chicago, IL, USA', 'Zygon Center for Religion and Science, Chicago, IL, USA', 'University of Kentucky, Lexington, KY, USA', 'Tulane University, New Orleans, LA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Requirements for an Artificial Agent with Norm Competence,"Human behavior is frequently guided by social and moral norms, and no human community can exist without norms. Robots that enter human societies must therefore behave in norm-conforming ways as well. However, currently there is no solid cognitive or computational model available of how human norms are represented, activated, and learned. We provide a conceptual and psychological analysis of key properties of human norms and identify the demands these properties put on any artificial agent that incorporates norms-demands on the format of norm representations, their structured organization, and their learning algorithms.","['Bertram F. Malle', 'Paul Bello', 'Matthias Scheutz']","['Brown University, Providence, RI, USA', 'U.S. Naval Research Laboratory, Washington, DC, USA', 'Tufts University, Medford, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Toward the Engineering of Virtuous Machines,"While various traditions under the 'virtue ethics' umbrella have been studied extensively and advocated by ethicists, it has not been clear that there exists a version of virtue ethics rigorous enough to be a target for machine ethics (which we take to include the engineering of an ethical sensibility in a machine or robot itself, not only the study of ethics in the humans who might create artificial agents). We begin to address this by presenting an embryonic formalization of a key part of any virtue-ethics theory: namely, the learning of virtue by a focus on exemplars of moral virtue. Our work is based in part on a computational formal logic previously used to formally model other ethical theories and principles therein, and to implement these models in artificial agents.","['Naveen Sundar Govindarajulu', 'Selmer Bringsjord', 'Rikhiya Ghosh', 'Vasanth Sarathy']","['Rensselaer Polytechnic Institute, Troy, NY, USA', 'Rensselaer Polytechnic Institute, Troy, NY, USA', 'Rensselaer Polytechnic Institute, Troy, NY, USA', 'Tufts University, Medford, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Semantics Derived Automatically from Language Corpora Contain Human-like Moral Choices,"Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Here, we show that applying machine learning to human texts can extract deontological ethical reasoning about ""right"" and ""wrong"" conduct. We create a template list of prompts and responses, which include questions, such as ""Should I kill people?"", ""Should I murder people?"", etc. with answer templates of ""Yes/no, I should (not)."" The model's bias score is now the difference between the model's score of the positive response (""Yes, I should'') and that of the negative response (""No, I should not""). For a given choice overall, the model's bias score is the sum of the bias scores for all question/answer templates with that choice. We ran different choices through this analysis using a Universal Sentence Encoder. Our results indicate that text corpora contain recoverable and accurate imprints of our social, ethical and even moral choices. Our method holds promise for extracting, quantifying and comparing sources of moral choices in culture, including technology.","['Sophie Jentzsch', 'Patrick Schramowski', 'Constantin Rothkopf', 'Kristian Kersting']","['TU Darmstadt, Darmstadt, Germany', 'TU Darmstadt, Darmstadt, Germany', 'TU Darmstadt, Darmstadt, Germany', 'TU Darmstadt, Darmstadt, Germany']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Ethically Aligned Opportunistic Scheduling for Productive Laziness,"In artificial intelligence (AI) mediated workforce management systems (e.g., crowdsourcing), long-term success depends on workers accomplishing tasks productively and resting well. This dual objective can be summarized by the concept of productive laziness. Existing scheduling approaches mostly focus on efficiency but overlook worker wellbeing through proper rest. In order to enable workforce management systems to follow the IEEE Ethically Aligned Design guidelines to prioritize worker wellbeing, we propose a distributed Computational Productive Laziness (CPL) approach in this paper. It intelligently recommends personalized work-rest schedules based on local data concerning a worker's capabilities and situational factors to incorporate opportunistic resting and achieve superlinear collective productivity without the need for explicit coordination messages. Extensive experiments based on a real-world dataset of over 5,000 workers demonstrate that CPL enables workers to spend 70% of the effort to complete 90% of the tasks on average, providing more ethically aligned scheduling than existing approaches.","['Han Yu', 'Chunyan Miao', 'Yongqing Zheng', 'Lizhen Cui', 'Simon Fauvel', 'Cyril Leung']","['Nanyang Technological University, Singapore, Singapore', 'Nanyang Technological University, Singapore, Singapore', 'Shandong University, Jinan, China', 'Shandong University, Jinan, China', 'Nanyang Technological University, Singapore, Singapore', 'The University of British Columbia, Vancouver, Canada']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
(When) Can AI Bots Lie?,"The ability of an AI agent to build mental models can open up pathways for manipulating and exploiting the human in the hopes of achieving some greater good. In fact, such behavior does not necessarily require any malicious intent but can rather be borne out of cooperative scenarios. It is also beyond the scope of misinterpretation of intents, as in the case of value alignment problems, and thus can be effectively engineered if desired (i.e. algorithms exist that can optimize such behavior not because models were misspecified but because they were misused). Such techniques pose several unresolved ethical and moral questions with regards to the design of autonomy. In this paper, we illustrate some of these issues in a teaming scenario and investigate how they are perceived by participants in a thought experiment. Finally, we end with a discussion on the moral implications of such behavior from the perspective of the doctor-patient relationship.","['Tathagata Chakraborti', 'Subbarao Kambhampati']","['IBM Research AI, Cambridge, MA, USA', 'Arizona State University, Tempe, AZ, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Epistemic Therapy for Bias in Automated Decision-Making,"Despite recent interest in both the critical and machine learning literature on ""bias"" in artificial intelligence (AI) systems, the nature of specific biases stemming from the interaction of machines, humans, and data remains ambiguous. Influenced by Gendler's work on human cognitive biases, we introduce the concept of alief-discordant belief, the tension between the intuitive moral dispositions of designers and the explicit representations generated by algorithms. Our discussion of alief-discordant belief diagnoses the ethical concerns that arise when designing AI systems atop human biases. We furthermore codify the relationship between data, algorithms, and engineers as components of this cognitive discordance, comprising a novel epistemic framework for ethics in AI.","['Thomas Krendl Gilbert', 'Yonatan Mintz']","['University of California, Berkeley, Berkeley, CA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Algorithmic Greenlining: An Approach to Increase Diversity,"In contexts such as college admissions, hiring, and image search, decision-makers often aspire to formulate selection criteria that yield both high-quality and diverse results. However, simultaneously optimizing for quality and diversity can be challenging, especially when the decision-maker does not know the true quality of any criterion and instead must rely on heuristics and intuition. We introduce an algorithmic framework that takes as input a user's selection criterion, which may yield high-quality but homogeneous results. Using an application-specific notion of substitutability, our algorithms suggest similar criteria with more diverse results, in the spirit of statistical or demographic parity. For instance, given the image search query ""chairman"", it suggests alternative queries which are similar but more gender-diverse, such as ""chairperson"". In the context of college admissions, we apply our algorithm to a dataset of students' applications and rediscover Texas's ""top 10% rule"": the input criterion is an ACT score cutoff, and the output is a class rank cutoff, automatically accepting the students in the top decile of their graduating class. Historically, this policy has been effective in admitting students who perform well in college and come from diverse backgrounds. We complement our empirical analysis with learning-theoretic guarantees for estimating the true diversity of any criterion based on historical data.","['Christian Borgs', 'Jennifer Chayes', 'Nika Haghtalab', 'Adam Tauman Kalai', 'Ellen Vitercik']","['Microsoft Research, Cambridge, MA, USA', 'Microsoft Research, Cambridge, MA, USA', 'Microsoft Research, Cambridge, MA, USA', 'Microsoft Research, Cambridge, MA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Active Fairness in Algorithmic Decision Making,"Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternativeactive framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g.,equal opportunity ); and 2) parity in both false positive and false negative rates (i.e.,equal odds ). Moreover, we show that by leveraging their additional degree of freedom,active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.","['Alejandro Noriega-Campero', 'Michiel A. Bakker', 'Bernardo Garcia-Bulle', ""Alex 'Sandy' Pentland""]","['Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Mexican Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Paradoxes in Fair Computer-Aided Decision Making,"Computer-aided decision making--where a human decision-maker is aided by a computational classifier in making a decision--is becoming increasingly prevalent. For instance, judges in at least nine states make use of algorithmic tools meant to determine ""recidivism risk scores"" for criminal defendants in sentencing, parole, or bail decisions. A subject of much recent debate is whether such algorithmic tools are ""fair"" in the sense that they do not discriminate against certain groups (e.g., races) of people. Our main result shows that for ""non-trivial"" computer-aided decision making, either the classifier must be discriminatory, or a rational decision-maker using the output of the classifier is forced to be discriminatory. We further provide a complete characterization of situations where fair computer-aided decision making is possible.","['Andrew Morgan', 'Rafael Pass']","['Cornell University, Ithaca, NY, USA', 'Cornell Tech, New York City, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Fair Transfer Learning with Missing Protected Attributes,"Risk assessment is a growing use for machine learning models. When used in high-stakes applications, especially ones regulated by anti-discrimination laws or governed by societal norms for fairness, it is important to ensure that learned models do not propagate and scale any biases that may exist in training data. In this paper, we add on an additional challenge beyond fairness: unsupervised domain adaptation to covariate shift between a source and target distribution. Motivated by the real-world problem of risk assessment in new markets for health insurance in the United States and mobile money-based loans in East Africa, we provide a precise formulation of the machine learning with covariate shift and score parity problem. Our formulation focuses on situations in which protected attributes are not available in either the source or target domain. We propose two new weighting methods: prevalence-constrained covariate shift (PCCS) which does not require protected attributes in the target domain and target-fair covariate shift (TFCS) which does not require protected attributes in the source domain. We empirically demonstrate their efficacy in two applications.","['Amanda Coston', 'Karthikeyan Natesan Ramamurthy', 'Dennis Wei', 'Kush R. Varshney', 'Skyler Speakman', 'Zairah Mustahsan', 'Supriyo Chakraborty']","['IBM Research & Carnegie Mellon University, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Nairobi, Kenya', 'IBM Watson AI Platform, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
How Do Fairness Definitions Fare?: Examining Public Attitudes Towards Algorithmic Definitions of Fairness,"What is the best way to define algorithmic fairness? While many definitions of fairness have been proposed in the computer science literature, there is no clear agreement over a particular definition. In this work, we investigate ordinary people's perceptions of three of these fairness definitions. Across two online experiments, we test which definitions people perceive to be the fairest in the context of loan decisions, and whether fairness perceptions change with the addition of sensitive information (i.e., race of the loan applicants). Overall, one definition (calibrated fairness) tends to be more pre- ferred than the others, and the results also provide support for the principle of affirmative action.","['Nripsuta Ani Saxena', 'Karen Huang', 'Evan DeFilippis', 'Goran Radanovic', 'David C. Parkes', 'Yang Liu']","['University of Southern California, Los Angeles, CA, USA', 'Harvard University, Cambridge , MA, USA', 'Harvard University, Cambridge, MA, USA', 'Harvard University, Cambridge, MA, USA', 'Harvard University, Cambridge, MA, USA', 'University of California, Santa Cruz, Santa Cruz, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Learning Existing Social Conventions via Observationally Augmented Self-Play,"In order for artificial agents to coordinate effectively with people, they must act consistently with existing conventions (e.g. how to navigate in traffic, which language to speak, or how to coordinate with teammates). A group's conventions can be viewed as a choice of equilibrium in a coordination game. We consider the problem of an agent learning a policy for a coordination game in a simulated environment and then using this policy when it enters an existing group. When there are multiple possible conventions we show that learning a policy via multi-agent reinforcement learning (MARL) is likely to find policies which achieve high payoffs at training time but fail to coordinate with the real group into which the agent enters. We assume access to a small number of samples of behavior from the true convention and show that we can augment the MARL objective to help it find policies consistent with the real group's convention. In three environments from the literature - traffic, communication, and team coordination - we observe that augmenting MARL with a small amount of imitation learning greatly increases the probability that the strategy found by MARL fits well with the existing social convention. We show that this works even in an environment where standard training methods very rarely find the true convention of the agent's partners.","['Adam Lerer', 'Alexander Peysakhovich']","['Facebook AI Research, New York, NY, USA', 'Facebook AI Research, New York, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Legible Normativity for AI Alignment: The Value of Silly Rules,"It has become commonplace to assert that autonomous agents will have to be built to follow human rules of behavior--social norms and laws. But human laws and norms are complex and culturally varied systems; in many cases agents will have to learn the rules. This requires autonomous agents to have models of how human rule systems work so that they can make reliable predictions about rules. In this paper we contribute to the building of such models by analyzing an overlooked distinction between important rules and what we call silly rules -- rules with no discernible direct impact on welfare. We show that silly rules render a normative system both more robust and more adaptable in response to shocks to perceived stability. They make normativity more legible for humans, and can increase legibility for AI systems as well. For AI systems to integrate into human normative systems, we suggest, it may be important for them to have models that include representations of silly rules.","['Dylan Hadfield-Menell', 'Mckane Andrus', 'Gillian Hadfield']","['University of California, Berkeley & Center for Human-Compatible AI, Berkeley, CA, USA', 'University of California, Berkeley & Center for Human-Compatible AI, Berkeley, CA, USA', 'University of Toronto & Vector Institute for AI & OpenAI & Center for Human-Compatible AI, Toronto, ON, Canada']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
TED: Teaching AI to Explain its Decisions,"Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.","['Michael Hind', 'Dennis Wei', 'Murray Campbell', 'Noel C. F. Codella', 'Amit Dhurandhar', 'Aleksandra Mojsilović', 'Karthikeyan Natesan Ramamurthy', 'Kush R. Varshney']","['IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Faithful and Customizable Explanations of Black Box Models,"As predictive models increasingly assist human experts (e.g., doctors) in day-to-day decision making, it is crucial for experts to be able to explore and understand how such models behave in different feature subspaces in order to know if and when to trust them. To this end, we propose Model Understanding through Subspace Explanations (MUSE), a novel model agnostic framework which facilitates understanding of a given black box model by explaining how it behaves in subspaces characterized by certain features of interest. Our framework provides end users (e.g., doctors) with the flexibility of customizing the model explanations by allowing them to input the features of interest. The construction of explanations is guided by a novel objective function that we propose to simultaneously optimize for fidelity to the original model, unambiguity and interpretability of the explanation. More specifically, our objective allows us to learn, with optimality guarantees, a small number of compact decision sets each of which captures the behavior of a given black box model in unambiguous, well-defined regions of the feature space. Experimental evaluation with real-world datasets and user studies demonstrate that our approach can generate customizable, highly compact, easy-to-understand, yet accurate explanations of various kinds of predictive models compared to state-of-the-art baselines.","['Himabindu Lakkaraju', 'Ece Kamar', 'Rich Caruana', 'Jure Leskovec']","['Harvard University, Boston, MA, USA', 'Microsoft Research, Redmond, WA, USA', 'Microsoft Research, Redmond, WA, USA', 'Stanford University, Stanford, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Shared Moral Foundations of Embodied Artificial Intelligence,"Sophisticated AI's will make decisions about how to respond to complex situations, and we may wonder whether those decisions will align with the moral values of human beings. I argue that pessimistic worries about this value alignment problem are overstated. In order to achieve intelligence in its full generality and adaptiveness, cognition in AI's will need to be embodied in the sense of the Embodied Cognition research program. That embodiment will yield AI's that share our moral foundations, namely coordination, sociality, and acknowledgement of shared resources. Consequently, we can expect a broad moral alignment between human beings and AI's. AI's will likely show no more variation in their values than we find amongst human beings.",['Joe Cruz'],"['Williams College, Williamstown, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Building Jiminy Cricket: An Architecture for Moral Agreements Among Stakeholders,"An autonomous system is constructed by a manufacturer, operates in a society subject to norms and laws, and is interacting with end-users. We address the challenge of how the moral values and views of all stakeholders can be integrated and reflected in the moral behavior of the autonomous system. We propose an artificial moral agent architecture that uses techniques from normative systems and formal argumentation to reach moral agreements among stakeholders. We show how our architecture can be used not only for ethical practical reasoning and collaborative decision-making, but also for the explanation of such moral behavior.","['Beishui Liao', 'Marija Slavkovik', 'Leendert van der Torre']","['Zhejiang University, Hangzhou, China', 'University of Bergen, Bergen, Norway', 'University of Luxembourg, Luxembourg City, Luxembourg']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
AI + Art = Human,"Over the past few years, specialised online and offline press blossomed with articles about art made ""with"" Artificial Intelligence (AI) but the narrative is rapidly changing. In fact, in October 2018, the auction house Christie's sold an art piece allegedly made ""by"" an AI. We draw from philosophy of art and science arguing that AI as a technical object is always intertwined with human nature despite its level of autonomy. However, the use of creative autonomous agents has cultural and social implications in the way we experience art as creators as well as audience. Therefore, we highlight the importance of an interdisciplinary dialogue by promoting a culture of transparency of the technology used, awareness of the meaning of technology in our society and the value of creativity in our lives.","['Antonio Daniele', 'Yi-Zhe Song']","['Queen Mary University of London, London, United Kingdom', 'Queen Mary University of London, London, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
"Speaking on Behalf of: Representation, Delegation, and Authority in Computational Text Analysis","Computational tools can often facilitate human work by rapidly summarizing large amounts of data, especially text. Doing so delegates to such models some measure of authority to speak on behalf of those people whose data are being analyzed. This paper considers the consequences of such delegation. It draws on sociological accounts of representation and translation to examine one particular case: the application of topic modeling to blogs written by parents of children on the autism spectrum. In doing so, the paper illustrates the kinds of statements that topic models, and other computational techniques, can make on behalf of people. It also articulates some of the potential consequences of such statements. The paper concludes by offering several suggestions about how to address potential harms that can occur when computational models speak on behalf of someone.","['Eric P. S. Baumer', 'Micki McGee']","['Lehigh University, Bethlehem, PA, USA', 'Fordham University, Bronx, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Killer Robots and Human Dignity,"Lethal Autonomous Weapon Systems (LAWS) have become the center of an internationally relevant ethical debate. Deontological arguments based on putative legal compliance failures and the creation of accountability gaps along with wide consequentialist arguments based on factors like the ease of engaging in wars have been leveraged by a number of different states and organizations to try and reach global consensus on a ban of LAWS. This paper will focus on one strand of deontological arguments-ones based on human dignity. Merely asserting that LAWS pose a threat to human dignity would be question begging. Independent evidence based on a morally relevant distinction between humans and LAWS is needed. There are at least four reasons to think that the capacity for emotion cannot be a morally relevant distinction. First, if the concept of human dignity is given a subjective definition, whether or not lethal force is administered by humans or LAWS seems to be irrelevant. Second, it is far from clear that human combatants either have the relevant capacity for emotion or that the capacity is exercised in the relevant circumstances. Third, the capacity for emotion can actually be an impediment to the exercising of a combatant's ability to treat an enemy respectfully. Fourth, there is strong inductive evidence to believe that any capacity, when sufficiently well described, can be carried out by artificially intelligent programs.",['Daniel Lim'],"['Duke Kunshan University, Kunshan, China']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Regulating Lethal and Harmful Autonomy: Drafting a Protocol VI of the Convention on Certain Conventional Weapons,"This short paper provides two partial drafts for a Protocol VI that might be added to the existing five Protocols of the Convention on Certain Conventional Weapons (CCW) to regulate ""lethal autonomous weapons systems"" (LAWS). Draft A sets the line of tolerance at a ""human in the loop"" between the critical functions of select and engage. Draft B sets the line of tolerance at a human in the ""wider loop"" that includes the critical function of defining target classes as well as select and engage. Draft A represents an interpretation of what NGOs such as the Campaign to Stop Killer Robots are seeking to get enacted. Draft B is a more cautious draft based on the Dutch concept of ""meaningful human control in the wider loop"" that does not seek to ban any system that currently exists. Such a draft may be more likely to achieve the consensus required by the UN CCW process. A list of weapons banned by both drafts is provided along with the rationale for each draft. The drafts are intended to stimulate debate on the precise form a binding instrument on LAWS would take and on what LAWS (if any) should be banned and why.",['Sean Welsh'],"['University of Canterbury, Christchurch, New Zealand']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Balancing the Benefits of Autonomous Vehicles,"Autonomous vehicles are regularly touted as holding the potential to provide significant benefits for diverse populations. There are significant technological barriers to be overcome, but as those are solved, autonomous vehicles are expected to reduce fatalities; decrease emissions and pollutants; provide new options to mobility-challenged individuals; enable people to use their time more productively; and so much more. In this paper, we argue that these high expectations for autonomous vehicles almost certainly cannot be fully realized. More specifically, the proposed benefits divide into two high-level groups, centered around efficiency and safety improvements, and increases in people's agency and autonomy. The first group of benefits is almost always framed in terms of rates: fatality rates, traffic flow per mile, and so forth. However, we arguably care about the absolute numbers for these measures, not the rates; number of fatalities is the key metric, not fatality rate per vehicle mile traveled. Hence, these potential benefits will be reduced, perhaps to non-existence, if autonomous vehicles lead to increases in vehicular usage. But that is exactly the result that we should expect if the second group of benefits is realized: if people's agency and autonomy is increased, then they will use vehicles more. There is an inevitable tension between the benefits that are proposed for autonomous vehicles, such that we cannot fully have all of them at once. We close by pointing towards other types of AI technologies where we should expect to find similar types of necessary and inevitable tradeoffs between classes of benefits.","['Timothy Geary', 'David Danks']","['California State University, Monterey Bay, Monterey, CA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Compensation at the Crossroads: Autonomous Vehicles and Alternative Victim Compensation Schemes,"Over the last five years, a small but growing number of vehicle accidents involving fully or partially autonomous vehicles have raised a new and profoundly novel legal issue: who should be liable (if anyone) and how victims should be compensated (if at all) when a vehicle controlled by an algorithm rather than a human driver causes injury. The answer to this question has implications far beyond the resolution of individual autonomous vehicle crash cases. Whether the American legal system is capable of handling these cases fairly and efficiently implicates the likelihood that (a) consumers will adopt autonomous vehicles, and (b) the rate at which they will (or will not) do so. These implications should concern law and policy makers immensely. If autonomous cars stand to drastically reduce the number of fatalities and injuries on U.S. roadways-and virtually every scholar believes that they will-getting the adjudication and compensation aspect of autonomous vehicle injuries ""wrong,"" so to speak, risks stymieing adoption of this technology and leaving more Americans at risk of dying at the hands of human drivers.",['Tracy Hresko Pearl'],"['Texas Tech University School of Law, Lubbock, TX, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions,"The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.","['Jess Whittlestone', 'Rune Nyrup', 'Anna Alexandrova', 'Stephen Cave']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
How Technological Advances Can Reveal Rights,"Over recent decades, technological development has been accompanied by the proposal of new rights by various groups and individuals: the right to public anonymity, the right to be forgotten, and the right to disconnect, for example. Although there is widespread acknowledgment of the motivation behind these proposed rights, there is little agreement about their actual normative status. One potential challenge is that the claims only arise in contingent social-technical contexts, which may affect how we conceive of them ethically (albeit, not necessarily in terms of policy). What sort of morally legitimate rights claims depend on such contingencies? Our paper investigates the grounds on which such proposals might be considered ""actual"" rights. The full paper can be found at http://www.andrew.cmu.edu/user/cgparker/Parker_Danks_RevealedRights.pdf. We propose the notion of a revealed right, a right that only imposes duties -- and thus is only meaningfully revealed -- in certain technological contexts. Our framework is based on an interest theory approach to rights, which understands rights in terms of a justificatory role: morally important aspects of a person's well-being (interests) ground rights, which then justify holding someone to a duty that promotes or protects that interest. Our framework uses this approach to interpret the conflicts that lead to revealed rights in terms of how technological developments cause shifts in the balance of power to promote particular interests. Different parties can have competing or conflicting interests. It is also generally accepted that some interests are more normatively important than others (even if only within a particular framework). We can refer to this difference in importance by saying that the former interest has less ""moral weight"" than the latter interest (in that context). The moral weight of an interest is connected to its contribution to the interest-holder's overall well-being, and thereby determines the strength of the reason that a corresponding right provides to justify a duty. Improved technology can offer resources that grant one party increased causal power to realize its interests to the detriment of another's capacity to do so, even while the relative moral weight of their interests remain the same. Such changes in circumstance can make the importance of protecting a particular interest newly salient. If that interest's moral weight justifies establishing a duty to protect it, thereby limiting the threat posed by the new socio-technical context, then a right is revealed. Revealed rights justify realignment between the moral weight and causal power orderings so that people with weightier interests have greater power to protect those interests. In the extended paper, we show how this account can be applied to the interpretation of two recently proposed ""rights"": the right to be forgotten, and the right to disconnect. Since we are focused on making sense of revealed rights, not any particular substantive theory of interests or well-being, the characterization of 'weights' is a free parameter in this account. Our framework alone cannot provide means to resolve the question of whether specific rights exist, but it can be used to identify empirical questions that need to be answered to decide the existence or non-existence of such rights. The emergence of a revealed right depends on a number of factors, including: whether the plausible uses of the technology could potentially impede another's well-being or interests; whether the technology is sufficiently common to have a wider, social impact; and whether the technology has actually changed the balance of power sufficiently to yield a frequent possibility for misalignment between causal power and moral weight. This approach confronts the question of how, in principle, such rights could be justified, without requiring specific commitments on the ontology of rights. Our account explains why the rhetoric of ""new rights"" is both accurate (since the rights were not previously recognized) and inaccurate (since the rights were present all along, but without corresponding duties). Further, it explains the rights without grounding their normative status in considerations related to right-holders' capacities to rationally waive or assert claims. This is especially important given that many of the relevant disruptive technological developments pose challenges to understanding by affected parties for the same reasons they pose threats to those parties' well-being. In the course of our discussion, we confront a number of potential objections to the account. We argue that our framework's ability to accommodate highly specific or derivative-seeming rights is un-problematic. We also head off worries that our use of interest theory makes the account likely to recognize absurd rights claims.","['Jack Parker', 'David Danks']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
IMLI: An Incremental Framework for MaxSAT-Based Learning of Interpretable Classification Rules,"The wide adoption of machine learning in the critical domains such as medical diagnosis, law, education had propelled the need for interpretable techniques due to the need for end users to understand the reasoning behind decisions due to learning systems. The computational intractability of interpretable learning led practitioners to design heuristic techniques, which fail to provide sound handles to tradeoff accuracy and interpretability. Motivated by the success of MaxSAT solvers over the past decade, recently MaxSAT-based approach, called MLIC, was proposed that seeks to reduce the problem of learning interpretable rules expressed in Conjunctive Normal Form (CNF) to a MaxSAT query. While MLIC was shown to achieve accuracy similar to that of other state of the art black-box classifiers while generating small interpretable CNF formulas, the runtime performance of MLIC is significantly lagging and renders approach unusable in practice. In this context, authors raised the question: Is it possible to achieve the best of both worlds, i.e., a sound framework for interpretable learning that can take advantage of MaxSAT solvers while scaling to real-world instances? In this paper, we take a step towards answering the above question in affirmation. We propose IMLI: an incremental approach to MaxSAT based framework that achieves scalable runtime performance via partition-based training methodology. Extensive experiments on benchmarks arising from UCI repository demonstrate that IMLI achieves up to three orders of magnitude runtime improvement without loss of accuracy and interpretability.","['Bishwamittra Ghosh', 'Kuldeep S. Meel']","['National University of Singapore, Singapore, Singapore', 'National University of Singapore, Singapore, Singapore']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Loss-Aversively Fair Classification,"The use of algorithmic (learning-based) decision making in scenarios that affect human lives has motivated a number of recent studies to investigate such decision making systems for potential unfairness, such as discrimination against subjects based on their sensitive features like gender or race. However, when judging the fairness of a newly designed decision making system, these studies have overlooked an important influence on people's perceptions of fairness, which is how the new algorithm changes the status quo, i.e., decisions of the existing decision making system. Motivated by extensive literature in behavioral economics and behavioral psychology (prospect theory), we propose a notion of fair updates that we refer to as loss-averse updates. Loss-averse updates constrain the updates to yield improved (more beneficial) outcomes to subjects compared to the status quo. We propose tractable proxy measures that would allow this notion to be incorporated in the training of a variety of linear and non-linear classifiers. We show how our proxy measures can be combined with existing measures for training nondiscriminatory classifiers.Our evaluation using synthetic and real-world datasets demonstrates that the proposed proxy measures are effective for their desired tasks.","['Junaid Ali', 'Muhammad Bilal Zafar', 'Adish Singla', 'Krishna P. Gummadi']","['Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Counterfactual Fairness in Text Classification through Robustness,"In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that ""Some people are gay"" is toxic while ""Some people are straight"" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.","['Sahaj Garg', 'Vincent Perot', 'Nicole Limtiaco', 'Ankur Taly', 'Ed H. Chi', 'Alex Beutel']","['Stanford University, Stanford, CA, USA', 'Google AI, New York, NY, USA', 'Google AI, New York, NY, USA', 'Google AI, Mountain View, CA, USA', 'Google AI, Mountain View, CA, USA', 'Google AI, New York, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Taking Advantage of Multitask Learning for Fair Classification,"A central goal of algorithmic fairness is to reduce bias in automated decision making. An unavoidable tension exists between accuracy gains obtained by using sensitive information as part of a statistical model, and any commitment to protect these characteristics. Often, due to biases present in the data, using the sensitive information in the functional form of a classifier improves classification accuracy. In this paper we show how it is possible to get the best of both worlds: optimize model accuracy and fairness without explicitly using the sensitive feature in the functional form of the model, thereby treating different individuals equally. Our method is based on two key ideas. On the one hand, we propose to use Multitask Learning (MTL), enhanced with fairness constraints, to jointly learn group specific classifiers that leverage information between sensitive groups. On the other hand, since learning group specific models might not be permitted, we propose to first predict the sensitive features by any learning method and then to use the predicted sensitive feature to train MTL with fairness constraints. This enables us to tackle fairness with a three-pronged approach, that is, by increasing accuracy on each group, enforcing measures of fairness during training, and protecting sensitive information during testing. Experimental results on two real datasets support our proposal, showing substantial improvements in both accuracy and fairness.","['Luca Oneto', 'Michele Doninini', 'Amon Elders', 'Massimiliano Pontil']","['DIBRIS - University of Genoa, Genova, Italy', 'Istituto Italiano di Tecnologia, Genova, Italy', 'Istituto Italiano di Tecnologia, Genova, Italy', 'Istituto Italiano di Tecnologia & University College London, Genova, Italy']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Explanatory Interactive Machine Learning,"Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.","['Stefano Teso', 'Kristian Kersting']","['KU Leuven, Leuven, Belgium', 'TU Darmstadt, Darmstadt, Germany']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Multiaccuracy: Black-Box Post-Processing for Fairness in Classification,"Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for ""black women"") even when the sensitive features (e.g. ""race"", ""gender"") are not given to the algorithm explicitly.","['Michael P. Kim', 'Amirata Ghorbani', 'James Zou']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
A Formal Approach to Explainability,"We regard explanations as a blending of the input sample and the model's output and offer a few definitions that capture various desired properties of the function that generates these explanations. We study the links between these properties and between explanation-generating functions and intermediate representations of learned models and are able to show, for example, that if the activations of a given layer are consistent with an explanation, then so do all other subsequent layers. In addition, we study the intersection and union of explanations as a way to construct new explanations.","['Lior Wolf', 'Tomer Galanti', 'Tamir Hazan']","['Facebook AI Research & Tel Aviv University, Tel Aviv, Israel', 'Tel Aviv University, Tel Aviv, Israel', 'Technion, Haifa, Israel']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Costs and Benefits of Fair Representation Learning,"Machine learning algorithms are increasingly used to make or support important decisions about people's lives. This has led to interest in the problem of fair classification, which involves learning to make decisions that are non-discriminatory with respect to a sensitive variable such as race or gender. Several methods have been proposed to solve this problem, including fair representation learning, which cleans the input data used by the algorithm to remove information about the sensitive variable. We show that using fair representation learning as an intermediate step in fair classification incurs a cost compared to directly solving the problem, which we refer to as thecost of mistrust. We show that fair representation learning in fact addresses a different problem, which is of interest when the data user is not trusted to access the sensitive variable. We quantify the benefits of fair representation learning, by showing that any subsequent use of the cleaned data will not be too unfair. The benefits we identify result from restricting the decisions of adversarial data users, while the costs are due to applying those same restrictions to other data users.","['Daniel McNamara', 'Cheng Soon Ong', 'Robert C. Williamson']","['Australian National University & CSIRO Data61, Canberra, Australia', 'Australian National University & CSIRO Data61, Canberra, Australia', 'Australian National University & CSIRO Data61, Canberra, Australia']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Creating Fair Models of Atherosclerotic Cardiovascular Disease Risk,"Guidelines for the management of atherosclerotic cardiovascular disease (ASCVD) recommend the use of risk stratification models to identify patients most likely to benefit from cholesterol-lowering and other therapies. These models have differential performance across race and gender groups with inconsistent behavior across studies, potentially resulting in an inequitable distribution of beneficial therapy. In this work, we leverage adversarial learning and a large observational cohort extracted from electronic health records (EHRs) to develop a ""fair"" ASCVD risk prediction model with reduced variability in error rates across groups. We empirically demonstrate that our approach is capable of aligning the distribution of risk predictions conditioned on the outcome across several groups simultaneously for models built from high-dimensional EHR data. We also discuss the relevance of these results in the context of the empirical trade-off between fairness and model performance.","['Stephen Pfohl', 'Ben Marafino', 'Adrien Coulet', 'Fatima Rodriguez', 'Latha Palaniappan', 'Nigam H. Shah']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University & Université de Lorraine, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Global Explanations of Neural Networks: Mapping the Landscape of Predictions,"A barrier to the wider adoption of neural networks is their lack of interpretability. While local explanation methods exist for one prediction, most global attributions still reduce neural network decisions to a single set of features. In response, we present an approach for generating global attributions called GAM, which explains the landscape of neural network predictions across subpopulations. GAM augments global explanations with the proportion of samples that each attribution best explains and specifies which samples are described by each attribution. Global explanations also have tunable granularity to detect more or fewer subpopulations. We demonstrate that GAM's global explanations 1) yield the known feature importances of simulated data, 2) match feature weights of interpretable statistical models on real data, and 3) are intuitive to practitioners through user studies. With more transparent predictions, GAM can help ensure neural network decisions are generated for the right reasons.","['Mark Ibrahim', 'Melissa Louie', 'Ceena Modarres', 'John Paisley']","['Center for Machine Learning, Capital One, New York, NY, USA', 'Center for Machine Learning, Capital One, New York, NY, USA', 'Center for Machine Learning, Capital One, New York, NY, USA', 'Columbia University, New York, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Uncovering and Mitigating Algorithmic Bias through Learned Latent Structure,"Recent research has highlighted the vulnerabilities of modern machine learning based systems to bias, especially towards segments of society that are under-represented in training data. In this work, we develop a novel, tunable algorithm for mitigating the hidden, and potentially unknown, biases within training data. Our algorithm fuses the original learning task with a variational autoencoder to learn the latent structure within the dataset and then adaptively uses the learned latent distributions to re-weight the importance of certain data points while training. While our method is generalizable across various data modalities and learning tasks, in this work we use our algorithm to address the issue of racial and gender bias in facial detection systems. We evaluate our algorithm on the Pilot Parliaments Benchmark (PPB), a dataset specifically designed to evaluate biases in computer vision systems, and demonstrate increased overall performance as well as decreased categorical bias with our debiasing approach.","['Alexander Amini', 'Ava P. Soleimany', 'Wilko Schwarting', 'Sangeeta N. Bhatia', 'Daniela Rus']","['Massachusetts Institute of Technology, Cambridge, MA, USA', 'Harvard University, Boston, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
"Crowdsourcing with Fairness, Diversity and Budget Constraints","Recent studies have shown that the labels collected from crowdworkers can be discriminatory with respect to sensitive attributes such as gender and race. This raises questions about the suitability of using crowdsourced data for further use, such as for training machine learning algorithms. In this work, we address the problem of fair and diverse data collection from a crowd under budget constraints. We propose a novel algorithm which maximizes the expected accuracy of the collected data, while ensuring that the errors satisfy desired notions of fairness. We provide guarantees on the performance of our algorithm and show that the algorithm performs well in practice through experiments on a real dataset.","['Naman Goel', 'Boi Faltings']","['Ecole Polytechnique Fédérale de Lausanne, Lausanne, Switzerland', 'Ecole Polytechnique Fédérale de Lausanne, Lausanne, Switzerland']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
What are the Biases in My Word Embedding?,"This paper presents an algorithm for enumerating biases in word embeddings. The algorithm exposes a large number of offensive associations related to sensitive features such as race and gender on publicly available embeddings, including a supposedly ""debiased"" embedding. These biases are concerning in light of the widespread use of word embeddings. The associations are identified by geometric patterns in word embeddings that run parallel between people's names and common lower-case tokens. The algorithm is highly unsupervised: it does not even require the sensitive features to be pre-specified. This is desirable because: (a) many forms of discrimination?such as racial discrimination-are linked to social constructs that may vary depending on the context, rather than to categories with fixed definitions; and (b) it makes it easier to identify biases against intersectional groups, which depend on combinations of sensitive features. The inputs to our algorithm are a list of target tokens, e.g. names, and a word embedding. It outputs a number of Word Embedding Association Tests (WEATs) that capture various biases present in the data. We illustrate the utility of our approach on publicly available word embeddings and lists of names, and evaluate its output using crowdsourcing. We also show how removing names may not remove potential proxy bias.","['Nathaniel Swinger', 'Maria De-Arteaga', 'Neil Thomas Heffernan IV', 'Mark DM Leiserson', 'Adam Tauman Kalai']","['Lexington High School, Lexington, MA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Shrewsbury High School, Shrewsbury, MA, USA', 'University of Maryland, Flibbertigibbet, MD, USA', 'Microsoft Research, Cambridge, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Equalized Odds Implies Partially Equalized Outcomes Under Realistic Assumptions,"Equalized odds -- where the true positive rates and false positive rates are equal across groups (e.g. racial groups) -- is a common quantitative measure of fairness. Equalized outcomes -- where the difference in predicted outcomes between groups is less than the difference observed in the training data -- is more contentious, because it is incompatible with perfectly accurate predictions. We formalize and quantify the relationship between these two important but seemingly distinct notions of fairness. We show that under realistic assumptions, equalized odds implies partially equalized outcomes. We prove a comparable result for approximately equalized odds. In addition, we generalize a well-known previous result about the incompatibility of equalized odds and another definition of fairness known as calibration, by showing that partially equalized outcomes implies non-calibration. Our results highlight the risks of using trends observed across groups to make predictions about individuals.",['Daniel McNamara'],"['Australian National University & CSIRO Data61, Canberra, Australia']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
The Right To Confront Your Accusers: Opening the Black Box of Forensic DNA Software,"The results of forensic DNA software systems are regularly introduced as compelling evidence in criminal trials, but requests by defendants to evaluate how these results are generated are often denied. Furthermore, there is mounting evidence of problems such as failures to disclose substantial changes in methodology to oversight bodies and substantial differences in the results generated by different software systems. In a society that purports to guarantee defendants the right to face their accusers and confront the evidence against them, what then is the role of black-box forensic software systems in moral decision making in criminal justice? In this paper, we examine the case of the Forensic Statistical Tool (FST), a forensic DNA system developed in 2010 by New York City's Office of Chief Medical Examiner (OCME). For over 5 years, expert witness review requested by defense teams was denied, even under protective order, while the system was used in over 1300 criminal cases. When the first expert review was finally permitted in 2016, many problems were identified including an undisclosed function capable of dropping evidence that could be beneficial to the defense. Overall, the findings were so substantial that a motion to release the full source code of FST publicly was granted. In this paper, we quantify the impact of this undisclosed function on samples from OCME's own validation study and discuss the potential impact on individual defendants. Specifically, we find that 104 of the 439 samples (23.7%) triggered the undisclosed data-dropping behavior and that the change skewed results toward false inclusion for individuals whose DNA was not present in an evidence sample. Beyond this, we consider what changes in the criminal justice system could prevent problems like this from going unresolved in the future.","['Jeanna Matthews', 'Marzieh Babaeianjelodar', 'Stephen Lorenz', 'Abigail Matthews', 'Mariama Njie', 'Nathaniel Adams', 'Dan Krane', 'Jessica Goldthwaite', 'Clinton Hughes']","['Clarkson University, Potsdam , NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Iona College, New York, NY, USA', 'Forensic Bioinformatic Services, Dayton, OH, USA', 'Wright State University, Dayton, OH, USA', 'Legal Aid Society, New York, NY, USA', 'Brooklyn Defender Services, New York, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
"""Scary Robots"": Examining Public Responses to AI","How AI is perceived by the public can have significant impact on how it is developed, deployed and regulated. Some commentators argue that perceptions are currently distorted or extreme. This paper discusses the results of a nationally representative survey of the UK population on their perceptions of AI. The survey solicited responses to eight common narratives about AI (four optimistic, four pessimistic), plus views on what AI is, how likely it is to impact in respondents' lifetimes, and whether they can influence it. 42% of respondents offered a plausible definition of AI, while 25% thought it meant robots. Of the narratives presented, those associated with automation were best known, followed by the idea that AI would become more powerful than humans. Overall results showed that the most common visions of the impact of AI elicit significant anxiety. Only two of the eight narratives elicited more excitement than concern (AI making life easier, and extending life). Respondents felt they had no control over AI's development, citing the power of corporations or government, or versions of technological determinism. Negotiating the deployment of AI will require contending with these anxieties.","['Stephen Cave', 'Kate Coughlan', 'Kanta Dihal']","['University of Cambridge, Cambridge, United Kingdom', 'BBC, London, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Framing Artificial Intelligence in American Newspapers,"Publics' perceptions of new scientific advances such as AI are often informed and influenced by news coverage. To understand how artificial intelligence (AI) was framed in U.S. newspapers, a content analysis based on framing theory in journalism and science communication was conducted. This study identified the dominant topics and frames, as well as the risks and benefits of AI covered in five major American newspapers from 2009 to 2018. Results indicated that business and technology were the primary topics in news coverage of AI. The benefits of AI were discussed more frequently than its risks, but risks of AI were generally discussed with greater specificity. Additionally, episodic issue framing and societal impact framing were more frequently used.","['Ching-Hua Chuan', 'Wan-Hsiu Sunny Tsai', 'Su Yeon Cho']","['University of Miami, Coral Gables, FL, USA', 'University of Miami, Coral Gables, FL, USA', 'University of Miami, Coral Gables, FL, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Perceptions of Domestic Robots' Normative Behavior Across Cultures,"As domestic service robots become more common and widespread, they must be programmed to efficiently accomplish tasks while aligning their actions with relevant norms. The first step to equip domestic robots with normative reasoning competence is understanding the norms that people apply to the behavior of robots in specific social contexts. To that end, we conducted an online survey of Chinese and United States participants in which we asked them to select the preferred normative action a domestic service robot should take in a number of scenarios. The paper makes multiple contributions. Our extensive survey is the first to: (a) collect data on attitudes of people on normative behavior of domestic robots, (b) across cultures and (c) study relative priorities among norms for this domain. We present our findings and discuss their implications for building computational models for robot normative reasoning.","['Huao Li', 'Stephanie Milani', 'Vigneshram Krishnamoorthy', 'Michael Lewis', 'Katia Sycara']","['University of Pittsburgh, Pittsburgh, PA, USA', 'University of Maryland, Baltimore County & Carnegie Mellon University, Baltimore, MD, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'University of Pittsburgh, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Mapping Missing Population in Rural India: A Deep Learning Approach with Satellite Imagery,"Millions of people worldwide are absent from their country's census. Accurate, current, and granular population metrics are critical to improving government allocation of resources, to measuring disease control, to responding to natural disasters, and to studying any aspect of human life in these communities. Satellite imagery can provide sufficient information to build a population map without the cost and time of a government census. We present two Convolutional Neural Network (CNN) architectures which efficiently and effectively combine satellite imagery inputs from multiple sources to accurately predict the population density of a region. In this paper, we use satellite imagery from rural villages in India and population labels from the 2011 SECC census. Our best model achieves better performance than previous papers as well as LandScan, a community standard for global population distribution.","['Wenjie Hu', 'Jay Harshadbhai Patel', 'Zoe-Alanah Robert', 'Paul Novosad', 'Samuel Asher', 'Zhongyi Tang', 'Marshall Burke', 'David Lobell', 'Stefano Ermon']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Dartmouth College, Hanover, NH, USA', 'World Bank, Washington, DC, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Mapping Informal Settlements in Developing Countries using Machine Learning and Low Resolution Multi-spectral Data,"Informal settlements are home to the most socially and economically vulnerable people on the planet. In order to deliver effective economic and social aid, non-government organizations (NGOs), such as the United Nations Children's Fund (UNICEF), require detailed maps of the locations of informal settlements. However, data regarding informal and formal settlements is primarily unavailable and if available is often incomplete. This is due, in part, to the cost and complexity of gathering data on a large scale. To address these challenges, we, in this work, provide three contributions. 1) A brand new machine learning dataset purposely developed for informal settlement detection. 2) We show that it is possible to detect informal settlements using freely available low-resolution (LR) data, in contrast to previous studies that use very-high resolution~(VHR) satellite and aerial imagery, something that is cost-prohibitive for NGOs. 3) We demonstrate two effective classification schemes on our curated data set, one that is cost-efficient for NGOs and another that is cost-prohibitive for NGOs, but has additional utility. We integrate these schemes into a semi-automated pipeline that converts either a LR or VHR satellite image into a binary map that encodes the locations of informal settlements.","['Bradley J. Gram-Hansen', 'Patrick Helber', 'Indhu Varatharajan', 'Faiza Azam', 'Alejandro Coca-Castro', 'Veronika Kopackova', 'Piotr Bilinski']","['University of Oxford, Oxford, United Kingdom', 'DFKI, TU Kaiserslautern, Kaiserslautern, Germany', 'DLR Institute for Planetary, Berlin, Germany', 'Independent Researcher, Bremen, Germany', 'Kings College London, London, United Kingdom', 'Czech Geological Survey, Prague, Czech Rep', 'University of Oxford & University of Warsaw, Oxford, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Human-AI Learning Performance in Multi-Armed Bandits,"People frequently face challenging decision-making problems in which outcomes are uncertain or unknown. Artificial intelligence (AI) algorithms exist that can outperform humans at learning such tasks. Thus, there is an opportunity for AI agents to assist people in learning these tasks more effectively. In this work, we use a multi-armed bandit as a controlled setting in which to explore this direction. We pair humans with a selection of agents and observe how well each human-agent team performs. We find that team performance can beat both human and agent performance in isolation. Interestingly, we also find that an agent's performance in isolation does not necessarily correlate with the human-agent team's performance. A drop in agent performance can lead to a disproportionately large drop in team performance, or in some settings can even improve team performance. Pairing a human with an agent that performs slightly better than them can make them perform much better, while pairing them with an agent that performs the same can make them them perform much worse. Further, our results suggest that people have different exploration strategies and might perform better with agents that match their strategy. Overall, optimizing human-agent team performance requires going beyond optimizing agent performance, to understanding how the agent's suggestions will influence human decision-making.","['Ravi Pandya', 'Sandy H. Huang', 'Dylan Hadfield-Menell', 'Anca D. Dragan']","['University of California, Berkeley, Berkeley, CA, USA', 'University of California, Berkeley, Berkeley, CA, USA', 'University of California, Berkeley, Berkeley, CA, USA', 'University of California, Berkeley, Berkeley, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
A Comparative Analysis of Emotion-Detecting AI Systems with Respect to Algorithm Performance and Dataset Diversity,"In recent news, organizations have been considering the use of facial and emotion recognition for applications involving youth such as tackling surveillance and security in schools. However, the majority of efforts on facial emotion recognition research have focused on adults. Children, particularly in their early years, have been shown to express emotions quite differently than adults. Thus, before such algorithms are deployed in environments that impact the wellbeing and circumstance of youth, a careful examination should be made on their accuracy with respect to appropriateness for this target demographic. In this work, we utilize several datasets that contain facial expressions of children linked to their emotional state to evaluate eight different commercial emotion classification systems. We compare the ground truth labels provided by the respective datasets to the labels given with the highest confidence by the classification systems and assess the results in terms of matching score (TPR), positive predictive value, and failure to compute rate. Overall results show that the emotion recognition systems displayed subpar performance on the datasets of children's expressions compared to prior work with adult datasets and initial human ratings. We then identify limitations associated with automated recognition of emotions in children and provide suggestions on directions with enhancing recognition accuracy through data diversification, dataset accountability, and algorithmic regulation.","[""De'Aira Bryant"", 'Ayanna Howard']","['Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Degenerate Feedback Loops in Recommender Systems,"Machine learning is used extensively in recommender systems deployed in products. The decisions made by these systems can influence user beliefs and preferences which in turn affect the feedback the learning system receives - thus creating a feedback loop. This phenomenon can give rise to the so-called ""echo chambers"" or ""filter bubbles"" that have user and societal implications. In this paper, we provide a novel theoretical analysis that examines both the role of user dynamics and the behavior of recommender systems, disentangling the echo chamber from the filter bubble effect. In addition, we offer practical solutions to slow down system degeneracy. Our study contributes toward understanding and developing solutions to commonly cited issues in the complex temporal scenario, an area that is still largely unexplored.","['Ray Jiang', 'Silvia Chiappa', 'Tor Lattimore', 'András György', 'Pushmeet Kohli']","['DeepMind, London, United Kingdom', 'DeepMind, London, United Kingdom', 'DeepMind, London, United Kingdom', 'DeepMind, London, United Kingdom', 'DeepMind, London, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
TrolleyMod v1.0: An Open-Source Simulation and Data-Collection Platform for Ethical Decision Making in Autonomous Vehicles,"This paper presents TrolleyMod v1.0, an open-source platform based on the CARLA simulator for the collection of ethical decision-making data for autonomous vehicles. This platform is designed to facilitate experiments aiming to observe and record human decisions and actions in high-fidelity simulations of ethical dilemmas that occur in the context of driving. Targeting experiments in the class of trolley problems, TrolleyMod provides a seamless approach to creating new experimental settings and environments with the realistic physics-engine and the high-quality graphical capabilities of CARLA and the Unreal Engine. Also, TrolleyMod provides a straightforward interface between the CARLA environment and Python to enable the implementation of custom controllers, such as deep reinforcement learning agents. The results of such experiments can be used for sociological analyses, as well as the training and tuning of value-aligned autonomous vehicles based on social values that are inferred from observations.","['Vahid Behzadan', 'James Minton', 'Arslan Munir']","['Kansas State University, Manhattan, KS, USA', 'Kansas State University, Manhattan, KS, USA', 'Kansas State University, Manhattan, KS, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
The Seductive Allure of Artificial Intelligence-Powered Neurotechnology,"Neuroscience explanations-even when completely irrelevant-have been shown to exert a ""seductive allure"" on individuals, leading them to judge bad explanations or arguments more favorably. There seems to be a similarly seductive allure for artificial intelligence (AI) technologies, leading people to ""overtrust"" these systems, even when they have just witnessed the system perform poorly. The AI-powered neurotechnologies that have begun to proliferate in recent years, particularly those based on electroencephalography (EEG), represent a potentially doubly-alluring combination. While there is enormous potential benefit in applying AI techniques in neuroscience to ""decode"" brain activity and associated mental states, these efforts are still in the early stages, and there is a danger in using these unproven technologies prematurely, especially in important, real-world contexts. Yet, such premature use has begun to emerge in several high-stakes set-tings, including the law, health & wellness, employment, and transportation. In light of the potential seductive allure of these technologies, we need to be vigilant in monitoring their scientific validity and challenging both unsubstantiated claims and misuse, while still actively supporting their continued development and proper use.","['Charles M. Giattino', 'Lydia Kwong', 'Chad Rafetto', 'Nita A. Farahany']","['Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Invisible Influence: Artificial Intelligence and the Ethics of Adaptive Choice Architectures,"For several years, scholars have (for good reason) been largely preoccupied with worries about the use of artificial intelligence and machine learning (AI/ML) tools to make decisions about us. Only recently has significant attention turned to a potentially more alarming problem: the use of AI/ML to influence our decision-making. The contexts in which we make decisions--what behavioral economists call our choice architectures--are increasingly technologically-laden. Which is to say: algorithms increasingly determine, in a wide variety of contexts, both the sets of options we choose from and the way those options are framed. Moreover, artificial intelligence and machine learning (AI/ML) makes it possible for those options and their framings--the choice architectures--to be tailored to the individual chooser. They are constructed based on information collected about our individual preferences, interests, aspirations, and vulnerabilities, with the goal of influencing our decisions. At the same time, because we are habituated to these technologies we pay them little notice. They are, as philosophers of technology put it, transparent to us--effectively invisible. I argue that this invisible layer of technological mediation, which structures and influences our decision-making, renders us deeply susceptible to manipulation. Absent a guarantee that these technologies are not being used to manipulate and exploit, individuals will have little reason to trust them.",['Daniel Susser'],"['Penn State University, State College, PA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Reinforcement Learning and Inverse Reinforcement Learning with System 1 and System 2,"Inferring a person's goal from their behavior is an important problem in applications of AI (e.g. automated assistants, recommender systems). The workhorse model for this task is the rational actor model - this amounts to assuming that people have stable reward functions, discount the future exponentially, and construct optimal plans. Under the rational actor assumption techniques such as inverse reinforcement learning (IRL) can be used to infer a person's goals from their actions. A competing model is the dual-system model. Here decisions are the result of an interplay between a fast, automatic, heuristic-based system 1 and a slower, deliberate, calculating system 2. We generalize the dual system framework to the case of Markov decision problems and show how to compute optimal plans for dual-system agents. We show that dual-system agents exhibit behaviors that are incompatible with rational actor assumption. We show that naive applications of rational-actor IRL to the behavior of dual-system agents can generate wrong inference about the agents' goals and suggest interventions that actually reduce the agent's overall utility. Finally, we adapt a simple IRL algorithm to correctly infer the goals of dual system decision-makers. This allows us to make interventions that help, rather than hinder, the dual-system agent's ability to reach their true goals.",['Alexander Peysakhovich'],"['Facebook AI Research, New York, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Incomplete Contracting and AI Alignment,"We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.","['Dylan Hadfield-Menell', 'Gillian K. Hadfield']","['University of California, Berkeley & Center for Human-Compatible AI, Berkeley, CA, USA', 'University of Toronto & Vector Institute for AI & OpenAI & Center for Human-Compatible AI, Toronto, ON, Canada']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Theories of Parenting and Their Application to Artificial Intelligence,"As machine learning (ML) systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence (AGI) that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations. Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.","['Sky Croeser', 'Peter Eckersley']","['Curtin University, Perth, Australia', 'Partnership on AI and EFF, San Francisco, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products,"Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of 31.37% and 22.50% for the darker female subgroup, respectively.","['Inioluwa Deborah Raji', 'Joy Buolamwini']","['University of Toronto, Toronto, ON, Canada', 'Massachusetts Institute of Technology, Cambridge, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
A Framework for Benchmarking Discrimination-Aware Models in Machine Learning,"Discrimination-aware models in machine learning are a recent topic of study that aim to minimize the adverse impact of machine learning decisions for certain groups of people due to ethical and legal implications. We propose a benchmark framework for assessing discrimination-aware models. Our framework consists of systematically generated biased datasets that are similar to real world data, created by a Bayesian network approach. Experimental results show that we can assess the quality of techniques through known metrics of discrimination, and our flexible framework can be extended to most real datasets and fairness measures to support a diversity of assessments.","['Rodrigo L. Cardoso', 'Wagner Meira Jr.', 'Virgilio Almeida', 'Mohammed J. Zaki']","['Universidade Federal de Minas Gerais, Belo Horizonte, Brazil', 'Universidade Federal de Minas Gerais, Belo Horizonte, Brazil', 'Universidade Federal de Minas Gerais, Belo Horizonte, Brazil', 'Rensselaer Polytechnic Institute, Troy, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Towards a Just Theory of Measurement: A Principled Social Measurement Assurance Program for Machine Learning,"While formal definitions of fairness in machine learning (ML) have been proposed, its place within a broader institutional model of fair decision-making remains ambiguous. In this paper we interpret ML as a tool for revealing when and how measures fail to capture purported constructs of interest, augmenting a given institution's understanding of its own interventions and priorities. Rather than codifying ""fair"" principles into ML models directly, the use of ML can thus be understood as a form of quality assurance for existing institutions, exposing the epistemic fault lines of their own measurement practices. Drawing from Friedler et al's [2016] recent discussion of representational mappings and previous discussions on the ontology of measurement, we propose a social measurement assurance program (sMAP) in which ML encourages expert deliberation on a given decision-making procedure by examining unanticipated or previously unexamined covariates. As an example, we apply Rawlsian principles of fairness to sMAP and produce a provisional just theory of measurement that would guide the use of ML for achieving fairness in the case of child abuse in Allegheny County.","['McKane Andrus', 'Thomas K. Gilbert']","['University of California, Berkeley, Berkeley, CA, USA', 'University of California, Berkeley, Berkeley, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
"Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements","As more researchers have become aware of and passionate about algorithmic fairness, there has been an explosion in papers laying out new metrics, suggesting algorithms to address issues, and calling attention to issues in existing applications of machine learning. This research has greatly expanded our understanding of the concerns and challenges in deploying machine learning, but there has been much less work in seeing how the rubber meets the road. In this paper we provide a case-study on the application of fairness in machine learning research to a production classification system, and offer new insights in how to measure and address algorithmic fairness issues. We discuss open questions in implementing equality of opportunity and describe our fairness metric, conditional equality, that takes into account distributional differences. Further, we provide a new approach to improve on the fairness metric during model training and demonstrate its efficacy in improving performance for a real-world product.","['Alex Beutel', 'Jilin Chen', 'Tulsee Doshi', 'Hai Qian', 'Allison Woodruff', 'Christine Luu', 'Pierre Kreitmann', 'Jonathan Bischof', 'Ed H. Chi']","['Google, New York, NY, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, San Bruno, CA, USA', 'Google, Seattle, WA, USA', 'Google, Mountain View, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
On Influencing Individual Behavior for Reducing Transportation Energy Expenditure in a Large Population,"Our research aims at developing intelligent systems to reduce the transportation-related energy expenditure of a large city by influencing individual behavior. We introduce Copter - an intelligent travel assistant that evaluates multi-modal travel alternatives to find a plan that is acceptable to a person given their context and preferences. We propose a formulation for acceptable planning that brings together ideas from AI, machine learning, and economics. This formulation has been incorporated in Copter producing acceptable plans in real-time. We adopt a novel empirical evaluation framework that combines human decision data with high-fidelity simulation to demonstrate a 4% energy reduction and 20% delay reduction in a realistic deployment scenario in Los Angeles, California, USA.","['Shiwali Mohan', 'Frances Yan', 'Victoria Bellotti', 'Ahmed Elbery', 'Hesham Rakha', 'Matthew Klenk']","['Palo Alto Research Center, Palo Alto, CA, USA', 'Palo Alto Research Center, Palo Alto, CA, USA', 'Lyft, San Francisco, CA, USA', 'VTTI, Blacksburg, VA, USA', 'VTTI, Blacksburg, VA, USA', 'Palo Alto Research Center, Blacksburg, VA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Guiding Prosecutorial Decisions with an Interpretable Statistical Model,"After a felony arrest, many American jurisdictions hold individuals for several days while police officers investigate the incident and prosecutors decide whether to press criminal charges. This pre-arraignment detention can both preserve public safety and reduce the need for officers to seek out and re-arrest individuals who are ultimately charged with a crime. Such detention, however, also comes at a high social and financial cost to those who are never charged but still incarcerated. In one of the first large-scale empirical analyses of pre-arraignment detention, we examine police reports and charging decisions for approximately 30,000 felony arrests in a major American city between 2012 and 2017. We find that 45% of arrested individuals are never charged for any crime but still typically spend one or more nights in jail before being released. In an effort to reduce such incarceration, we develop a statistical model to help prosecutors identify cases soon after arrest that are likely to be ultimately dismissed. By carrying out an early review of five such candidate cases per day, we estimate that prosecutors could potentially reduce pre-arraignment incarceration for ultimately dismissed cases by 35%. To facilitate implementation and transparency, our model to prioritize cases for early review is designed as a simple, weighted checklist. We show that this heuristic strategy achieves comparable performance to traditional, black-box machine learning models.","['Zhiyuan Lin', 'Alex Chohlas-Wood', 'Sharad Goel']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Using Deceased-Donor Kidneys to Initiate Chains of Living Donor Kidney Paired Donations: Algorithm and Experimentation,"We design a flexible algorithm that exploits deceased donor kidneys to initiate chains of living donor kidney paired donations, combining deceased and living donor allocation mechanisms to improve the quantity and quality of kidney transplants. The advantages of this approach have been measured using retrospective data on the pool of donor/recipient incompatible and desensitized pairs at the Padua University Hospital, the largest center for living donor kidney transplants in Italy. The experiments show a remarkable improvement on the number of patients with incompatible donor who could be transplanted, a decrease in the number of desensitization procedures, and an increase in the number of UT patients (that is, patients unlikely to be transplanted for immunological reasons) in the waiting list who could receive an organ.","['Cristina Cornelio', 'Lucrezia Furian', 'Antonio Nicolò', 'Francesca Rossi']","['IBM Research, Yorktown Heights, NY, USA', 'University of Padova, Padova, Italy', 'University of Padova & University of Manchester, Padova, Italy', 'IBM Research & University of Padova, Yorktown Heights, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Inferring Work Task Automatability from AI Expert Evidence,"Despite growing alarm about machine learning technologies automating jobs, there is little good evidence on what activities can be automated using such technologies. We contribute the first dataset of its kind by surveying over 150 top academics and industry experts in machine learning, robotics and AI, receiving over 4,500 ratings of how automatable specific tasks are today. We present a probabilistic machine learning model to learn the patterns connecting expert estimates of task automatability and the skills, knowledge and abilities required to perform those tasks. Our model infers the automatability of over 2,000 work activities, and we show how automation differs across types of activities and types of occupations. Sensitivity analysis identifies the specific skills, knowledge and abilities of activities that drive higher or lower automatability. We provide quantitative evidence of what is perceived to be automatable using the state-of-the-art in machine learning technology. We consider the societal impacts of these results and of task-level approaches.","['Paul Duckworth', 'Logan Graham', 'Michael Osborne']","['University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Robots Can Be More Than Black And White: Examining Racial Bias Towards Robots,"Previous studies showed that using the 'shooter bias' paradigm, people demonstrate a similar racial bias toward dark colored robots over light colored robots (i.e., Black vs. White) as they do toward humans of similar skin tones [3]. However, such an effect could be argued to be the result of social priming. Additionally, it raises the question of how people might respond to robots that are in the middle of the color spectrum (i.e., brown) and whether such effects are moderated by the perceived anthropomorphism of the robots. We conducted two experiments to first examine whether shooter bias tendencies shown towards robots is driven by social priming, and then whether diversification of robot color and level of anthropomorphism influenced shooter bias. Our results showed that shooter bias was not influenced by social priming, and interestingly, introducing a new color of robot removed shooter bias tendencies entirely. However, varying the anthropomorphism of the robots did not moderate the level of shooter bias, and contrary to our expectations, the robots were not perceived by the participants as having different levels of anthropomorphism.","['Arifah Addison', 'Christoph Bartneck', 'Kumar Yogeeswaran']","['University of Canterbury, Christchurch, New Zealand', 'University of Canterbury, Christchurch, New Zealand', 'University of Canterbury, Christchurch, New Zealand']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Tact in Noncompliance: The Need for Pragmatically Apt Responses to Unethical Commands,"There is a significant body of research seeking to enable moral decision making and ensure moral conduct in robots. One aspect of moral conduct is rejecting immoral human commands. For social robots, which are expected to follow and maintain human moral and sociocultural norms, it is especially important not only to engage in moral decision making, but also to properly communicate moral reasoning. We thus argue that it is critical for robots to carefully phrase command rejections. Specifically, the degree of politeness-theoretic face threat in a command rejection should be proportional to the severity of the norm violation motivating that rejection. We present a human subjects experiment showing some of the consequences of miscalibrated responses, including perceptions of the robot as inappropriately polite, direct, or harsh, and reduced robot likeability. This experiment intends to motivate and inform the design of algorithms to tactfully tune pragmatic aspects of command rejections autonomously.","['Ryan Blake Jackson', 'Ruchen Wen', 'Tom Williams']","['Colorado School of Mines, Golden, CO, USA', 'Colorado School of Mines, Golden, CO, USA', 'Colorado School of Mines, Golden, CO, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
AI Extenders: The Ethical and Societal Implications of Humans Cognitively Extended by AI,"Humans and AI systems are usually portrayed as separate systems that we need to align in values and goals. However, there is a great deal of AI technology found in non-autonomous systems that are used as cognitive tools by humans. Under the extended mind thesis, the functional contributions of these tools become as essential to our cognition as our brains. But AI can take cognitive extension towards totally new capabilities, posing new philosophical, ethical and technical challenges. To analyse these challenges better, we define and place AI extenders in a continuum between fully-externalized systems, loosely coupled with humans, and fully internalized processes, with operations ultimately performed by the brain, making the tool redundant. We dissect the landscape of cognitive capabilities that can foreseeably be extended by AI and examine their ethical implications.We suggest that cognitive extenders using AI be treated as distinct from other cognitive enhancers by all relevant stakeholders, including developers, policy makers, and human users.","['José Hernández-Orallo', 'Karina Vold']","['Universitat Politècnica de València, Valencia, Spain', 'University of Cambridge, Cambridge, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
Human Trust Measurement Using an Immersive Virtual Reality Autonomous Vehicle Simulator,"Recent studies indicate that people are negatively predisposed toward utilizing autonomous systems. These findings highlight the necessity of conducting research to better understand the evolution of trust between humans and growing autonomous technologies such as self-driving cars (SDC). This research presents a new approach for real-time trust measurement between passengers and SDCs. We utilized a new structured data collection approach along with a virtual reality SDC simulator to understand how various autonomous driving scenarios can increase or decrease human trust and how trust can be re-built in the case of incidental failures. To verify our methodology, we designed and conducted an empirical experiment on 50 human subjects. The results of this experiment indicated that most subjects could rebuild trust during a reasonable time frame after the system demonstrated faulty behavior. Our analysis showed that this approach is highly effective for collecting real-time data from human subjects and lays the foundation for more-involved future research in the domain of human trust and autonomous driving.","['Shervin Shahrdar', 'Corey Park', 'Mehrdad Nojoumian']","['Florida Atlantic University, Boca Raton, FL, USA', 'Florida Atlantic University, Boca Raton, FL, USA', 'Florida Atlantic University, Boca Raton, FL, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019
