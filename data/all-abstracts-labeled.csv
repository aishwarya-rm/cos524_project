title,abstract,authors,institutions,conference,date,industry
"""How do I fool you?"": Manipulating User Trust via Misleading Black Box Explanations","As machine learning black boxes are increasingly being deployed in critical domains such as healthcare and criminal justice, there has been a growing emphasis on developing techniques for explaining these black boxes in a human interpretable manner. There has been recent concern that a high-fidelity explanation of a black box ML model may not accurately reflect the biases in the black box. As a consequence, explanations have the potential to mislead human users into trusting a problematic black box. In this work, we rigorously explore the notion of misleading explanations and how they influence user trust in black box models. Specifically, we propose a novel theoretical framework for understanding and generating misleading explanations, and carry out a user study with domain experts to demonstrate how these explanations can be used to mislead users. Our work is the first to empirically establish how user trust in black box models can be manipulated via misleading explanations.","['Himabindu Lakkaraju', 'Osbert Bastani']","['Harvard University, Cambridge, MA, USA', 'University of Pennsylvania, Philadelphia, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
"""I Can’t Reply with That"": Characterizing Problematic Email Reply Suggestions","In email interfaces, providing users with reply suggestions may simplify or accelerate correspondence.  While the “success'” of such systems is typically quantified using the number of suggestions selected by users, this ignores the impact of social context, which can change how suggestions are perceived.  To address this, we developed a mixed-methods framework involving qualitative interviews and crowdsourced experiments to characterize problematic email reply suggestions.  Our interviews revealed issues with over-positive, dissonant, cultural, and gender-assuming replies, as well as contextual politeness.  In our experiments, crowdworkers assessed email scenarios that we generated and systematically controlled, showing that contextual factors like social ties and the presence of salutations impacts users’ perceptions of email correspondence.  These assessments created a novel dataset of human-authored corrections for problematic email replies. Our study highlights the social complexity of providing suggestions for email correspondence, raising issues that may apply to all social messaging systems.","['Ronald Robertson', 'Alexandra Olteanu', 'Fernando Diaz', 'Milad Shokouhi', 'Peter Bailey']",Microsoft,CHI Conference on Human Factors in Computing Systems (CHI ’21),2021-05-01,TRUE
"""Meaningful Information"" and the Right to Explanation","There is no single, neat statutory provision labeled the ""right to explanation"" in Europe's new General Data Protection Regulation (GDPR). But nor is such a right illusory. Responding to two prominent papers that, in turn, conjure and critique the right to explanation in the context of automated decision-making, we advocate a return to the text of the GDPR. Articles 13-15 provide rights to ""meaningful information about the logic involved"" in automated decisions. This is a right to explanation, whether one uses the phrase or not. The right to explanation should be interpreted functionally, flexibly, and should, at a minimum, enable a data subject to exercise his or her rights under the GDPR and human rights law. ","Andrew Selbst, Julia Powles","[""Data & Society Research Institute"", ""Yale ISP"", ""Cornell Tech"", ""NYU"", ""University of Cambridge""]",FAT* 2018,2018,FALSE
"""Scary Robots"": Examining Public Responses to AI","How AI is perceived by the public can have significant impact on how it is developed, deployed and regulated. Some commentators argue that perceptions are currently distorted or extreme. This paper discusses the results of a nationally representative survey of the UK population on their perceptions of AI. The survey solicited responses to eight common narratives about AI (four optimistic, four pessimistic), plus views on what AI is, how likely it is to impact in respondents' lifetimes, and whether they can influence it. 42% of respondents offered a plausible definition of AI, while 25% thought it meant robots. Of the narratives presented, those associated with automation were best known, followed by the idea that AI would become more powerful than humans. Overall results showed that the most common visions of the impact of AI elicit significant anxiety. Only two of the eight narratives elicited more excitement than concern (AI making life easier, and extending life). Respondents felt they had no control over AI's development, citing the power of corporations or government, or versions of technological determinism. Negotiating the deployment of AI will require contending with these anxieties.","['Stephen Cave', 'Kate Coughlan', 'Kanta Dihal']","['University of Cambridge, Cambridge, United Kingdom', 'BBC, London, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
"""The Global South is everywhere, but also always somewhere"": National Policy Narratives and AI Justice","There is more attention than ever on the social implications of AI. In contrast to universalized paradigms of ethics and fairness, a growing body of critical work highlights bias and discrimination in AI within the frame of social justice and human rights (""AI justice""). However, the geographical location of much of this critique in the West could be engendering its own blind spots. The global supply chain of AI (data, computational power, natural resources, labor) today replicates historical colonial inequities, and the continued subordination of Global South countries. This paper draws attention to official narratives from the Indian government and the United Nations Conference on Trade and Development (UNCTAD) advocating for the role (and place) of these regions in the AI economy. Domestically, these policies are being contested for their top-down formulation, and reflect narrow industry interests. This underscores the need to approach the political economy of AI from varying altitudes - global, national, and from the perspective of communities whose lives and livelihoods are most directly impacted in this economy. Without a deliberate effort at centering this conversation it is inevitable that mainstream discourse on AI justice will grow parallel to (and potentially undercut) demands emanating from Global South governments and communities",['Amba Kak'],"['AI Now Institute, New York, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
"""The human body is a black box"": supporting clinical decision-making with deep learning","Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.","['Mark Sendak', 'Madeleine Clare Elish', 'Michael Gao', 'Joseph Futoma', 'William Ratliff', 'Marshall Nichols', 'Armando Bedoya', 'Suresh Balu', ""Cara O'Brien""]","['Duke Institute for Health Innovation', 'Data & Society Research Institute', 'Duke Institute for Health Innovation', 'Harvard University and Duke University', 'Duke Institute for Health Innovation', 'Duke Institute for Health Innovation', 'Duke School of Medicine', 'Duke Institute for Health Innovation', 'Duke School of Medicine']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
“I Don't Want Someone to Watch Me While I'm Working”: Gendered Views of Facial Recognition Technology in Workplace Surveillance,"Employers are increasingly using information and communication technologies to monitor employees. Such workplace surveillance is extensive in the United States, but its experience and potential consequences differ across groups based on gender. We thus sought to identify whether self‐reported male and female employees differ in the extent to which they find the use of workplace cameras equipped with facial recognition technology (FRT) acceptable, and examine the role of privacy attitudes more generally in mediating views on workplace surveillance. Using data from a nationally representative survey conducted by the Pew Research Center, we find that women are much less likely than men to approve of the use of cameras using FRT in the workplace. We then further explore whether men and women think differently about privacy, and if perceptions of privacy moderate the relationship between gender and approval of workplace surveillance. Finally, we consider the implications of these findings for privacy and surveillance via embedded technologies, and how the consequences of surveillance and technologies like FRT may be gendered. Note: We recognize evaluations based on a binary definition of gender are invariably partial and exclusionary. As we note in our discussion of the study’s limitations, we were constrained by the survey categories provided by Pew.","['Luke Stark', 'Amanda Stanhaus', 'Denise L. Anthony']",Microsoft,Journal of the Association for Information Science and Technology,2020-03-10,TRUE
(When) Can AI Bots Lie?,"The ability of an AI agent to build mental models can open up pathways for manipulating and exploiting the human in the hopes of achieving some greater good. In fact, such behavior does not necessarily require any malicious intent but can rather be borne out of cooperative scenarios. It is also beyond the scope of misinterpretation of intents, as in the case of value alignment problems, and thus can be effectively engineered if desired (i.e. algorithms exist that can optimize such behavior not because models were misspecified but because they were misused). Such techniques pose several unresolved ethical and moral questions with regards to the design of autonomy. In this paper, we illustrate some of these issues in a teaming scenario and investigate how they are perceived by participants in a thought experiment. Finally, we end with a discussion on the moral implications of such behavior from the perspective of the doctor-patient relationship.","['Tathagata Chakraborti', 'Subbarao Kambhampati']","['IBM Research AI, Cambridge, MA, USA', 'Arizona State University, Tempe, AZ, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,TRUE
50 Years of Test (Un)fairness: Lessons for Machine Learning,"Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.","['Ben Hutchinson', 'Margaret Mitchell']","['Google', 'Google']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
A Bayesian Model of Cash Bail Decisions,"The use of cash bail as a mechanism for detaining defendants pretrial is an often-criticized system that many have argued violates the presumption of ""innocent until proven guilty."" Many studies have sought to understand both the long-term effects of cash bail's use and the disparate rate of cash bail assignments along demographic lines (race, gender, etc). However, such work is often susceptible to problems of infra-marginality - that the data we observe can only describe average outcomes, and not the outcomes associated with the marginal decision. In this work, we address this problem by creating a hierarchical Bayesian model of cash bail assignments. Specifically, our approach models cash bail decisions as a probabilistic process whereby judges balance the relative costs of assigning cash bail with the cost of defendants potentially skipping court dates, and where these skip probabilities are estimated based upon features of the individual case. We then use Monte Carlo inference to sample the distribution over these costs for different magistrates and across different races. We fit this model to a data set we have collected of over 50,000 court cases in the Allegheny and Philadelphia counties in Pennsylvania. Our analysis of 50 separate judges shows that they are uniformly more likely to assign cash bail to black defendants than to white defendants, even given identical likelihood of skipping a court appearance. This analysis raises further questions about the equity of the practice of cash bail, irrespective of its underlying legal justification.","['Joshua Williams', 'J. Zico Kolter']","['Computer Science Department, Carnegie Mellon University', 'Computer Science Department, Carnegie Mellon University Bosch Center for AI']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions,"Every year there are more than 3.6 million referrals made to child protection agencies across the US. The practice of screening calls is left to each jurisdiction to follow local practices and policies, potentially leading to large variation in the way in which referrals are treated across the country. Whilst increasing access to linked administrative data is available, it is difficult for welfare workers to make systematic use of historical information about all the children and adults on a single referral call. Risk prediction models that use routinely collected administrative data can help call workers to better identify cases that are likely to result in adverse outcomes. However, the use of predictive analytics in the area of child welfare is contentious. There is a possibility that some communities-such as those in poverty or from particular racial and ethnic groups-will be disadvantaged by the reliance on government administrative data. On the other hand, these analytics tools can augment or replace human judgments, which themselves are biased and imperfect. In this paper we describe our work on developing, validating, fairness auditing, and deploying a risk prediction model in Allegheny County, Pennsylvania, USA. We discuss the results of our analysis to-date, and also highlight key problems and data bias issues that present challenges for model evaluation and deployment.","Alexandra Chouldechova, Diana Benavides-Prado, Oleksandr Fialko, Rhema Vaithianathan","[""CMU"", ""USC"", ""Auckland University of Technology""]",FAT* 2018,2018,FALSE
A Comparative Analysis of Emotion-Detecting AI Systems with Respect to Algorithm Performance and Dataset Diversity,"In recent news, organizations have been considering the use of facial and emotion recognition for applications involving youth such as tackling surveillance and security in schools. However, the majority of efforts on facial emotion recognition research have focused on adults. Children, particularly in their early years, have been shown to express emotions quite differently than adults. Thus, before such algorithms are deployed in environments that impact the wellbeing and circumstance of youth, a careful examination should be made on their accuracy with respect to appropriateness for this target demographic. In this work, we utilize several datasets that contain facial expressions of children linked to their emotional state to evaluate eight different commercial emotion classification systems. We compare the ground truth labels provided by the respective datasets to the labels given with the highest confidence by the classification systems and assess the results in terms of matching score (TPR), positive predictive value, and failure to compute rate. Overall results show that the emotion recognition systems displayed subpar performance on the datasets of children's expressions compared to prior work with adult datasets and initial human ratings. We then identify limitations associated with automated recognition of emotions in children and provide suggestions on directions with enhancing recognition accuracy through data diversification, dataset accountability, and algorithmic regulation.","[""De'Aira Bryant"", 'Ayanna Howard']","['Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
A comparative study of fairness-enhancing interventions in machine learning,"Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption. We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought.","['Sorelle A. Friedler', 'Carlos Scheidegger', 'Suresh Venkatasubramanian', 'Sonam Choudhary', 'Evan P. Hamilton', 'Derek Roth']","['Haverford College', 'University of Arizona', 'University of Utah', 'University of Utah', 'Haverford College', 'Haverford College']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
A Computational Model of Commonsense Moral Decision Making,"We introduce a computational model for building moral autonomous vehicles by learning and generalizing from human moral judgments. We draw on a cognitively inspired model of how people and young children learn moral theories from sparse and noisy data and integrate observations made from different people in different groups. The problem of moral learning for autonomous vehicles is cast as learning how to weigh the different features of the dilemma using utility calculus, with the goal of making these trade-offs reflect how people make them in a wide variety of moral dilemma. By modeling the structures of individuals and groups in a hierarchical Bayesian model, we show that an individual's moral values -- as well as a group's shared values -- can be inferred from sparse and noisy data. We evaluate our approach with data from the Moral Machine, a web application that collects human judgments on moral dilemmas involving autonomous vehicles, and show that the model rapidly and accurately infers people's preferences and can predict the difficulty of moral dilemmas from limited data.","['Richard Kim', 'Max Kleiman-Weiner', 'Andrés Abeliuk', 'Edmond Awad', 'Sohan Dsouza', 'Joshua B. Tenenbaum', 'Iyad Rahwan']","['Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
A Deontic Logic for Programming Rightful Machines,"A ""rightful machine"" is an explicitly moral, autonomous machine agent whose behavior conforms to principles of justice and the positive public law of a legitimate state. In this paper, I set out some basic elements of a deontic logic appropriate for capturing conflicting legal obligations for purposes of programming rightful machines. Justice demands that the prescriptive system of enforceable public laws be consistent, yet statutes or case holdings may often describe legal obligations that contradict; moreover, even fundamental constitutional rights may come into conflict. I argue that a deontic logic of the law should not try to work around such conflicts but, instead, identify and expose them so that the rights and duties that generate inconsistencies in public law can be explicitly qualified and the conflicts resolved. I then argue that a credulous, non-monotonic deontic logic can describe inconsistent legal obligations while meeting the normative demand for consistency in the prescriptive system of public law. I propose an implementation of this logic via a modified form of ""answer set programming,"" which I demonstrate with some simple examples.",['Ava Thomas Wright'],"['Northeastern University, Boston, MA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
A Fairness-aware Incentive Scheme for Federated Learning,"In federated learning (FL), data owners ""share"" their local data in a privacy preserving manner in order to build a federated model, which in turn, can be used to generate revenues for the participants. However, in FL involving business participants, they might incur significant costs if several competitors join the same federation. Furthermore, the training and commercialization of the models will take time, resulting in delays before the federation accumulates enough budget to pay back the participants. The issues of costs and temporary mismatch between contributions and rewards have not been addressed by existing payoff-sharing schemes. In this paper, we propose the Federated Learning Incentivizer (FLI) payoff-sharing scheme. The scheme dynamically divides a given budget in a context-aware manner among data owners in a federation by jointly maximizing the collective utility while minimizing the inequality among the data owners, in terms of the payoff gained by them and the waiting time for receiving payoff. Extensive experimental comparisons with five state-of-the-art payoff-sharing schemes show that FLI is the most attractive to high quality data owners and achieves the highest expected revenue for a data federation.","['Han Yu', 'Zelei Liu', 'Yang Liu', 'Tianjian Chen', 'Mingshu Cong', 'Xi Weng', 'Dusit Niyato', 'Qiang Yang']","['Nanyang Technological University, Singapore, Singapore', 'Nanyang Technological University, Singapore, Singapore', 'WeBank, Shenzhen, China', 'WeBank, Shenzhen, China', 'The University of Hong Kong, Hong Kong, China', 'Peking University, Beijing, China', 'Nanyang Technological University, Singapore, Singapore', 'Hong Kong University of Science and Technology, Hong Kong, China']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
A Formal Approach to Explainability,"We regard explanations as a blending of the input sample and the model's output and offer a few definitions that capture various desired properties of the function that generates these explanations. We study the links between these properties and between explanation-generating functions and intermediate representations of learned models and are able to show, for example, that if the activations of a given layer are consistent with an explanation, then so do all other subsequent layers. In addition, we study the intersection and union of explanations as a way to construct new explanations.","['Lior Wolf', 'Tomer Galanti', 'Tamir Hazan']","['Facebook AI Research & Tel Aviv University, Tel Aviv, Israel', 'Tel Aviv University, Tel Aviv, Israel', 'Technion, Haifa, Israel']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,TRUE
A Framework for Benchmarking Discrimination-Aware Models in Machine Learning,"Discrimination-aware models in machine learning are a recent topic of study that aim to minimize the adverse impact of machine learning decisions for certain groups of people due to ethical and legal implications. We propose a benchmark framework for assessing discrimination-aware models. Our framework consists of systematically generated biased datasets that are similar to real world data, created by a Bayesian network approach. Experimental results show that we can assess the quality of techniques through known metrics of discrimination, and our flexible framework can be extended to most real datasets and fairness measures to support a diversity of assessments.","['Rodrigo L. Cardoso', 'Wagner Meira Jr.', 'Virgilio Almeida', 'Mohammed J. Zaki']","['Universidade Federal de Minas Gerais, Belo Horizonte, Brazil', 'Universidade Federal de Minas Gerais, Belo Horizonte, Brazil', 'Universidade Federal de Minas Gerais, Belo Horizonte, Brazil', 'Rensselaer Polytechnic Institute, Troy, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
A Framework for Grounding the Moral Status of Intelligent Machines,"I propose a framework, derived from moral theory, for assessing the moral status of intelligent machines. Using this framework, I claim that some current and foreseeable intelligent machines have approximately as much moral status as plants, trees, and other environmental entities. This claim raises the question: what obligations could a moral agent (e.g., a normal adult human) have toward an intelligent machine? I propose that the threshold for any moral obligation should be the ""functional morality"" of Wallach and Allen [20], while the upper limit of our obligations should not exceed the upper limit of our obligations toward plants, trees, and other environmental entities.",['Michael R. Scheessele'],"['Indiana University South Bend, South Bend, IN, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
A General Approach to Adding Differential Privacy to Iterative Training Procedures,"In this work we address the practical challenges of training machine learning models on privacy-sensitive datasets by introducing a modular approach that minimizes changes to training algorithms, provides a variety of configuration strategies for the privacy mechanism, and then isolates and simplifies the critical logic that computes the final privacy guarantees. A key challenge is that training algorithms often require estimating many different quantities (vectors) from the same set of examples --- for example, gradients of different layers in a deep learning architecture, as well as metrics and batch normalization parameters. Each of these may have different properties like dimensionality, magnitude, and tolerance to noise. By extending previous work on the Moments Accountant for the subsampled Gaussian mechanism, we can provide privacy for such heterogeneous sets of vectors, while also structuring the approach to minimize software engineering challenges.","['Brendan McMahan', 'Nicolas Papernot', 'Peter Kairouz']",Google,NIPS (2018),2018,TRUE
A Geometric Solution to Fair Representations,"To reduce human error and prejudice, many high-stakes decisions have been turned over to machine algorithms. However, recent research suggests that this does not remove discrimination, and can perpetuate harmful stereotypes. While algorithms have been developed to improve fairness, they typically face at least one of three shortcomings: they are not interpretable, their prediction quality deteriorates quickly compared to unbiased equivalents, and %the methodology cannot easily extend other algorithms they are not easily transferable across models% (e.g., methods to reduce bias in random forests cannot be extended to neural networks) . To address these shortcomings, we propose a geometric method that removes correlations between data and any number of protected variables. Further, we can control the strength of debiasing through an adjustable parameter to address the trade-off between prediction quality and fairness. The resulting features are interpretable and can be used with many popular models, such as linear regression, random forest, and multilayer perceptrons. The resulting predictions are found to be more accurate and fair compared to several state-of-the-art fair AI algorithms across a variety of benchmark datasets. Our work shows that debiasing data is a simple and effective solution toward improving fairness.","['Yuzi He', 'Keith Burghardt', 'Kristina Lerman']","['University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
A Human in the Loop is Not Enough: The Need for Human-Subject Experiments in Facial Recognition,"The deployment of facial recognition systems in high-stakes scenarios has sparked widespread concerns about privacy, fairness, and accountability. A common response to these concerns is the suggestion of adding a human in the loop to provide oversight and ensure fairness and accountability. However, the effectiveness of this approach is seldom studied empirically, and humans are known to have biases of their own. In this position paper, we argue for the necessity of empirical studies of human-in-the-loop facial recognition systems. We outline several technical and ethical challenges that arise when conducting such empirical studies and when interpreting their results. Our goal is to initiate a discussion about ways for AI and HCI researchers to work together on human-centered approaches to empirically studying human-in-the-loop facial recognition systems.","['Forough Poursabzi-Sangdeh', 'Samira Samadi', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,CHI workshop on Human-Centered Approaches to Fair and Responsible AI,2020-04-01,TRUE
A Just Approach Balancing Rawlsian Leximax Fairness and Utilitarianism,"Numerous AI-assisted resource allocation decisions need to balance the conflicting goals of fairness and efficiency. Our paper studies the challenging task of defining and modeling a proper fairness-efficiency trade off. We define fairness with Rawlsian leximax fairness, which views the lexicographic maximum among all feasible outcomes as the most equitable; and define efficiency with Utilitarianism, which seeks to maximize the sum of utilities received by entities regardless of individual differences. Motivated by a justice-driven trade off principle: prioritize fairness to benefit the less advantaged unless too much efficiency is sacrificed, we propose a sequential optimization procedure to balance leximax fairness and utilitarianism in decision-making. Each iteration of our approach maximizes a social welfare function, and we provide a practical mixed integer/linear programming (MILP) formulation for each maximization problem. We illustrate our method on a budget allocation example. Compared with existing approaches of balancing equity and efficiency, our method is more interpretable in terms of parameter selection, and incorporates a strong equity criterion with a thoroughly balanced perspective.","['Violet (Xinying) Chen', 'J. N. Hooker']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
A Moral Framework for Understanding Fair ML through Economic Models of Equality of Opportunity,"We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity (EOP)---an extensively studied ideal of fairness in political philosophy. We formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new family of measures for algorithmic fairness. We illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.","['Hoda Heidari', 'Michele Loi', 'Krishna P. Gummadi', 'Andreas Krause']","['ETH Zürich', 'University of Zürich', 'MPI-SWS', 'ETH Zürich']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
A Pilot Study in Surveying Clinical Judgments to Evaluate Radiology Report Generation,"The recent release of many Chest X-Ray datasets has prompted a lot of interest in radiology report generation. To date, this has been framed as an image captioning task, where the machine takes an RGB image as input and generates a 2-3 sentence summary of findings as output. The quality of these reports has been canonically measured using metrics from the NLP community for language generation such as Machine Translation and Summarization. However, the evaluation metrics (e.g. BLEU, CIDEr) are inappropriate for the medical domain, where clinical correctness is critical. To address this, our team brought together machine learning experts with radiologists for a pilot study in co-designing a better metric for evaluating the quality of an algorithmically-generated radiology report. The interdisciplinary collaborative process involved multiple interviews, outreach, and preliminary annotation to design a larger scale study - which is now underway - to build a more meaningful evaluation tool.","['William Boag', 'Hassan Kané', 'Saumya Rawat', 'Jesse Wei', 'Alexander Goehler']","['MIT, USA', 'WL Research, USA', 'MIT, USA', 'Beth Israel Deaconess Medical Center, Department of Radiology, USA', 'Beth Israel Deaconess Medical Center, Department of Radiology, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
A practical algorithm for balancing the max-min fairness and throughput objectives in traffic engineering,"One of the goals of traffic engineering is to achieve a
flexible trade-off between fairness and throughput so that users
are satisfied with their bandwidth allocation and the network
operator is satisfied with the utilization of network resources. In
this paper, we propose a novel way to balance the throughput
and fairness objectives with linear programming. It allows the
network operator to precisely control the trade-off by bounding
the fairness degradation for each commodity compared to the
max-min fair solution or the throughput degradation compared
to the optimal throughput. We also present improvements to a
previous algorithm that achieves max-min fairness by solving a
series of linear programs. We significantly reduce the number
of steps needed when the access rate of commodities is limited.
We extend the algorithm to two important practical use cases:
importance weights and piece-wise linear utility functions for
commodities. Our experiments on synthetic and real networks
show that our algorithms achieve a significant speedup and
provide practical insights on the trade-off between fairness and
throughput.",['Subhasree Mandal'],Google,INFOCOM (2012),2012,TRUE
A Qualitative Exploration of Perceptions of Algorithmic Fairness,"Algorithmic systems increasingly shape information people are exposed to as well as influence decisions about employment, finances, and other opportunities. In some cases, algorithmic systems may be more or less favorable to certain groups or individuals, sparking substantial discussion of algorithmic fairness in public policy circles, academia, and the press. We broaden this discussion by exploring how members of potentially affected communities feel about algorithmic fairness. We conducted workshops and interviews with 44 participants from several populations traditionally marginalized by categories of race or class in the United States. While the concept of algorithmic fairness was largely unfamiliar, learning about algorithmic (un)fairness elicited negative feelings that connect to current national discussion about racial injustice and economic inequality. In addition to their concerns about potential harms to themselves and society, participants also indicated that algorithmic fairness (or lack thereof) could substantially affect their trust in a company or product.",[],Google,CHI 2018 (2018),2018,TRUE
A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development,"One of the important challenges regarding the development of morally responsible and ethically qualified digital technologies is how to support designers and developers in producing those technologies, especially when conceptualizing their vision of what the technology will be, how it will benefit users, and avoid doing harm. However, traditional software design and development life cycles do not explicitly support the reflection upon either ethical or moral issues. In this paper we look at how a number of ethical issues may be dealt with during digital technology design and development, to prevent damage and improve technological fairness, accountability, and transparency. Starting from mature work on semiotic theory and methods in human-computer interaction, we propose to extend the core artifact used in semiotic engineering of human-centered technology design, so as to directly address moral responsibility and ethical issues. The resulting extension is an epistemic tool, that is, an instrument to create and elaborate on this specific kind of knowledge. The paper describes the tool, illustrates how it is to be used, and discusses its promises and limitations against the background of related work. It also includes proposed empirical studies, accompanied by briefly described methodological challenges and considerations that deserve our attention.","['Simone Diniz Junqueira Barbosa', 'Gabriel Diniz Junqueira Barbosa', 'Clarisse Sieckenius de Souza', 'Carla Faria Leitão']","['Department of Informatics, PUC-Rio Rio de Janeiro, RJ', 'Department of Informatics, PUC-Rio Rio de Janeiro, RJ', 'Department of Informatics, PUC-Rio Rio de Janeiro, RJ', 'Department of Psychology, PUC-Rio Rio de Janeiro, RJ']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
A Statistical Test for Probabilistic Fairness,"Algorithms are now routinely used to make consequential decisions that affect human lives. Examples include college admissions, medical interventions or law enforcement. While algorithms empower us to harness all information hidden in vast amounts of data, they may inadvertently amplify existing biases in the available datasets. This concern has sparked increasing interest in fair machine learning, which aims to quantify and mitigate algorithmic discrimination. Indeed, machine learning models should undergo intensive tests to detect algorithmic biases before being deployed at scale. In this paper, we use ideas from the theory of optimal transport to propose a statistical hypothesis test for detecting unfair classifiers. Leveraging the geometry of the feature space, the test statistic quantifies the distance of the empirical distribution supported on the test samples to the manifold of distributions that render a pre-trained classifier fair. We develop a rigorous hypothesis testing mechanism for assessing the probabilistic fairness of any pre-trained logistic classifier, and we show both theoretically as well as empirically that the proposed test is asymptotically correct. In addition, the proposed framework offers interpretability by identifying the most favorable perturbation of the data so that the given classifier becomes fair.","['Bahar Taskesen', 'Jose Blanchet', 'Daniel Kuhn', 'Viet Anh Nguyen']","['Ecole Polytechnique Fédérale de Lausanne, Switzerland', 'Stanford University, USA', 'Ecole Polytechnique Fédérale de Lausanne, Switzerland', 'Stanford University, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
A Taxonomy of Ethical Tensions in Inferring Mental Health States from Social Media,"Powered by machine learning techniques, social media provides an unobtrusive lens into individual behaviors, emotions, and psychological states. Recent research has successfully employed social media data to predict mental health states of individuals, ranging from the presence and severity of mental disorders like depression to the risk of suicide. These algorithmic inferences hold great potential in supporting early detection and treatment of mental disorders and in the design of interventions. At the same time, the outcomes of this research can pose great risks to individuals, such as issues of incorrect, opaque algorithmic predictions, involvement of bad or unaccountable actors, and potential biases from intentional or inadvertent misuse of insights. Amplifying these tensions, there are also divergent and sometimes inconsistent methodological gaps and under-explored ethics and privacy dimensions. This paper presents a taxonomy of these concerns and ethical challenges, drawing from existing literature, and poses questions to be resolved as this research gains traction. We identify three areas of tension: ethics committees and the gap of social media research; questions of validity, data, and machine learning; and implications of this research for key stakeholders. We conclude with calls to action to begin resolving these interdisciplinary dilemmas.","['Stevie Chancellor', 'Michael L. Birnbaum', 'Eric D. Caine', 'Vincent M. B. Silenzio', 'Munmun De Choudhury']","['Georgia Tech, Atlanta, GA, US', 'Northwell Health, Glen Oaks, NY, US', 'University of Rochester, Rochester, NY, US', 'University of Rochester, Rochester, NY, US', 'Georgia Tech, Atlanta, GA, US']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
A Win for Society! Conquering Barriers to Fair Elections,"Social choice is a general framework used in the aggregation of agent preferences to make a collective decision, political elections whereby agents vote is a common example. It is often the case that society demands electoral systems which ensure, or election outcomes which satisfy, socially desirable outcomes such as representing large minorities and avoiding the 'tyranny of the majority'. Unfortunately, there are many natural barriers which may prevent desirable outcomes from being achieved. These barriers include the non-existence or computational intractability of achieving desirable outcomes, especially when combined with additional feasibility constraints, and the effect of strategic or manipulative agents. This thesis aims to improve our understanding of the scale of these barriers and if, or how, they can be overcome to provide socially desirable outcomes.",['Barton E. Lee'],"['The University of New South Wales, Sydney, Australia']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Access to Population-Level Signaling as a Source of Inequality,"We identify and explore differential access to population-level signaling (also known as information design) as a source of unequal access to opportunity. A population-level signaler has potentially noisy observations of a binary type for each member of a population and, based on this, produces a signal about each member. A decision-maker infers types from signals and accepts those individuals whose type is high in expectation. We assume the signaler of the disadvantaged population reveals her observations to the decision-maker, whereas the signaler of the advantaged population forms signals strategically. We study the expected utility of the populations as measured by the fraction of accepted members, as well as the false positive rates (FPR) and false negative rates (FNR). We first show the intuitive results that for a fixed environment, the advantaged population has higher expected utility, higher FPR, and lower FNR, than the disadvantaged one (despite having identical population quality), and that more accurate observations improve the expected utility of the advantaged population while harming that of the disadvantaged one. We next explore the introduction of a publicly-observable signal, such as a test score, as a potential intervention. Our main finding is that this natural intervention, intended to reduce the inequality between the populations' utilities, may actually exacerbate it in settings where observations and test scores are noisy.","['Nicole Immorlica', 'Katrina Ligett', 'Juba Ziani']","['Microsoft Research', 'Hebrew University of Jerusalem', 'California Institute of Technology']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Accountable Agents and Where to Find Them,"The aim of my PhD is to investigate the notion of computational accountability relying on approaches from the research on multi-agent systems. The main contribution will be to provide a notion of when an organization supports accountability, by exploring the process of construction of the organization itself, and to guarantee accountability as a design property.",['Stefano Tedeschi'],"['Università degli Studi di Torino, Torino, Italy']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products,"Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of 31.37% and 22.50% for the darker female subgroup, respectively.","['Inioluwa Deborah Raji', 'Joy Buolamwini']","['University of Toronto, Toronto, ON, Canada', 'Massachusetts Institute of Technology, Cambridge, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Actionable Recourse in Linear Classification,"Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood. In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.","['Berk Ustun', 'Alexander Spangher', 'Yang Liu']","['Harvard University, Cambridge, MA', 'Carnegie Mellon University, Pittsburgh, PA', 'UC Santa Cruz CSE, Santa Cruz, CA']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Active Fairness in Algorithmic Decision Making,"Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternativeactive framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g.,equal opportunity ); and 2) parity in both false positive and false negative rates (i.e.,equal odds ). Moreover, we show that by leveraging their additional degree of freedom,active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.","['Alejandro Noriega-Campero', 'Michiel A. Bakker', 'Bernardo Garcia-Bulle', ""Alex 'Sandy' Pentland""]","['Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Mexican Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Activism by the AI Community: Analysing Recent Achievements and Future Prospects,"The artificial intelligence (AI) community has recently engaged in activism in relation to their employers, other members of the community, and their governments in order to shape the societal and ethical implications of AI. It has achieved some notable successes, but prospects for further political organising and activism are uncertain. We survey activism by the AI community over the last six years; apply two analytical frameworks drawing upon the literature on epistemic communities, and worker organising and bargaining; and explore what they imply for the future prospects of the AI community. Success thus far has hinged on a coherent shared culture, and high bargaining power due to the high demand for a limited supply of AI 'talent'. Both are crucial to the future of AI activism and worthy of sustained attention.",['Haydn Belfield'],"['University of Cambridge, Cambridge, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Adapting a Kidney Exchange Algorithm to Align with Human Values,"The efficient allocation of limited resources is a classical problem in economics and computer science. In kidney exchanges, a central market maker allocates living kidney donors to patients in need of an organ. Patients and donors in kidney exchanges are prioritized using ad-hoc weights decided on by committee and then fed into an allocation algorithm that determines who get what---and who does not. In this paper, we provide an end-to-end methodology for estimating weights of individual participant profiles in a kidney exchange. We first elicit from human subjects a list of patient attributes they consider acceptable for the purpose of prioritizing patients (e.g., medical characteristics, lifestyle choices, and so on). Then, we ask subjects comparison queries between patient profiles and estimate weights in a principled way from their responses. We show how to use these weights in kidney exchange market clearing algorithms. We then evaluate the impact of the weights in simulations and find that the precise numerical values of the weights we computed matter little, other than the ordering of profiles that they imply. However, compared to not prioritizing patients at all, there is a significant effect, with certain classes of patients being (de)prioritized based on the human-elicited value judgments.","['Rachel Freedman', 'Jana Schaich Borg', 'Walter Sinnott-Armstrong', 'John P. Dickerson', 'Vincent Conitzer']","['Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA', 'University of Maryland, College Park, MD, USA', 'Duke University, Durham, NC, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Adoption Dynamics and Societal Impact of AI Systems in Complex Networks,"We propose a game-theoretical model to simulate the dynamics of AI adoption in adaptive networks. This formalism allows us to understand the impact of the adoption of AI systems for society as a whole, addressing some of the concerns on the need for regulation. Using this model we study the adoption of AI systems, the distribution of the different types of AI (from selfish to utilitarian), the appearance of clusters of specific AI types, and the impact on the fitness of each individual. We suggest that the entangled evolution of individual strategy and network structure constitutes a key mechanism for the sustainability of utilitarian and human-conscious AI. Differently, in the absence of rewiring, a minority of the population can easily foster the adoption of selfish AI and gains a benefit at the expense of the remaining majority.","['Pedro M. Fernandes', 'Francisco C. Santos', 'Manuel Lopes']","['Universidade de Lisboa, Lisbon, Portugal', 'Universidade de Lisboa, Lisbon, Portugal', 'Universidade de Lisboa, Lisbon, Portugal']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Adversarial Examples as an Input-Fault Tolerance Problem,"We analyze the adversarial examples problem in terms of a modelâs fault tolerance
with respect to its input. Whereas previous work focuses on arbitrarily strict threat
models, i.e., -perturbations, we consider arbitrary valid inputs and propose an
information-based characteristic for evaluating tolerance to diverse input faults.",[],Google,NeurIPS Workshop on Security in Machine Learning (2018),2018,TRUE
Adversarial Spheres,"State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size O(1/sqrt(d)). Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.","['Luke Metz', 'Maithra Raghu', 'Martin Wattenberg']",Google,ICLR Workshop (2018),2018,TRUE
Adversarially Robust Generalization Requires More Data,"Machine learning models are often susceptible to adversarial perturbations of their inputs.
Even small perturbations can cause state-of-the-art classifiers with high âstandardâ accuracy to
produce an incorrect prediction with high confidence. To better understand this phenomenon, we
study adversarially robust learning from the viewpoint of generalization. We show that already
in a simple natural data model, the sample complexity of robust learning can be significantly
larger than that of âstandardâ learning. This gap is information theoretic and holds irrespective
of the training algorithm or the model family. We complement our theoretical results with
experiments on popular image classification datasets and show that a similar gap exists here as
well. We postulate that the difficulty of training robust classifiers stems, at least partially, from
this inherently larger sample complexity.",[],Google,NeurIPS (Spotlight) (2018),2018,TRUE
AI + Art = Human,"Over the past few years, specialised online and offline press blossomed with articles about art made ""with"" Artificial Intelligence (AI) but the narrative is rapidly changing. In fact, in October 2018, the auction house Christie's sold an art piece allegedly made ""by"" an AI. We draw from philosophy of art and science arguing that AI as a technical object is always intertwined with human nature despite its level of autonomy. However, the use of creative autonomous agents has cultural and social implications in the way we experience art as creators as well as audience. Therefore, we highlight the importance of an interdisciplinary dialogue by promoting a culture of transparency of the technology used, awareness of the meaning of technology in our society and the value of creativity in our lives.","['Antonio Daniele', 'Yi-Zhe Song']","['Queen Mary University of London, London, United Kingdom', 'Queen Mary University of London, London, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
AI and Holistic Review: Informing Human Reading in College Admissions,"College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant's fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.","['A.J. Alvero', 'Noah Arthurs', 'anthony lising antonio', 'Benjamin W. Domingue', 'Ben Gebre-Medhin', 'Sonia Giebel', 'Mitchell L. Stevens']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
AI Extenders: The Ethical and Societal Implications of Humans Cognitively Extended by AI,"Humans and AI systems are usually portrayed as separate systems that we need to align in values and goals. However, there is a great deal of AI technology found in non-autonomous systems that are used as cognitive tools by humans. Under the extended mind thesis, the functional contributions of these tools become as essential to our cognition as our brains. But AI can take cognitive extension towards totally new capabilities, posing new philosophical, ethical and technical challenges. To analyse these challenges better, we define and place AI extenders in a continuum between fully-externalized systems, loosely coupled with humans, and fully internalized processes, with operations ultimately performed by the brain, making the tool redundant. We dissect the landscape of cognitive capabilities that can foreseeably be extended by AI and examine their ethical implications.We suggest that cognitive extenders using AI be treated as distinct from other cognitive enhancers by all relevant stakeholders, including developers, policy makers, and human users.","['José Hernández-Orallo', 'Karina Vold']","['Universitat Politècnica de València, Valencia, Spain', 'University of Cambridge, Cambridge, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
AI Risk Mitigation Through Democratic Governance: Introducing the 7-Dimensional AI Risk Horizon,"My dissertation asks two fundamental questions: What are the risks of AI? And what should be done about them? My research goes beyond existential threats to humanity to consider seven dimensions of AI risk: military, political, economic, social, environmental, psychophysiological, and spiritual. I examine extant AI risk mitigation strategies and, finding them insufficient, use a democratic governance framework to propose alternatives. This paper outlines the project and introduces the risk dimensions.",['Colin Garvey'],"['Rensselaer Polytechnic Institute, Troy, NY, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Algorithmic accountability in public administration: the GDPR paradox,"The EU General Data Protection Regulation (""GDPR"") is often represented as a larger than life behemoth that will fundamentally transform the world of big data. Abstracted from its constituent parts of corresponding rights, responsibilities, and exemptions, the operative scope of the GDPR can be unduly aggrandized, when in reality, it caters to the specific policy objectives of legislators and institutional stakeholders. With much uncertainty ahead on the precise implementation of the GDPR, academic and policy discussions are debating the adequacy of protections for automated decision-making in GDPR Articles 13 (right to be informed of automated treatment), 15 (right of access by the data subject), and 22 (safeguards to profiling). Unfortunately, the literature to date disproportionately focuses on the impact of AI in the private sector, and deflects any extensive review of automated enforcement tools in public administration. Even though the GDPR enacts significant safeguards against automated decisions, it does so with deliberate design: to balance the interests of data protection with the growing demand for algorithms in the administrative state. In order to facilitate inter-agency data flows and sensitive data processing that fuel the predictive power of algorithmic enforcement tools, the GDPR decisively surrenders to the procedural autonomy of Member States to authorize these practices. Yet, due to a dearth of research on the GDPR's stance on government deployed algorithms, it is not widely known that public authorities can benefit from broadly worded exemptions to restrictions on automated decision-making, and even circumvent remedies for data subjects through national legislation. The potential for public authorities to invoke derogations from the GDPR must be contained by the fundamental guarantees of due process, judicial review, and equal treatment. This paper examines the interplay of these principles within the prospect of algorithmic decision-making by public authorities.",['Sunny Seon Kang'],['Inpher'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Algorithmic Fairness from a Non-ideal Perspective,"Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In the hopes of mitigating these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might hope to observe in a fair world, offering a variety of algorithms that attempt to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to fair machine learning to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and ideal worlds. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of their actions, naive applications of ideal thinking can lead to misguided policies. In this paper, we demonstrate a connection between the recent literature on fair machine learning and the ideal approach in political philosophy, and show that some recently uncovered shortcomings in proposed algorithms reflect broader troubles faced by the ideal approach. We work this analysis through for different formulations of fairness and conclude with a critical discussion of real-world impacts and directions for new research.","['Sina Fazelpour', 'Zachary C. Lipton']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Algorithmic Fairness in Predicting Opioid Use Disorder using Machine Learning,"There has been recent interest by payers, health care systems, and researchers in the development of machine learning and artificial intelligence models that predict an individual's probability of developing opioid use disorder. The scores generated by these algorithms can be used by physicians to tailor the prescribing of opioids for the treatment of pain, reducing or foregoing prescribing to individuals deemed to be at high risk, or increasing prescribing for patients deemed to be at low risk. This paper constructs a machine learning algorithm to predict opioid use disorder risk using commercially available claims data similar to those utilized in the development of proprietary opioid use disorder prediction algorithms. We study risk scores generated by the machine learning model in a setting with quasi-experimental variation in the likelihood that doctors prescribe opioids, generated by changes in the legal structure for monitoring physician prescribing. We find that machine-predicted risk scores do not appear to correlate at all with the individual-specific heterogeneous treatment effect of receiving opioids. The paper identifies a new source of algorithmic unfairness in machine learning applications for health care and precision medicine, arising from the researcher's choice of objective function. While precision medicine should guide physician treatment decisions based on the heterogeneous causal impact of a course of treatment for an individual, allocating treatments to individuals receiving the most benefit and recommending caution for those most likely to experience harmful side effects, ML models in health care are often trained on proxies like individual baseline risk, and are not necessarily informative in deciding who would most benefit, or be harmed, by a course of treatment.",['Angela E. Kilby'],"['Northeastern University Boston, Massachusetts, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Algorithmic Greenlining: An Approach to Increase Diversity,"In contexts such as college admissions, hiring, and image search, decision-makers often aspire to formulate selection criteria that yield both high-quality and diverse results. However, simultaneously optimizing for quality and diversity can be challenging, especially when the decision-maker does not know the true quality of any criterion and instead must rely on heuristics and intuition. We introduce an algorithmic framework that takes as input a user's selection criterion, which may yield high-quality but homogeneous results. Using an application-specific notion of substitutability, our algorithms suggest similar criteria with more diverse results, in the spirit of statistical or demographic parity. For instance, given the image search query ""chairman"", it suggests alternative queries which are similar but more gender-diverse, such as ""chairperson"". In the context of college admissions, we apply our algorithm to a dataset of students' applications and rediscover Texas's ""top 10% rule"": the input criterion is an ACT score cutoff, and the output is a class rank cutoff, automatically accepting the students in the top decile of their graduating class. Historically, this policy has been effective in admitting students who perform well in college and come from diverse backgrounds. We complement our empirical analysis with learning-theoretic guarantees for estimating the true diversity of any criterion based on historical data.","['Christian Borgs', 'Jennifer Chayes', 'Nika Haghtalab', 'Adam Tauman Kalai', 'Ellen Vitercik']","['Microsoft Research, Cambridge, MA, USA', 'Microsoft Research, Cambridge, MA, USA', 'Microsoft Research, Cambridge, MA, USA', 'Microsoft Research, Cambridge, MA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,TRUE
Algorithmic Impact Assessments and Accountability: The Co-construction of Impacts,"Algorithmic impact assessments (AIAs) are an emergent form of accountability for organizations that build and deploy automated decision-support systems. They are modeled after impact assessments in other domains. Our study of the history of impact assessments shows that ""impacts"" are an evaluative construct that enable actors to identify and ameliorate harms experienced because of a policy decision or system. Every domain has different expectations and norms around what constitutes impacts and harms, how potential harms are rendered as impacts of a particular undertaking, who is responsible for conducting such assessments, and who has the authority to act on them to demand changes to that undertaking. By examining proposals for AIAs in relation to other domains, we find that there is a distinct risk of constructing algorithmic impacts as organizationally understandable metrics that are nonetheless inappropriately distant from the harms experienced by people, and which fall short of building the relationships required for effective accountability. As impact assessments become a commonplace process for evaluating harms, the FAccT community, in its efforts to address this challenge, should A) understand impacts as objects that are co-constructed accountability relationships, B) attempt to construct impacts as close as possible to actual harms, and C) recognize that accountability governance requires the input of various types of expertise and affected communities. We conclude with lessons for assembling cross-expertise consensus for the co-construction of impacts and building robust accountability relationships.","['Jacob Metcalf', 'Emanuel Moss', 'Elizabeth Anne Watkins', 'Ranjit Singh', 'Madeleine Clare Elish']","['Data & Society Research Institute New York, NY', 'Data & Society Research Institute New York, NY CUNY Graduate Center New York, NY', 'Princeton Center for Information Technology Policy Princeton, NJ Data & Society Research Institute New York, NY', 'Data & Society Research Institute New York, NY', 'Data & Society Research Institute New York, NY']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Algorithmic realism: expanding the boundaries of algorithmic thought,"Although computer scientists are eager to help address social problems, the field faces a growing awareness that many well-intentioned applications of algorithms in social contexts have led to significant harm. We argue that addressing this gap between the field's desire to do good and the harmful impacts of many of its interventions requires looking to the epistemic and methodological underpinnings of algorithms. We diagnose the dominant mode of algorithmic reasoning as ""algorithmic formalism"" and describe how formalist orientations lead to harmful algorithmic interventions. Addressing these harms requires pursuing a new mode of algorithmic thinking that is attentive to the internal limits of algorithms and to the social concerns that fall beyond the bounds of algorithmic formalism. To understand what a methodological evolution beyond formalism looks like and what it may achieve, we turn to the twentieth century evolution in American legal thought from legal formalism to legal realism. Drawing on the lessons of legal realism, we propose a new mode of algorithmic thinking---""algorithmic realism""---that provides tools for computer scientists to account for the realities of social life and of algorithmic impacts. These realist approaches, although not foolproof, will better equip computer scientists to reduce algorithmic harms and to reason well about doing good.","['Ben Green', 'Salomé Viljoen']","['Harvard University', 'Cornell University & New York University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Algorithmic Recourse: from Counterfactual Explanations to Interventions,"As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -""how the world would have (had) to be different for a desirable outcome to occur""- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.","['Amir-Hossein Karimi', 'Bernhard Schölkopf', 'Isabel Valera']","['MPI-IS, Germany, ETH Zürich, Switzerland', 'MPI-IS, Germany', 'MPI-IS, Germany, Saarland University, Germany']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
"Algorithmic targeting of social policies: fairness, accuracy, and distributed governance","Targeted social policies are the main strategy for poverty alleviation across the developing world. These include targeted cash transfers (CTs), as well as targeted subsidies in health, education, housing, energy, childcare, and others. Due to the scale, diversity, and widespread relevance of targeted social policies like CTs, the algorithmic rules that decide who is eligible to benefit from them---and who is not---are among the most important algorithms operating in the world today. Here we report on a year-long engagement towards improving social targeting systems in a couple of developing countries. We demonstrate that a shift towards the use of AI methods in poverty-based targeting can substantially increase accuracy, extending the coverage of the poor by nearly a million people in two countries, without increasing expenditure. However, we also show that, absent explicit parity constraints, both status quo and AI-based systems induce disparities across population subgroups. Moreover, based on qualitative interviews with local social institutions, we find a lack of consensus on normative standards for prioritization and fairness criteria. Hence, we close by proposing a decision-support platform for distributed governance, which enables a diversity of institutions to customize the use of AI-based insights into their targeting decisions.","['Alejandro Noriega-Campero', 'Bernardo Garcia-Bulle', 'Luis Fernando Cantu', 'Michiel A. Bakker', 'Luis Tejerina', 'Alex Pentland']","['Prosperia Labs', 'MIT', 'ITAM, Mexico City, Mexico', 'MIT', 'IADB', 'MIT']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Algorithmized but not Atomized? How Digital Platforms Engender New Forms of Worker Solidarity in Jakarta,"Jakarta's roads are green, filled as they are with the fluorescent green jackets, bright green logos and fluttering green banners of basecamps created by the city's digitized, 'online' motorbike-taxi drivers (ojol). These spaces function as waiting posts, regulatory institutions, information networks and spaces of solidarity for the ojol working for mobility-app companies, Grab and GoJek. Their existence though, presents a puzzle. In the world of on-demand matching, literature either predicts an isolated, atomized, disempowered digital worker or expects workers to have only temporary, online, ephemeral networks of mutual aid. Yet, Jakarta's ojol then introduce us to a new form of labor action that relies on an interface of the physical world and digital realm, complete with permanent shelters, quirky names, emblems, social media accounts and even their own emergency response service. This paper explores the contours of these labor formations and asks why digital workers in Jakarta are able to create collective structures of solidarity, even as app-mediated work may force them towards an individualized labor regime? I argue that these digital labor collectives are not accidental but a product of interactions between histories of social organization structures in Jakarta and affordances created by technological-mediation. Through participant observation and semi-structured interviews I excavate the bi-directional conversation between globalizing digital platforms and social norms, civic culture and labor market conditions in Jakarta which has allowed for particular forms of digital worker resistances to emerge. I recover power for the digital worker, who provides us with a path to resisting algorithmization of work while still participating in it through agentic labor actions rooted in shared identities, enabled by technological fluency and borne out of a desire for community.",['Rida Qadri'],"['Massachusetts Institute of Technology, Cambridge, MA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
"All The Cool Kids, How Do They Fit In?: Popularity and Demographic Biases in Recommender Evaluation and Effectiveness","In the research literature, evaluations of recommender system effectiveness typically report results over a given data set, providing an aggregate measure of effectiveness over each instance (e.g. user) in the data set. Recent advances in information retrieval evaluation, however, demonstrate the importance of considering the distribution of effectiveness across diverse groups of varying sizes. For example, do users of different ages or genders obtain similar utility from the system, particularly if their group is a relatively small subset of the user base? We apply this consideration to recommender systems, using offline evaluation and a utility-based metric of recommendation effectiveness to explore whether different user demographic groups experience similar recommendation accuracy. We find demographic differences in measured recommender effectiveness across two data sets containing different types of feedback in different domains; these differences sometimes, but not always, correlate with the size of the user group in question. Demographic effects also have a complex-and likely detrimental-interaction with popularity bias, a known deficiency of recommender evaluation. These results demonstrate the need for recommender system evaluation protocols that explicitly quantify the degree to which the system is meeting the information needs of all its users, as well as the need for researchers and operators to move beyond naïve evaluations that favor the needs of larger subsets of the user population while ignoring smaller subsets. ","Michael D. Ekstrand, Mucun Tian, Ion Madrazo Azpiazu, Jennifer D. Ekstrand, Oghenemaro Anuyah, David McNeill, Maria Soledad Pera",Boise State University,FAT* 2018,2018,FALSE
Allocating Opportunities in a Dynamic Model of Intergenerational Mobility,"Opportunities such as higher education can promote intergenerational mobility, leading individuals to achieve levels of socioeconomic status above that of their parents. We develop a dynamic model for allocating such opportunities in a society that exhibits bottlenecks in mobility; the problem of optimal allocation reflects a trade-off between the benefits conferred by the opportunities in the current generation and the potential to elevate the socioeconomic status of recipients, shaping the composition of future generations in ways that can benefit further from the opportunities. We show how optimal allocations in our model arise as solutions to continuous optimization problems over multiple generations, and we find in general that these optimal solutions can favor recipients of low socioeconomic status over slightly higher-performing individuals of high socioeconomic status --- a form of socioeconomic affirmative action that the society in our model discovers in the pursuit of purely payoff-maximizing goals. We characterize how the structure of the model can lead to either temporary or persistent affirmative action, and we consider extensions of the model with more complex processes modulating the movement between different levels of socioeconomic status.","['Hoda Heidari', 'Jon Kleinberg']","['Carnegie Mellon University', 'Cornell University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection,"Web-based human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its often-latent nature. The use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decision-making, and, more broadly, help curb a widespread social problem. Trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the Web -- a higher score indicates a greater risk of trafficking-related (involuntary) activities. In this paper, we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the DARPA Memex program. Drawing on multi-institutional data, systems, and experiences collected during this time, we also conduct post hoc bias analyses and present a bias mitigation plan. Our findings show that, while automatic trafficking detection is an important application of AI for social good, it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate de-biasing. This ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads.","['Kyle Hundman', 'Thamme Gowda', 'Mayank Kejriwal', 'Benedikt Boecking']","['California Institute of Technology, Pasadena, CA, USA', 'University of Southern California, Marina Del Rey, CA, USA', 'University of Southern California, Marina Del Rey, CA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity,"Sensitive statistics are often collected across sets of users, with repeated collection of reports done over time. For example, trends in users' private preferences or software usage may be monitored via such reports. We study the collection of such statistics in the local differential privacy (LDP) model, and describe an algorithm whose privacy cost is polylogarithmic in the number of changes to a user's value. 
",[],Google,ACM-SIAM Symposium on Discrete Algorithms (SODA) (2019),2019,TRUE
An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists,"Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.","['P. M. Krafft', 'Meg Young', 'Michael Katell', 'Jennifer E. Lee', 'Shankar Narayan', 'Micah Epstein', 'Dharma Dailey', 'Bernease Herman', 'Aaron Tam', 'Vivian Guetler', 'Corinne Bintz', 'Daniella Raz', 'Pa Ousman Jobe', 'Franziska Putz', 'Brian Robick', 'Bissan Barghouti']","['Creative Computing Institute, University of Arts London', 'Digital Life Initiative, Cornell Tech', 'Public Policy Programme, Alan Turing Institute', 'ACLU of Washington', 'MIRA', 'Coveillance Collective', 'Human Centered Design & Engineering, University of Washington', 'eScience Institute, University of Washington', 'Evans School of Public Policy & Governance, University of Washington', 'Department of Sociology, West Virginia University', 'Department of Computer Science, Middlebury College', 'School of Information, University of Michigan', 'Albers School of Business & Economics, Seattle University', 'Oxford Department of International Development, University of Oxford', 'ACLU of Washington', 'ACLU of Washington']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
An Agent-based Model to Evaluate Interventions on Online Dating Platforms to Decrease Racial Homogamy,"Perhaps the most controversial questions in the study of online platforms today surround the extent to which platforms can intervene to reduce the societal ills perpetrated on them. Up for debate is whether there exist any effective and lasting interventions a platform can adopt to address, e.g., online bullying, or if other, more far-reaching change is necessary to address such problems. Empirical work is critical to addressing such questions. But it is also challenging, because it is time-consuming, expensive, and sometimes limited to the questions companies are willing to ask. To help focus and inform this empirical work, we here propose an agent-based modeling (ABM) approach. As an application, we analyze the impact of a set of interventions on a simulated online dating platform on the lack of long-term interracial relationships in an artificial society. In the real world, a lack of interracial relationships are a critical vehicle through which inequality is maintained. Our work shows that many previously hypothesized interventions online dating platforms could take to increase the number of interracial relationships from their website have limited effects, and that the effectiveness of any intervention is subject to assumptions about sociocultural structure. Further, interventions that are effective in increasing diversity in long-term relationships are at odds with platforms' profit-oriented goals. At a general level, the present work shows the value of using an ABM approach to help understand the potential effects and side effects of different interventions that a platform could take.","['Stefania Ionescu', 'Anikó Hannák', 'Kenneth Joseph']","['University of Zürich, Zürich, Switzerland', 'University of Zürich, Zürich, Switzerland', 'University at Buffalo, Buffalo, NY, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
An AI Race for Strategic Advantage: Rhetoric and Risks,"The rhetoric of the race for strategic advantage is increasingly being used with regard to the development of artificial intelligence (AI), sometimes in a military context, but also more broadly. This rhetoric also reflects real shifts in strategy, as industry research groups compete for a limited pool of talented researchers, and nation states such as China announce ambitious goals for global leadership in AI. This paper assesses the potential risks of the AI race narrative and of an actual competitive race to develop AI, such as incentivising corner-cutting on safe-ty and governance, or increasing the risk of conflict. It explores the role of the research community in respond-ing to these risks. And it briefly explores alternative ways in which the rush to develop powerful AI could be framed so as instead to foster collaboration and respon-sible progress.","['Stephen Cave', 'Seán S. ÓhÉigeartaigh']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
An Autonomous Architecture that Protects the Right to Privacy,"The advent and widespread adoption of wearable cameras and autonomous robots raises important issues related to privacy. The mobile cameras on these systems record and may re-transmit enormous amounts of video data that can then be used to identify, track, and characterize the behavior of the general populous. This paper presents a preliminary computational architecture designed to preserve specific types of privacy over a video stream by identifying categories of individuals, places, and things that require higher than normal privacy protection. This paper describes the architecture as a whole as well as preliminary results testing aspects of the system. Our intention is to implement and test the system on ground robots and small UAVs and demonstrate that the system can provide selective low-level masking or deletion of data requiring higher privacy protection.",['Alan R. Wagner'],"['Pennsylvania State University, University Park, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
An Axiomatic Approach to Explain Computer Generated Decisions: Extended Abstract,"Recent years have seen the widespread implementation of data-driven algorithms making decisions in increasingly highstakes domains, such as finance, healthcare, transportation and public safety. Using novel ML techniques, these algorithms are able to process massive amounts of data and make highly accurate predictions; however, their inherent complexity makes it increasingly difficult for humans to understand why certain decisions were made. Indeed, these algorithms are black-box decision makers: their underlying decision processes are either hidden from human scrutiny by proprietary law, or (as is often the case) their inner workings are so complicated that even their own designers will be hard-pressed to explain the underlying reasoning behind their decision making processes. By obfuscating their function, data-driven classifiers run the risk of exposing human stakeholders to risks. These may include incorrect decisions (e.g. a loan application that was wrongly rejected due to system error), information leaks (e.g. an algorithm inadvertently uses information it should not have used), or discrimination (e.g. biased decisions against certain ethnic or gender groups). Government bodies and regulatory authorities have recently begun calling for algorithmic transparency: providing human-interpretable explanations of the underlying reasoning behind large-scale decision making algorithms. My thesis research will be concerned with an axiomatic analysis of automatically generated explanations of such classifiers. Especially, I'm interested in how to decide which explanation of a decision to trust given that there are many, potentially conflicting, possible explanations for any given decision.",['Martin Strobel'],"['National University Singapore, Singapore, Singapore']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
An Education Model of Reasonable and Good-Faith Effort for Autonomous Systems,"In this paper we propose a framework for conceptualizing and demonstrating a good-faith effort when developing autonomous systems. The framework addresses two fundamental problems facing autonomous systems: (1) the disconnect between human-mental models and machine-based sensors and algorithms,  and (2) unpredictability in complex systems. We address these problems using a mix of education - explicitly delineating the mapping between human concepts and their machine equivalents in a structured manner - and data sampling with expected ranges as a testing mechanism. ","['Cindy M. Grimm', 'William D. Smart', 'Woodrow Hartzog']","['Oregon State University, Corvallis, OR, USA', 'Oregon State University, Corvallis, OR, USA', 'Northeastern University, Boston, MA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
An Empirical Approach to Capture Moral Uncertainty in AI,As AI Systems become increasingly autonomous they are expected to engage in complex moral decision-making processes. For the purpose of guidance of such processes theoretical and empirical solutions have been sought. In this research we integrate both theoretical and empirical lines of thought to address the matters of moral reasoning in AI Systems. We reconceptualize a metanormative framework for decision-making under moral uncertainty within the Discrete Choice Analysis domain and we operationalize it through a latent class choice model. The discrete choice analysis-based formulation of the metanormative framework is theory-rooted and practical as it captures moral uncertainty through a small set of latent classes. To illustrate our approach we conceptualize a society in which AI Systems are in charge of making policy choices. In the proof of concept two AI systems make policy choices on behalf of a society but while one of the systems uses a baseline moral certain model the other uses a moral uncertain model. It was observed that there are cases in which the AI Systems disagree about the policy to be chosen which we believe is an indication about the relevance of moral uncertainty.,"['Andreia Martinho', 'Maarten Kroesen', 'Caspar Chorus']","['Delft University of Technology, Delft, Netherlands', 'Delft University of Technology, Delft, Netherlands', 'Delft University of Technology, Delft, Netherlands']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
An Empirical Study of Rich Subgroup Fairness for Machine Learning,"Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.","['Michael Kearns', 'Seth Neel', 'Aaron Roth', 'Zhiwei Steven Wu']","['Department of Computer and Information Sciences, University of Pennsylvania', 'Department of Statistics, University of Pennsylvania', 'Department of Computer and Information Sciences, University of Pennsylvania', 'Department of Computer Science and Engineering, University of Minnesota']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
"An empirical study on the perceived fairness of realistic, imperfect machine learning models","There are many competing definitions of what statistical properties make a machine learning model fair. Unfortunately, research has shown that some key properties are mutually exclusive. Realistic models are thus necessarily imperfect, choosing one side of a trade-off or the other. To gauge perceptions of the fairness of such realistic, imperfect models, we conducted a between-subjects experiment with 502 Mechanical Turk workers. Each participant compared two models for deciding whether to grant bail to criminal defendants. The first model equalized one potentially desirable model property, with the other property varying across racial groups. The second model did the opposite. We tested pairwise trade-offs between the following four properties: accuracy; false positive rate; outcomes; and the consideration of race. We also varied which racial group the model disadvantaged. We observed a preference among participants for equalizing the false positive rate between groups over equalizing accuracy. Nonetheless, no preferences were overwhelming, and both sides of each trade-off we tested were strongly preferred by a non-trivial fraction of participants. We observed nuanced distinctions between participants considering a model ""unbiased"" and considering it ""fair."" Furthermore, even when a model within a trade-off pair was seen as fair and unbiased by a majority of participants, we did not observe consensus that a machine learning model was preferable to a human judge. Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations.","['Galen Harrison', 'Julia Hanson', 'Christine Jacinto', 'Julio Ramirez', 'Blase Ur']","['University of Chicago', 'University of Chicago', 'University of Chicago', 'University of Chicago', 'University of Chicago']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
An Invitation to System-wide Algorithmic Fairness,"We propose a framework for analyzing and evaluating system-wide algorithmic fairness. The core idea is to use simulation techniques in order to extend the scope of current fairness assessments by incorporating context and feedback to a phenomenon of interest. By doing so, we expect to better understand the interaction among the social behavior giving rise to discrimination, automated decision making tools, and fairness-inspired statistical constraints. In particular, we invite the community to use agent based models as an explanatory tool for causal mechanisms of population level properties. We also propose embedding these into a reinforcement learning algorithm to find optimal actions for meaningful change. As an incentive for taking a system-wide approach , we show through a simple model of predictive policing and trials that if we limit our attention to one portion of the system, we may determine some blatantly unfair practices as fair, and be blind to overall unfairness.","['Efrén Cruz Cortés', 'Debashis Ghosh']","['The Pennsylvania State University, State College, PA, USA', 'University of Colorado Anschutz, Aurora, CO, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
"Analyze, Detect and Remove Gender Stereotyping from Bollywood Movies","The presence of gender stereotypes in many aspects of society is a well-known phenomenon. In this paper, we focus on studying such stereotypes and bias in Hindi movie industry (\it Bollywood) and propose an algorithm to remove these stereotypes from text. We analyze movie plots and posters for all movies released since 1970. The gender bias is detected by semantic modeling of plots at sentence and intra-sentence level. Different features like occupation, introductions, associated actions and descriptions are captured to show the pervasiveness of gender bias and stereotype in movies. Using the derived semantic graph, we compute centrality of each character and observe similar bias there. We also show that such bias is not applicable for movie posters where females get equal importance even though their character has little or no impact on the movie plot. The silver lining is that our system was able to identify 30 movies over last 3 years where such stereotypes were broken. The next step, is to generate debiased stories. The proposed debiasing algorithm extracts gender biased graphs from unstructured piece of text in stories from movies and de-bias these graphs to generate plausible unbiased stories. ","Nishtha Madaan, Sameep Mehta, Taneea Agrawaal, Vrinda Malhotra, Aditi Aggarwal, Yatin Gupta, Mayank Saxena","[""IBM Research India"", ""IIIT Delhi"", ""MSIT Delhi"", ""DTU Delhi""]",FAT* 2018,2018,TRUE
Analyzing Biases in Perception of Truth in News Stories and Their Implications for Fact Checking,"Recently, social media sites like Facebook and Twitter have been severely criticized by policy makers, and media watchdog groups for allowing fake news stories to spread unchecked on their platforms. In response, these sites are encouraging their users to report any news story they encounter on the site, which they perceive as fake. Stories that are reported as fake by a large number of users are prioritized for fact checking by (human) experts at fact checking organizations like Snopes and PolitiFact. Thus, social media sites today are relying on their users' perceptions of the truthfulness of news stories to select stories to fact check. However, few studies have focused on understanding how users perceive truth in news stories, or how biases in their perceptions might affect current strategies to detect and label fake news stories. To this end, we present an in-depth analysis on users' perceptions of truth in news stories. Specifically, we analyze users' truth perception biases for 150 stories fact checked by Snopes. Based on their ground truth and the truth value perceived by users, we can classify the stories into four categories -- (i) C1: false stories perceived as false by most users, (ii) C2: true stories perceived as false by most users, (iii) C3: false stories perceived as true by most users, and (iv) C4: true stories perceived as true by most users. The stories that are likely to be reported (flagged) for fact checking are from the two classes C1 and C2 that have the lowest perceived truth levels. We argue that there is little to be gained by fact checking stories from C1 whose truth value is correctly perceived by most users. Although stories in C2 reveal the cynicality of users about true stories, social media sites presently do not explicitly mark them as true to resolve the confusion. On the contrary, stories in C3 are false stories, yet perceived as true by most users. Arguably, these stories are more damaging than C1 because the truth values of the the story in former situation is incorrectly perceived while truth values of the latter is correctly perceived. Nevertheless, the stories in C1 is likely to be fact checked with greater priority than the stories in C3! In fact, in today's social media sites, the higher the gullibility of users towards believing a false story, the less likely it is to be reported for fact checking. In summary, we make the following contributions in this work. 1. Methodological: We develop a novel method for assessing users' truth perceptions of news stories. We design a test for users to rapidly assess (i.e., at the rate of a few seconds per story) how truthful or untruthful the claims in a news story are. We then conduct our truth perception tests on-line and gather truth perceptions of 100 US-based Amazon Mechanical Turk workers for each story. 2. Empirical: Our exploratory analysis of users' truth perceptions reveal several interesting insights. For instance, (i) for many stories, the collective wisdom of the crowd (average truth rating) differs significantly from the actual truth of the story, i.e., wisdom of crowds is inaccurate, (ii) across different stories, we find evidence for both false positive perception bias (i.e., a gullible user perceiving the story to be more true than it is in reality) and false negative perception bias (i.e., a cynical user perceiving a story to be more false than it is in reality), and (iii) users' political ideologies influence their truth perceptions for the most controversial stories, it is frequently the result of users' political ideologies influencing their truth perceptions. 3. Practical: Based on our observations, we call for prioritizing stories to fact check in order to achieve the following three important goals: (i) Remove false news stories from circulation, (ii) Correct the misperception of the users, and (iii) Decrease the disagreement between different users' perceptions of truth. Finally, we provide strategies which utilize users' truth perceptions (and predictive analysis of their biases) to achieve the three goals stated above while prioritizing stories for fact checking. The full paper is available at: https://bit.ly/2T7raFO","['Mahmoudreza Babaei', 'Abhijnan Chakraborty', 'Juhi Kulshrestha', 'Elissa M. Redmiles', 'Meeyoung Cha', 'Krishna P. Gummadi']","['MPI-SWS, Germany', 'MPI-SWS, Germany', 'GESIS, Germany', 'University of Maryland, US', 'KAIST, South Korea', 'MPI-SWS, Germany']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Analyzing Federated Learning through an Adversarial Lens,"Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server. In this work, we explore the threat of model poisoning attacks on federated learning initiated by a single, non-colluding malicious agent where the adversarial objective is to cause the model to misclassify a set of chosen inputs with high confidence. We explore a number of strategies to carry out this attack, starting with simple boosting of the malicious agent's update to overcome the effects of other agents' updates. To increase attack stealth, we propose an alternating minimization strategy, which alternately optimizes for the training loss and the adversarial objective. We follow up by using parameter estimation for the benign agents' updates to improve on attack success. Finally, we use a suite of interpretability techniques to generate visual explanations of model decisions for both benign and malicious models and show that the explanations are nearly visually indistinguishable. Our results indicate that even a highly constrained adversary can carry out model poisoning attacks while simultaneously maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develop effective defense strategies.","Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, Seraphin Calo",IBM,ICML (2019),2019,TRUE
Anatomy of a Privacy-Safe Large-Scale Information Extraction System Over Email,"Extracting structured data from emails can enable several assistive experiences, such as reminding the user when a bill payment is due, answering queries about the departure time of a booked flight, or proactively surfacing an emailed discount coupon while the user is at that store.
This paper presents Juicer, a system for extracting information from email that is serving over a billion Gmail users daily. We describe how the design of the system was informed by three key principles: scaling to a planet-wide email service, isolating the complexity to provide a simple experience for the developer, and safeguarding the privacy of users (our team and the developers we support are not allowed to view any single email). We describe the design tradeoffs made in building this system, the challenges faced and the approaches used to tackle them. We present case studies of three extraction tasks implemented on this platformâbill reminders, commercial offers, and hotel reservationsâto illustrate the effectiveness of the platform despite challenges unique to each task. Finally, we outline several areas of ongoing research in large-scale machine-learned information extraction from email.","['Ying Sheng', 'Sandeep Tata', 'James B. Wendt', 'Jing Xie', 'Qi Zhao', 'Marc Najork']",Google,24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2018),2018,TRUE
Arbiter: A Domain-Specific Language for Ethical Machine Learning,"The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.","['Julian Zucker', ""Myraeka d'Leeuwen""]","['Northeastern University, Boston, MA, USA', 'Northeastern University, Boston, MA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Artificial Artificial Intelligence: Measuring Influence of AI 'Assessments' on Moral Decision-Making,"Given AI's growing role in modeling and improving decision-making, how and when to present users with feedback is an urgent topic to address. We empirically examined the effect of feedback from false AI on moral decision-making about donor kidney allocation. We found some evidence that judgments about whether a patient should receive a kidney can be influenced by feedback about participants' own decision-making perceived to be given by AI, even if the feedback is entirely random. We also discovered different effects between assessments presented as being from human experts and assessments presented as being from AI.","['Lok Chan', 'Kenzie Doyle', 'Duncan McElfresh', 'Vincent Conitzer', 'John P. Dickerson', 'Jana Schaich Borg', 'Walter Sinnott-Armstrong']","['Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA', 'University of Maryland, College Park, MD, USA', 'Duke University, Durham, NC, USA', 'University of Maryland, College Park, MD, USA', 'Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Artificial Intelligence and Indigenous Perspectives: Protecting and Empowering Intelligent Human Beings,"As 'control' is increasingly ceded to AI systems, potentially Artificial General Intelligence (AGI) humanity may be facing an identity crisis sooner rather than later, whereby the notion of 'intelligence' no longer remains solely our own. This paper characterizes the problem in terms of an impending loss of control and proposes a relational shift in our attitude towards AI. The shortcomings of value alignment as a solution to the problem are outlined which necessitate an extension of these principles. One such approach is considering strongly relational Indigenous epistemologies. The value of Indigenous perspectives has not been canvassed widely in the literature. Their utility becomes clear when considering the existence of well-developed epistemologies adept at accounting for the non-human, a task that defies Western anthropocentrism. Accommodating AI by considering it as part of our network is a step towards building a symbiotic relationship. Given that AGI questions our fundamental notions of what it means to have human rights, it is argued that in order to co-exist, we find assistance in Indigenous traditions such as the Hawaiian and Lakota ontologies. Lakota rituals provide comfort with the conception of non-human soul-bearer while Hawaiian stories provide possible relational schema to frame our relationship with AI.",['Suvradip Maitra'],"['University of Queensland, Brisbane, QLD, Australia']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Artificial mental phenomena: psychophysics as a framework to detect perception biases in AI models,"Detecting biases in artificial intelligence has become difficult because of the impenetrable nature of deep learning. The central difficulty is in relating unobservable phenomena deep inside models with observable, outside quantities that we can measure from inputs and outputs. For example, can we detect gendered perceptions of occupations (e.g., female librarian, male electrician) using questions to and answers from a word embedding-based system? Current techniques for detecting biases are often customized for a task, dataset, or method, affecting their generalization. In this work, we draw from Psychophysics in Experimental Psychology---meant to relate quantities from the real world (i.e., ""Physics"") into subjective measures in the mind (i.e., ""Psyche"")---to propose an intellectually coherent and generalizable framework to detect biases in AI. Specifically, we adapt the two-alternative forced choice task (2AFC) to estimate potential biases and the strength of those biases in black-box models. We successfully reproduce previously-known biased perceptions in word embeddings and sentiment analysis predictions. We discuss how concepts in experimental psychology can be naturally applied to understanding artificial mental phenomena, and how psychophysics can form a useful methodological foundation to study fairness in AI.","['Lizhen Liang', 'Daniel E. Acuna']","['Syracuse University', 'Syracuse University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Assessing algorithmic fairness with unobserved protected class using data combination,"The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts, or performance disparities in general, in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, that includes protected class labels but not decisions or outcomes. We show that a variety of common disparity measures are generally unidentifiable aside for some unrealistic cases, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the sharpest-possible partial identification set of disparities either under no assumptions or when we incorporate mild smoothness constraints. We further provide optimization-based algorithms for computing and visualizing these sets of simultaneously achievable pairwise disparties for assessing disparities that arise between multiple groups, which enables reliable and robust assessments - an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.","['Nathan Kallus', 'Xiaojie Mao', 'Angela Zhou']","['Cornell University', 'Cornell University', 'Cornell University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Assessing National Development Plans for Alignment with Sustainable Development Goals via Semantic Search,"The United Nations Development Programme (UNDP) helps countries implement the United Nations (UN) Sustainable Development Goals (SDGs), an agenda for tackling major societal issues such as poverty, hunger, and environmental degradation by the year 2030. A key service provided by UNDP to countries that seek it is a review of national development plans and sector strategies by policy experts to assess alignment of national targets with one or more of the 169 targets of the 17 SDGs. Known as the Rapid Integrated Assessment (RIA), this process involves manual review of hundreds, if not thousands, of pages of documents and takes weeks to complete. In this work, we develop a natural language processing-based methodology to accelerate the work-flow of policy experts. Specifically we use paragraph embedding techniques to find paragraphs in the documents that match the semantic concepts of each of the SDG targets. One novel technical contribution of our work is in our use of historical RIAs from other countries as a form of neighborhood-based supervision for matches in the country under study. We have successfully piloted the algorithm to perform the RIA for Papua New Guineas national plan, with the UNDP estimating it will help reduce their completion time from an estimated 3-4 weeks to 3 days. Assessing National Development Plans for... (PDF Download Available). Available from: https://www.researchgate.net/publication/322071708_Assessing_National_Development_Plans_for_Alignment_with_Sustainable_Development_Goals_via_Semantic_Search [accessed Apr 11 2018].","J. Galsurkar, M. Singh, L. Wu, A. Vempaty, M. Sushkov, D. Iyer, S. Kapto, K. Varshney",IBM,AAAI (2018),2018,TRUE
Assessing Post-hoc Explainability of the BKT Algorithm,"As machine intelligence is increasingly incorporated into educational technologies, it becomes imperative for instructors and students to understand the potential flaws of the algorithms on which their systems rely. This paper describes the design and implementation of an interactive post-hoc explanation of the Bayesian Knowledge Tracing algorithm which is implemented in learning analytics systems used across the United States. After a user-centered design process to smooth out interaction design difficulties, we ran a controlled experiment to evaluate whether the interactive or static version of the explainable led to increased learning. Our results reveal that learning about an algorithm through an explainable depends on users' educational background. For other contexts, designers of post-hoc explainables must consider their users' educational background to best determine how to empower more informed decision-making with AI-enhanced systems.","['Tongyu Zhou', 'Haoyu Sheng', 'Iris Howley']","['Williams College, Williamstown, MA, USA', 'Williams College, Williamstown, MA, USA', 'Williams College, Williamstown, MA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning,"Visual language grounding is widely studied in modern neural image captioning systems, which typically adopts an encoder-decoder framework consisting of two principal components: a convolutional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in machine vision and perception, we propose Show-and-Fool, a novel algorithm for crafting adversarial examples in neural image captioning. The proposed algorithm provides two evaluation approaches, which check whether neural image captioning systems can be mislead to output some randomly chosen captions or keywords. Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords, and the adversarial examples can be made highly transferable to other image captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.","H. Chen, H. Zhang, P.-Y. Chen, J. Yi and C.-J. Hsieh",IBM,ACL 2018,2017,TRUE
Attribute-based Propensity for Unbiased Learning in Recommender Systems: Algorithm and Case Studies,"Many modern recommender systems train their models based on a large amount of implicit user feedback data. Due to the inherent bias in this data (e.g., position bias), learning from it directly can lead to suboptimal models. Recently, unbiased learning was proposed to address such problems by leveraging counterfactual techniques like inverse propensity weighting (IPW). In these methods, propensity scores estimation is usually limited to item's display position in a single user interface (UI).","['Zhen Qin', 'Don Metzler', 'Xuanhui Wang']",Google,26TH ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) (2020),2020,TRUE
Audio De-identification: A New Entity Recognition Task,"Named Entity Recognition (NER) has been mostly studied in the context of written text. Specifically, NER is an important step in de-identification (de-ID) of medical records, many of which are recorded conversations between a patient and a doctor. In such recordings, audio spans with personal information should be redacted, similar to the redaction of sensitive character spans in de-ID for written text. The application of NER in the context of audio de-identification has yet to be fully investigated. To this end, we define the task of audio de-ID, in which audio spans with entity mentions should be detected. We then present our pipeline for this task, which involves Automatic Speech Recognition (ASR), NER on the transcript text, and text-to-audio alignment. Finally, we introduce a novel metric for audio de-ID and a new evaluation benchmark consisting of a large labeled segment of the Switchboard and Fisher audio datasets and detail our pipeline's results on it.","['Ido Cohn', 'Itay Laish', 'Genady Beryozkin', 'Gang Li', 'Izhak Shafran', 'Idan Szpektor', 'Avinatan Hassidim', 'Yossi Matias']",Google,NAACL (2019),2019,TRUE
Auditing Algorithms: On Lessons Learned and the Risks of Data Minimization,"In this paper, we present the Algorithmic Audit (AA) of REM!X, a personalized well-being recommendation app developed by Telefónica Innovación Alpha. The main goal of the AA was to identify and mitigate algorithmic biases in the recommendation system that could lead to the discrimination of protected groups. The audit was conducted through a qualitative methodology that included five focus groups with developers and a digital ethnography relying on users comments reported in the Google Play Store. To minimize the collection of personal information, as required by best practice and the GDPR [1], the REM!X app did not collect gender, age, race, religion, or other protected attributes from its users. This limited the algorithmic assessment and the ability to control for different algorithmic biases. Indirect evidence was thus used as a partial mitigation for the lack of data on protected attributes, and allowed the AA to identify four domains where bias and discrimination were still possible, even without direct personal identifiers. Our analysis provides important insights into how general data ethics principles such as data minimization, fairness, non-discrimination and transparency can be operationalized via algorithmic auditing, their potential and limitations, and how the collaboration between developers and algorithmic auditors can lead to better technologies","['Gemma Galdon Clavell', 'Mariano Martín Zamorano', 'Carlos Castillo', 'Oliver Smith', 'Aleksandar Matic']","['Eticas Research and Consulting, Barcelona, Spain', 'Eticas Research and Consulting, Barcelona, Spain', 'Pompeu Fabra University, Barcelona, Spain', 'ALPHA Telefónica, Barcelona, Spain', 'ALPHA Telefónica, Barcelona, Spain']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
Auditing radicalization pathways on YouTube,"Non-profits, as well as the media, have hypothesized the existence of a radicalization pipeline on YouTube, claiming that users systematically progress towards more extreme content on the platform. Yet, there is to date no substantial quantitative evidence of this alleged pipeline. To close this gap, we conduct a large-scale audit of user radicalization on YouTube. We analyze 330,925 videos posted on 349 channels, which we broadly classified into four types: Media, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right. According to the aforementioned radicalization hypothesis, channels in the I.D.W. and the Alt-lite serve as gateways to fringe far-right ideology, here represented by Alt-right channels. Processing 72M+ comments, we show that the three channel types indeed increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube's recommendation algorithm, looking at more than 2M video and channel recommendations between May/July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels, while Alt-right videos are reachable only through channel recommendations. Overall, we paint a comprehensive picture of user radicalization on YouTube.","['Manoel Horta Ribeiro', 'Raphael Ottoni', 'Robert West', 'Virgílio A. F. Almeida', 'Wagner Meira']","['EPFL', 'UFMG', 'EPFL', 'UFMG', 'UFMG']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Automated Test Generation to Detect Individual Discrimination in AI Models,"Dependability on AI models is of utmost importance to ensure full acceptance of the AI systems. One of the key aspects of the dependable AI system is to ensure that all its decisions are fair and not biased towards any individual. In this paper, we address the problem of detecting whether a model has an individual discrimination. Such a discrimination exists when two individuals who differ only in the values of their protected attributes (such as, gender/race) while the values of their non-protected ones are exactly the same, get different decisions. Measuring individual discrimination requires an exhaustive testing, which is infeasible for a non-trivial system. In this paper, we present an automated technique to generate test inputs, which is geared towards finding individual discrimination. Our technique combines the well-known technique called symbolic execution along with the local explainability for generation of effective test cases. Our experimental results clearly demonstrate that our technique produces 3.72 times more successful test cases than the existing state-of-the-art across all our chosen benchmarks.","Aniya Agarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, Diptikalyan Saha",IBM,"IEEE Journal of Selected Topics in Signal Processing, August 2018",2018,TRUE
Avoiding Disparity Amplification under Different Worldviews,"We mathematically compare four competing definitions of group-level nondiscrimination: demographic parity, equalized odds, predictive parity, and calibration. Using the theoretical framework of Friedler et al., we study the properties of each definition under various worldviews, which are assumptions about how, if at all, the observed data is biased. We argue that different worldviews call for different definitions of fairness, and we specify the worldviews that, when combined with the desire to avoid a criterion for discrimination that we call disparity amplification, motivate demographic parity and equalized odds. We also argue that predictive parity and calibration are insufficient for avoiding disparity amplification because predictive parity allows an arbitrarily large inter-group disparity and calibration is not robust to post-processing. Finally, we define a worldview that is more realistic than the previously considered ones, and we introduce a new notion of fairness that corresponds to this worldview.","['Samuel Yeom', 'Michael Carl Tschantz']","['Carnegie Mellon University', 'International Computer Science Institute']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Awareness in practice: tensions in access to sensitive attribute data for antidiscrimination,"Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted. This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities. This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.","['Miranda Bogen', 'Aaron Rieke', 'Shazeda Ahmed']","['Upturn', 'Upturn', 'University of California']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Balanced Neighborhoods for Multi-sided Fairness in Recommendation,"Fairness has emerged as an important category of analysis for machine learning systems in some application areas. In extending the concept of fairness to recommender systems, there is an essential tension between the goals of fairness and those of personalization. However, there are contexts in which equity across recommendation outcomes is a desirable goal. It is also the case that in some applications fairness may be a multisided concept, in which the impacts on multiple groups of individuals must be considered. In this paper, we examine two different cases of fairness-aware recommender systems: consumer-centered and provider-centered. We explore the concept of a balanced neighborhood as a mechanism to preserve personalization in recommendation while enhancing the fairness of recommendation outcomes. We show that a modified version of the Sparse Linear Method (SLIM) can be used to improve the balance of user and item neighborhoods, with the result of achieving greater outcome fairness in real-world datasets with minimal loss in ranking performance. ","Robin Burke, Nasim Sonboli, Aldo Ordonez-Gauger",DePaul University,FAT* 2018,2018,FALSE
Balancing Competing Objectives with Noisy Data: Score-Based Classifiers for Welfare-Aware Machine Learning,"While real-world decisions involve many competing objectives, algorithmic decisions are often evaluated with a single objective function. In this paper, we study algorithmic policies which explicitly trade off between a private objective (such as profit) and a public objective (such as social welfare). We analyze a natural class of policies which trace an empirical Pareto frontier based on learned scores, and focus on how such decisions can be made in noisy or data-limited regimes. Our theoretical results characterize the optimal strategies in this class, bound the Pareto errors due to inaccuracies in the scores, and show an equivalence between optimal strategies and a rich class of fairness-constrained profit-maximizing policies. We then present empirical results in two different contexts --- online content recommendation and sustainable abalone fisheries --- to underscore the generality of our approach to a wide range of practical decisions. Taken together, these results shed light on inherent trade-offs in using machine learning for decisions that impact social welfare.","Esther Rolf, Max Simchowitz, Sarah Dean, Lydia T. Liu, Daniel Bjorkegren, Moritz Hardt, Joshua Blumenstock ","[""UC Berkeley"" , ""Brown University""]",ICML 2020,2020-07,FALSE
Balancing Explicability and Explanations for Human-Aware Planning,"Human-aware  planning  involves  generating  plans that  are  explicable  as  well  as  providing  explanations when such plans cannot be found.  In this paper, we bring these two concepts together and show how an agent can achieve a trade-off between these two competing characteristics of a plan. In order to achieve this, we conceive a first of its kind planner MEGA that can augment the possibility of explaining a plan in the plan generation process itself. We situate our discussion in the context of recent work on explicable planning and explanation generation,  and illustrate these concepts in two well- known planning domains,  as well as in a demonstration  of  a  robot  in  a  typical  search  and  reconnaissance task. Human factor studies in the latter highlight the usefulness of the proposed approach.","Tathagata Chakraborti, Sarath Sreedharan, Subbarao Kambhampati",IBM,IJCAI (2019),2019,TRUE
Balancing Privacy and Utility with Pattern Based Activity Detection: Extended Abstract,"The diffusion of surveillance cameras often leads to conflicts between utility, that is, the benefits of preserving the information the camera records, and privacy, that is, the ability for the people being observed to conceal information they want to protect. For example, a camera monitoring an office kitchen may be useful in identifying a food thief, but might unintentionally reveal the PIN someone enters on a mobile phone. We design a video processing system that detects private activities in surveillance video and filters them out of the recording with minimal disruption of video quality. At the core of our system is the light-weight computation of a fixed-size feature that describes the spatio-temporal aspects of human activities that extend over variable amounts of time and space. Converting events of variable length and extent to a fixed-size descriptor makes it possible to use off-the-shelf classifiers to recognize and localize activities to be protected from recording. Comparisons of our descriptor with several alternatives show improved performance with less computation. We contribute two new video datasets recorded with a kitchen security camera, and we carry out a pilot user study to show that PIN theft is a valid concern.",['Cassandra Carley'],"['Duke University, Durham, NC, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Balancing the Benefits of Autonomous Vehicles,"Autonomous vehicles are regularly touted as holding the potential to provide significant benefits for diverse populations. There are significant technological barriers to be overcome, but as those are solved, autonomous vehicles are expected to reduce fatalities; decrease emissions and pollutants; provide new options to mobility-challenged individuals; enable people to use their time more productively; and so much more. In this paper, we argue that these high expectations for autonomous vehicles almost certainly cannot be fully realized. More specifically, the proposed benefits divide into two high-level groups, centered around efficiency and safety improvements, and increases in people's agency and autonomy. The first group of benefits is almost always framed in terms of rates: fatality rates, traffic flow per mile, and so forth. However, we arguably care about the absolute numbers for these measures, not the rates; number of fatalities is the key metric, not fatality rate per vehicle mile traveled. Hence, these potential benefits will be reduced, perhaps to non-existence, if autonomous vehicles lead to increases in vehicular usage. But that is exactly the result that we should expect if the second group of benefits is realized: if people's agency and autonomy is increased, then they will use vehicles more. There is an inevitable tension between the benefits that are proposed for autonomous vehicles, such that we cannot fully have all of them at once. We close by pointing towards other types of AI technologies where we should expect to find similar types of necessary and inevitable tradeoffs between classes of benefits.","['Timothy Geary', 'David Danks']","['California State University, Monterey Bay, Monterey, CA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Balancing the Tradeoff Between Clustering Value and Interpretability,"Graph clustering groups entities -- the vertices of a graph -- based on their similarity, typically using a complex distance function over a large number of features. Successful integration of clustering approaches in automated decision-support systems hinges on the interpretability of the resulting clusters. This paper addresses the problem of generating interpretable clusters, given features of interest that signify interpretability to an end-user, by optimizing interpretability in addition to common clustering objectives. We propose a β-interpretable clustering algorithm that ensures that at least β fraction of nodes in each cluster share the same feature value. The tunable parameter β is user-specified. We also present a more efficient algorithm for scenarios with β\!=\!1$ and analyze the theoretical guarantees of the two algorithms. Finally, we empirically demonstrate the benefits of our approaches in generating interpretable clusters using four real-world datasets. The interpretability of the clusters is complemented by generating simple explanations denoting the feature values of the nodes in the clusters, using frequent pattern mining.","['Sandhya Saisubramanian', 'Sainyam Galhotra', 'Shlomo Zilberstein']","['University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Balancing the Tradeoff between Profit and Fairness in Rideshare Platforms during High-Demand Hours,"Rideshare platforms, when assigning requests to drivers, tend to maximize profit for the system and/or minimize waiting time for riders. Such platforms can exacerbate biases that drivers may have over certain types of requests. We consider the case of peak hours when the demand for rides is more than the supply of drivers. Drivers are well aware of their advantage during the peak hours and can choose to be selective about which rides to accept. Moreover, if in such a scenario, the assignment of requests to drivers (by the platform) is made only to maximize profit and/or minimize wait time for riders, requests of a certain type (e.g., from a non-popular pickup location, or to a non-popular drop-off location) might never be assigned to a driver. Such a system can be highly unfair to riders. However, increasing fairness might come at a cost of the overall profit made by the rideshare platform. To balance these conflicting goals, we present a flexible, non-adaptive algorithm, NAdap, that allows the platform designer to control the profit and fairness of the system via parameters α and β respectively.We model the matching problem as an online bipartite matching where the set of drivers is offline and requests arrive online. Upon the arrival of a request, we use NAdap to assign it to a driver (the driver might then choose to accept or reject it) or reject the request. We formalize the measures of profit and fairness in our setting and show that by using NAdap, the competitive ratios for profit and fairness measures would be no worse than α/e and β/e respectively. Extensive experimental results on both real-world and synthetic datasets confirm the validity of our theoretical lower bounds. Additionally, they show that NAdap under some choice of (α, β) can beat two natural heuristics, Greedy and Uniform, on both fairness and profit. Code is available at: https://github.com/nvedant07/rideshare-fairness-peak/. Full paper can be found in the proceedings of AAAI 2020 and on ArXiv: http://arxiv.org/abs/1912.08388).","['Vedant Nanda', 'Pan Xu', 'Karthik Abinav Sankararaman', 'John P. Dickerson', 'Aravind Srinivasan']","['University of Maryland & MPI-SWS, College Park, MD, USA', 'New Jersey Institute of Technology, Newark, NJ, USA', 'University of Maryland & Facebook, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Bayesian Inference of Temporal Specifications to Explain How Plans Differ,"Temporal logics are useful for describing dynamic system behavior, and have been successfully used as a language for goal definitions during task planning. Prior works on inferring temporal logic specifications have focused on ``summarizing'' the input dataset -- i.e., finding specifications that are satisfied by all plan traces belonging to the given set. In this paper, we examine the problem of inferring specifications that describe temporal differences between two sets of plan traces. We formalize the concept of providing such \emph{contrastive} explanations, then present a Bayesian probabilistic model for inferring contrastive explanations as linear temporal logic specifications. We demonstrate the efficacy, scalability, and robustness of our model for inferring correct specifications across various benchmark planning domains and for a simulated air combat mission.","Joseph Kim, Christian Muise, Ankit Shah, Shubham Agarwal, Julie A. Shah",IBM,IJCAI (2019),2019,TRUE
Bayesian Sensitivity Analysis for Offline Policy Evaluation,"On a variety of complex decision-making tasks, from doctors prescribing treatment to judges setting bail, machine learning algorithms have been shown to outperform expert human judgments. One complication, however, is that it is often difficult to anticipate the effects of algorithmic policies prior to deployment, as one generally cannot use historical data to directly observe what would have happened had the actions recommended by the algorithm been taken. A common strategy is to model potential outcomes for alternative decisions assuming that there are no unmeasured confounders (i.e., to assume ignorability). But if this ignorability assumption is violated, the predicted and actual effects of an algorithmic policy can diverge sharply. In this paper we present a flexible Bayesian approach to gauge the sensitivity of predicted policy outcomes to unmeasured confounders. In particular, and in contrast to past work, our modeling framework easily enables confounders to vary with the observed covariates. We demonstrate the efficacy of our method on a large dataset of judicial actions, in which one must decide whether defendants awaiting trial should be required to pay bail or can be released without payment.","['Jongbin Jung', 'Ravi Shroff', 'Avi Feller', 'Sharad Goel']","['Stanford University, Stanford, CA, USA', 'New York University, New York, NY, USA', 'University of California, Berkeley, Berkeley, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Better Together?: How Externalities of Size Complicate Notions of Solidarity and Actuarial Fairness,"Consider a cost-sharing game with players of different costs: an example might be an insurance company calculating premiums for a population of mixed-risk individuals. Two natural and competing notions of fairness might be to a) charge each individual the same or b) charge each individual according to the cost that they bring to the pool. In the insurance literature, these approaches are referred to as ""solidarity"" and ""actuarial fairness"" and are commonly viewed as opposites. However, in insurance (and many other natural settings), the cost-sharing game also exhibits externalities of size: all else being equal, larger groups have lower average cost. In the insurance case, we analyze model where costs strictly decreases with pooling due to a reduction in the variability of losses. In this paper, we explore how this complicates traditional understandings of fairness, drawing on literature in cooperative game theory. First, we explore solidarity: we show that it is possible for both groups (high risk and low risk) to strictly benefit by joining an insurance pool where costs are evenly split, as opposed to being in separate risk pools. We build on this by producing a pricing scheme that maximally subsidizes the high risk group, while maintaining an incentive for lower risk people to stay in the insurance pool. Next, we demonstrate that with this new model, the price charged to each individual has to depend on the risk of other participants, making naive actuarial fairness inefficient. Furthermore, we prove that stable pricing schemes must be ones where players have the antisocial incentive desiring riskier partners, contradicting motivations for using actuarial fairness. Finally, we describe how these results relate to debates about fairness in machine learning and potential avenues for future research.","['Kate Donahue', 'Solon Barocas']","['Cornell University', 'Microsoft Research and Cornell University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society,"One way of carving up the broad 'AI ethics and society' research space that has emerged in recent years is to distinguish between 'near-term' and 'long-term' research. While such ways of breaking down the research space can be useful, we put forward several concerns about the near/long-term distinction gaining too much prominence in how research questions and priorities are framed. We highlight some ambiguities and inconsistencies in how the distinction is used, and argue that while there are differing priorities within this broad research community, these differences are not well-captured by the near/long-term distinction. We unpack the near/long-term distinction into four different dimensions, and propose some ways that researchers can communicate more clearly about their work and priorities using these dimensions. We suggest that moving towards a more nuanced conversation about research priorities can help establish new opportunities for collaboration, aid the development of more consistent and coherent research agendas, and enable identification of previously neglected research areas.","['Carina Prunkl', 'Jess Whittlestone']","['University of Oxford, Oxford, United Kingdom', 'University of Cambridge, Cambridge , United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing,"Data too sensitive to be ""open"" for analysis and re-purposing typically remains ""closed"" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.","['Meg Young', 'Luke Rodriguez', 'Emily Keller', 'Feiyang Sun', 'Boyang Sa', 'Jan Whittington', 'Bill Howe']","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting,"We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators---such as first names and pronouns---in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are ""scrubbed,"" and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.","['Maria De-Arteaga', 'Alexey Romanov', 'Hanna Wallach', 'Jennifer Chayes', 'Christian Borgs', 'Alexandra Chouldechova', 'Sahin Geyik', 'Krishnaram Kenthapadi', 'Adam Tauman Kalai']","['Carnegie Mellon University', 'University of Massachusetts Lowell', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'Carnegie Mellon University', 'LinkedIn', 'LinkedIn', 'Microsoft Research']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Bias in word embeddings,"Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings. We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data. We investigate bias diffusion and prove that existing biases are transferred to further machine learning models. We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient. Finally, we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data. Given that word embeddings are widely used by commercial companies, we discuss the challenges and required actions towards fair algorithmic implementations and applications.","['Orestis Papakyriakopoulos', 'Simon Hegelich', 'Juan Carlos Medina Serrano', 'Fabienne Marco']","['Technical University of Munich, Munich, Germany', 'Technical University of Munich, Munich, Germany', 'Technical University of Munich, Munich, Germany', 'Technical University of Munich, Munich, Germany']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
"Biased Priorities, Biased Outcomes: Three Recommendations for Ethics-oriented Data Annotation Practices","In this paper, we analyze the relation between data-related biases and practices of data annotation, by placing them in the context of market economy. We understand annotation as a praxis related to the sensemaking of data and investigate annotation practices for vision models by focusing on the values that are prioritized by industrial decision-makers and practitioners. The quality of data is critical for machine learning models as it holds the power to (mis-)represent the population it is intended to analyze. For autonomous systems to be able to make sense of the world, humans first need to make sense of the data these systems will be trained on. This paper addresses this issue, guided by the following research questions: Which goals are prioritized by decision-makers at the data annotation stage? How do these priorities correlate with data-related bias issues? Focusing on work practices and their context, our research goal aims at understanding the logics driving companies and their impact on the performed annotations. The study follows a qualitative design and is based on 24 interviews with relevant actors and extensive participatory observations, including several weeks of fieldwork at two companies dedicated to data annotation for vision models in Buenos Aires, Argentina and Sofia, Bulgaria. The prevalence of market-oriented values over socially responsible approaches is argued based on three corporate priorities that inform work practices in this field and directly shape the annotations performed: profit (short deadlines connected to the strive for profit are prioritized over alternative approaches that could prevent biased outcomes), standardization (the strive for standardized and, in many cases, reductive or biased annotations to make data fit the products and revenue plans of clients), and opacity (related to client's power to impose their criteria on the annotations that are performed. Criteria that most of the times remain opaque due to corporate confidentiality). Finally, we introduce three elements, aiming at developing ethics-oriented practices of data annotation, that could help prevent biased outcomes: transparency (regarding the documentation of data transformations, including information on responsibilities and criteria for decision-making.), education (training on the potential harms caused by AI and its ethical implications, that could help data annotators and related roles adopt a more critical approach towards the interpretation and labeling of data), and regulations (clear guidelines for ethical AI developed at the governmental level and applied both in private and public organizations).","['Gunay Kazimzade', 'Milagros Miceli']","['Technische Universität Berlin, Berlin, Germany', 'Technische Universität Berlin, Berlin, Germany']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Biases in Generative Art: A Causal Look from the Lens of Art History,"With rapid progress in artificial intelligence (AI), popularity of generative art has grown substantially. From creating paintings to generating novel art styles, AI based generative art has showcased a variety of applications. However, there has been little focus concerning the ethical impacts of AI based generative art. In this work, we investigate biases in the generative art AI pipeline right from those that can originate due to improper problem formulation to those related to algorithm design. Viewing from the lens of art history, we discuss the socio-cultural impacts of these biases. Leveraging causal models, we highlight how current methods fall short in modeling the process of art creation and thus contribute to various types of biases. We illustrate the same through case studies, in particular those related to style transfer. To the best of our knowledge, this is the first extensive analysis that investigates biases in the generative art AI pipeline from the perspective of art history. We hope our work sparks interdisciplinary discussions related to accountability of generative art.","['Ramya Srinivasan', 'Kanji Uchino']","['Fujitsu Laboratories of America', 'Fujitsu Laboratories of America']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Bidding strategies with gender nondiscrimination constraints for online ad auctions,"Interactions between bids to show ads online can lead to an advertiser's ad being shown to more men than women even when the advertiser does not target towards men. We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism. We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination, proving our strategies to be optimal in some settings. We use simulations to understand other settings.","['Milad Nasr', 'Michael Carl Tschantz']","['University of Massachusetts Amherst', 'International Computer Science Institute']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Black Feminist Musings on Algorithmic Oppression,"This paper uses a theory of oppression to ground and extend algorithmic oppression. Algorithmic oppression is then situated through a Black feminist lens part of which entails highlighting the double bind of technology. To reconcile algorithmic oppression with respect to the fairness, accountability, and transparency community, I critique the language of the community. Lastly, I place algorithmic oppression in a broader conversation of feminist science, technology, and society studies to ground the discussion of ways forward through abolition and empowering marginalized communities.",['Lelia Marie Hampton'],"['EECS and CSAIL, MIT, Cambridge, MA, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,"Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.","['Jwala Dhamala', 'Tony Sun', 'Varun Kumar', 'Satyapriya Krishna', 'Yada Pruksachatkun', 'Kai-Wei Chang', 'Rahul Gupta']","['Amazon Alexa AI-NU, USA', 'UC Santa Barbara, USA', 'Amazon Alexa AI-NU, USA', 'Amazon Alexa AI-NU, USA', 'Amazon Alexa AI-NU, USA', 'Amazon Alexa AI-NU, UCLA USA', 'Amazon Alexa AI-NU, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Boolean Decision Rules via Column Generation,"This paper considers the learning of Boolean rules in either disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) or conjunctive normal form (CNF, AND-of-ORs) as an interpretable model for classification. An integer program is formulated to optimally trade classification accuracy for rule simplicity. Column generation (CG) is used to efficiently search over an exponential number of candidate clauses (conjunctions or disjunctions) without the need for heuristic rule mining. This approach also bounds the gap between the selected rule set and the best possible rule set on the training data. To handle large datasets, we propose an approximate CG algorithm using randomization. Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity trade-off in 7 out of 15 datasets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurate.","Sanjeeb (Sanjeeb) Dash, Oktay (Oktay) Gunluk, Dennis (Dennis) Wei",IBM,NeurIPS (2018),2018,TRUE
Bounding the fairness and accuracy of classifiers from population statistics,"We consider the study of a classification model whose properties are impossible to estimate using a validation set, either due to the absence of such a set or because access to the classifier, even as a black-box, is impossible. Instead, only aggregate statistics on the rate of positive predictions in each of several sub-populations are available, as well as the true rates of positive labels in each of these sub-populations. We show that these aggregate statistics can be used to lower-bound the discrepancy of a classifier, which is a measure that balances inaccuracy and unfairness. To this end, we define a new measure of unfairness, equal to the fraction of the population on which the classifier behaves differently, compared to its global, ideally fair behavior, as defined by the measure of equalized odds. We propose an efficient and practical procedure for finding the best possible lower bound on the discrepancy of the classifier, given the aggregate statistics, and demonstrate in experiments the empirical tightness of this lower bound, as well as its possible uses on various types of problems, ranging from estimating the quality of voting polls to measuring the effectiveness of patient identification from internet search queries. The code and data are available at https://github.com/sivansabato/bfa.","Sivan Sabato, Elad Yom-Tov","[""Ben-Gurion University"", ""Microsoft Research""]",ICML 2020,2020-07,TRUE
Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness,"Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.","['Jessie Finocchiaro', 'Roland Maio', 'Faidra Monachou', 'Gourab K Patro', 'Manish Raghavan', 'Ana-Andreea Stoica', 'Stratis Tsirtsis']","['CU Boulder', 'Columbia University', 'Stanford University', 'IIT Kharagpur', 'Cornell University', 'Columbia University', 'Max Planck Institute for Software Systems']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Bringing the People Back In: Contesting Benchmark Machine Learning Datasets,"In response to algorithmic unfairness embedded in sociotechnical systems, significant attention has been focused on the contents of machine learning datasets which have revealed biases towards white, cisgender, male, and Western data subjects. In contrast, comparatively less attention has been paid to the histories, values, and norms embedded in such datasets. In this work, we outline a research program - a genealogy of machine learning data - for investigating how and why these datasets have been created, what and whose values influence the choices of data to collect, the contextual and contingent conditions of their creation. We describe the ways in which benchmark datasets in machine learning operate as infrastructure and pose four research questions for these datasets. This interrogation forces us to ""bring the people back in"" by aiding us in understanding the labor embedded in dataset construction, and thereby presenting new avenues of contestation for other researchers encountering the data.",['Alex Hanna'],Google,"Participatory Approaches to Machine Learning, ICML 2020 Workshop (2020)",2020,TRUE
Building and Auditing Fair Algorithms: A Case Study in Candidate Screening,"Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of ""fairness"" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps. In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool. We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.","['Christo Wilson', 'Avijit Ghosh', 'Shan Jiang', 'Alan Mislove', 'Lewis Baker', 'Janelle Szary', 'Kelly Trindel', 'Frida Polli']","['Northeastern University', 'Northeastern University', 'Northeastern University', 'Northeastern University', 'pymetrics, inc.', 'pymetrics, inc.', 'pymetrics, inc.', 'pymetrics, inc.']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Building Calibrated Deep Models via Uncertainty Matching with Auxiliary Interval Predictors,"With rapid adoption of deep learning in high-regret applications, the question of when and how much to trust these models often arises, which drives the need to quantify the inherent uncertainties. While identifying all sources that account for the stochasticity of learned models is challenging, it is common to augment predictions with confidence intervals to convey the expected variations in a model's behavior. In general, we require confidence intervals to be well-calibrated, reflect the true uncertainties, and to be sharp. However, most existing techniques for obtaining confidence intervals are known to produce unsatisfactory results in terms of at least one of those criteria. To address this challenge, we develop a novel approach for building calibrated estimators. More specifically, we construct separate models for predicting the target variable, and for estimating the confidence intervals, and pose a bi-level optimization problem that allows the predictive model to leverage estimates from the interval estimator through an \textit{uncertainty matching} strategy. Using experiments in regression, time-series forecasting, and object localization, we show that our approach achieves significant improvements over existing uncertainty quantification methods, both in terms of model fidelity and calibration error.","Prasanna Sattigeri, Jayaraman J. Thiagarajan",IBM,AAAI (2020),2020,TRUE
Building Jiminy Cricket: An Architecture for Moral Agreements Among Stakeholders,"An autonomous system is constructed by a manufacturer, operates in a society subject to norms and laws, and is interacting with end-users. We address the challenge of how the moral values and views of all stakeholders can be integrated and reflected in the moral behavior of the autonomous system. We propose an artificial moral agent architecture that uses techniques from normative systems and formal argumentation to reach moral agreements among stakeholders. We show how our architecture can be used not only for ethical practical reasoning and collaborative decision-making, but also for the explanation of such moral behavior.","['Beishui Liao', 'Marija Slavkovik', 'Leendert van der Torre']","['Zhejiang University, Hangzhou, China', 'University of Bergen, Bergen, Norway', 'University of Luxembourg, Luxembourg City, Luxembourg']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
CAG: A Real-time Low-cost Enhanced-robustness High-transferability Content-aware Adversarial Attack Generator,"Deep neural networks (DNNs) are vulnerable to adversarial attack despite their tremendous success in many AI fields. Adversarial attack is a method that
causes the intended misclassfication by adding imperceptible perturbations to legitimate inputs. Researchers have developed numerous types of adversarial
attack methods. However, from the perspective of practical deployment, these methods suffer from several drawbacks such as long attack generating time, high
memory cost, insufficient robustness and low transferability. We propose a Content-aware Adversarial Attack Generator (CAG) to achieve real-time,
low-cost, enhanced-robustness and high-transferability adversarial attack. First, as a type of generative model-based attack, CAG shows significant
speedup (at least 500 times) in generating adversarial examples compared to the state-of-the-art attacks such as PGD and C\&W. CAG only needs a single
generative model to perform targeted attack to any targeted class. Because CAG encodes the label information into a trainable embedding layer, it differs from
prior generative model-based adversarial attacks that use n different copies of generative models for n different targeted classes. As a result, CAG
significantly reduces the required memory cost for generating adversarial examples. CAG can generate adversarial perturbations that focus on the critical
areas of input by integrating the class activation maps information in the training process, and hence improve the robustness of CAG attack against the
state-of-art adversarial defenses. In addition, CAG exhibits high transferability across different DNN classifier models in black-box attack
scenario by introducing random dropout in the process of generating perturbations. Extensive experiments on different datasets and DNN models have
verified the real-time, low-cost, enhanced-robustness, and high-transferability benefits of CAG.

    ",Jie Chen,IBM,AAAI (2020),2020,TRUE
"Cake, Death, and Trolleys: Dilemmas as benchmarks of ethical decision-making","Artificial intelligence (AI) systems are becoming part of our lives and societies. The more decisions such systems make for us, the more we need to ensure that the decisions they make have a positive individual and societal ethical impact. How can we estimate how good a system is at making ethical decisions? Benchmarking is used to evaluate how good a machine or a process performs with respect to industry bests. In this paper we argue that (some) ethical dilemmas can be used as benchmarks for estimating the ethical performance of an autonomous system. We advocate that an open source repository of such dilemmas should be maintained. We present a prototype of such a repository available at https://imdb. uib.no/dilemmaz/articles/all1.","['Edvard P. Bjørgen', 'Simen Madsen', 'Therese S. Bjørknes', 'Fredrik V. Heimsæter', 'Robin Håvik', 'Morten Linderud', 'Per-Niklas Longberg', 'Louise A. Dennis', 'Marija Slavkovik']","['University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Liverpool, Liverpool, United Kingdom', 'University of Bergen, Bergen, Norway']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Can You Fake It Until You Make It?: Impacts of Differentially Private Synthetic Data on Downstream Classification Fairness,"The recent adoption of machine learning models in high-risk settings such as medicine has increased demand for developments in privacy and fairness. Rebalancing skewed datasets using synthetic data created by generative adversarial networks (GANs) has shown potential to mitigate disparate impact on minoritized subgroups. However, such generative models are subject to privacy attacks that can expose sensitive data from the training dataset. Differential privacy (DP) is the current leading solution for privacy-preserving machine learning. Differentially private GANs (DP GANs) are often considered a potential solution for improving model fairness while maintaining privacy of sensitive training data. We investigate the impact of using synthetic images from DP GANs on downstream classification model utility and fairness. We demonstrate that existing DP GANs cannot simultaneously maintain model utility, privacy, and fairness. The images generated from GAN models trained with DP exhibit extreme decreases in image quality and utility which leads to poor downstream classification model performance. Our evaluation highlights the friction between privacy, fairness, and utility and how this directly translates into real loss of performance and representation in common machine learning settings. Our results show that additional work improving the utility and fairness of DP generative models is required before they can be utilized as a potential solution to privacy and fairness issues stemming from lack of diversity in the training dataset.","['Victoria Cheng', 'Vinith M. Suriyakumar', 'Natalie Dullerud', 'Shalmali Joshi', 'Marzyeh Ghassemi']","['Vector Institute, University of Toronto, Snap Inc.', 'Vector Institute, University of Toronto', 'Vector Institute, University of Toronto', 'Vector Institute', 'Vector Institute, University of Toronto Canadian CIFAR AI Chair']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Case study: predictive fairness to reduce misdemeanor recidivism through social service interventions,"The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses. Often due to constraints of caseload and poor record linkage, prior interactions with an individual may not be considered when an individual comes back into the system, let alone in a proactive manner through the application of diversion programs. The Los Angeles City Attorney's Office recently created a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing recidivism in this population. Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting. The program seeks to improve outcomes by developing individually-tailored social service interventions (i.e., diversions, conditional plea agreements, stayed sentencing, or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods) for individuals likely to experience subsequent interactions with the criminal justice system, a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case. Seeking to achieve both efficiency (through predictive accuracy) and equity (improving outcomes in traditionally under-served communities and working to mitigate existing disparities in criminal justice outcomes), we discuss the equity outcomes we seek to achieve, describe the corresponding choice of a metric for measuring predictive fairness in this context, and explore a set of options for balancing equity and efficiency when building and selecting machine learning models in an operational public policy setting.","['Kit T. Rodolfa', 'Erika Salomon', 'Lauren Haynes', 'Iván Higuera Mendieta', 'Jamie Larson', 'Rayid Ghani']","['Carnegie Mellon University', 'University of Chicago', 'University of Chicago', 'University of Chicago', ""Los Angeles City Attorney's Office"", 'Carnegie Mellon University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation,"NLP models are shown to suffer from robustness issues, for example, a model's prediction can be easily changed under small perturbations to the input. In this work, we aim to present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, it can generate adversarial texts through controllable attributes that are known to be invariant to task labels. For example, for a main task like sentiment classification, an example attribute can be different categories/domains, and a model should have similar performance across them; for a coreference resolution task, a model's performance should not differ across different demographic attributes. Different from many existing adversarial text generation approaches, we show that our model can generate adversarial texts that are more fluent, diverse, and with better task-label invariance guarantees. We aim to use this model to generate counterfactual texts that could better improve robustness in NLP models (e.g., through adversarial training), and we argue that our generation can create more natural attacks.","['Xuezhi Wang', 'Alex Beutel', 'Ed H. Chi']",Google,EMNLP 2020,2020,TRUE
Causal Modeling for Fairness In Dynamical Systems,"In many applications areas---lending, education, and online recommenders, for example---fairness and equity concerns emerge when a machine learning system interacts with a dynamically changing environment to produce both immediate and long-term effects for individuals and demographic groups. We discuss causal directed acyclic graphs (DAGs) as a unifying framework for the recent literature on fairness in such dynamical systems. We show that this formulation affords several new directions of inquiry to the modeler, where sound causal assumptions can be expressed and manipulated. We emphasize the importance of computing interventional quantities in the dynamical fairness setting, and show how causal assumptions enable simulation (when environment dynamics are known) and estimation by adjustment (when dynamics are unknown) of intervention on short- and long-term outcomes, at both the group and individual levels. ","Elliot Creager, David Madras, Toniann Pitassi, Richard Zemel ","[""University of Toronto"", ""Vector Institute""]",ICML 2020,2020-07,FALSE
Censorship of Online Encyclopedias: Implications for NLP Models,"While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them.","['Eddie Yang', 'Margaret E. Roberts']","['University of California, San Diego La Jolla, California', 'University of California, San Diego La Jolla, California']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-box Models,"Concerns within the machine learning community and external pressures from regulators over the vulnerabilities of machine learning algorithms have spurred on the fields of explainability, robustness, and fairness. Often, issues in explainability, robustness, and fairness are confined to their specific sub-fields and few tools exist for model developers to use to simultaneously build their modeling pipelines in a transparent, accountable, and fair way. This can lead to a bottleneck on the model developer's side as they must juggle multiple methods to evaluate their algorithms. In this paper, we present a single framework for analyzing the robustness, fairness, and explainability of a classifier. The framework, which is based on the generation of counterfactual explanations through a custom genetic algorithm, is flexible, model-agnostic, and does not require access to model internals. The framework allows the user to calculate robustness and fairness scores for individual models and generate explanations for individual predictions which provide a means for actionable recourse (changes to an input to help get a desired outcome). This is the first time that a unified tool has been developed to address three key issues pertaining towards building a responsible artificial intelligence system.","['Shubham Sharma', 'Jette Henderson', 'Joydeep Ghosh']","['University of Texas at Austin, Austin, TX, USA', 'CognitiveScale, Austin, TX, USA', 'CognitiveScale, Austin, TX, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
Characterizing Audio Adversarial Examples Using Temporal Dependency,"Recent studies have highlighted adversarial examples as a ubiquitous threat to different neural network models and many downstream applications. Nonetheless, as unique data properties have inspired distinct and powerful learning principles, this paper aims to explore their potentials towards mitigating adversarial inputs. In particular, our results reveal the importance of using the temporal dependency in audio data to gain discriminate power against adversarial examples. Tested on the automatic speech recognition (ASR) tasks and three recent audio adversarial attacks, we find that (i) input transformation developed from image adversarial defense provides limited robustness improvement and is subtle to advanced attacks; (ii) temporal dependency can be exploited to gain discriminative power against audio adversarial examples and is resistant to adaptive attacks considered in our experiments. Our results not only show promising means of improving the robustness of ASR systems, but also offer novel insights in exploiting domain-specific data properties to mitigate negative effects of adversarial examples.","Zhuolin Yang, Bo Li, Pin-Yu Chen, Dawn Song ",IBM,ICLR (2019),2019,TRUE
Characterizing Membership Privacy in Stochastic Gradient Langevin Dynamics,"Bayesian deep learning is recently regarded as an intrinsic way to characterize the weight uncertainty of deep neural networks~(DNNs). Stochastic Gradient Langevin Dynamics~(SGLD) is an effective method to enable Bayesian deep learning on large-scale datasets. Previous theoretical studies have shown various appealing properties of SGLD, ranging from the convergence properties to the generalization bounds. In this paper, we study the properties of SGLD from a novel perspective of membership privacy protection (ie, preventing the membership attack). The membership attack, which aims to determine whether a specific sample is used for training a given DNN model, has emerged as a common threat against deep learning algorithms. To this end, we build a theoretical framework to analyze the information leakage (wrt the training dataset) of a model trained using SGLD. Based on this framework, we demonstrate that SGLD can prevent the information leakage of the training dataset to a certain extent. Moreover, our theoretical analysis can be naturally extended to other types of Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods. Empirical results on different datasets and models verify our theoretical findings and suggest that the SGLD algorithm can not only reduce the information leakage but also improve the generalization ability of the DNN models in real-world applications.","Bingzhe Wu, Chaochao Chen, Shi Wan Zhao, Cen Chen, Yuan Yao, Guangyu Sun, Li Wang, Xiaolu Zhang, Jun Zhou",IBM,AAAI (2020),2020,TRUE
Characterizing Sources of Uncertainty to Proxy Calibration and Disambiguate Annotator and Data Bias,"Supporting model interpretability for complex phenomena where annotators can legitimately disagree, such as emotion recognition, is a challenging machine learning task. In this work, we show that explicitly quantifying the uncertainty in such settings has interpretability benefits. We use a simple modification of a classical network inference using Monte Carlo dropout to give measures of epistemic and aleatoric uncertainty. We identify a significant correlation between aleatoric uncertainty and human annotator disagreement (r â .3). Additionally, we demonstrate how difficult and subjective training samples can be identified using aleatoric uncertainty and how epistemic uncertainty can reveal data bias that could result in unfair predictions. We identify the total uncertainty as a suitable surrogate for model calibration, i.e. the degree we can trust model's predicted confidence. In addition to explainability benefits, we observe modest performance boosts from incorporating model uncertainty.",['Brendan Jou'],Google,ICCV Workshop on Interpreting and Explaining Visual Artificial Intelligence Models (2019),2019,TRUE
Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings,"Machine learning models in health care are often deployed in settings where it is important to protect patient privacy. In such settings, methods for differentially private (DP) learning provide a general-purpose approach to learn models with privacy guarantees. Modern methods for DP learning ensure privacy through the addition of calibrated noise. The resulting privacy-preserving models are unable to learn too much information about the tails of a data distribution, resulting in a loss of accuracy that can disproportionately affect small groups. In this paper, we study the effects of DP learning in health care. We use state-of-the-art methods for DP learning to train privacy-preserving models in clinical prediction tasks, including x-ray classification of images and mortality prediction in time series data. We use these models to perform a comprehensive empirical investigation of the tradeoffs between privacy, utility, robustness to dataset shift and fairness. Our results highlight lesser-known limitations of methods for DP learning in health care, models that exhibit steep tradeoffs between privacy and utility, and models whose predictions are disproportionately influenced by large demographic groups in the training data. We discuss the costs and benefits of differentially private learning in health care with open directions for differential privacy, machine learning and health care.","['Vinith M. Suriyakumar', 'Nicolas Papernot', 'Anna Goldenberg', 'Marzyeh Ghassemi']","['University of Toronto, Vector Institute', 'University of Toronto, Vector Institute', 'University of Toronto, Vector Institute', 'University of Toronto, Vector Institute']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees,"Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts. Several recent works have focused on studying classification with respect to specific fairness metrics, modeled the corresponding fair classification problem as constrained optimization problems, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which there are no fair classifiers with theoretical guarantees; primarily because the resulting optimization problem is non-convex. The main contribution of this paper is a meta-algorithm for classification that can take as input a general class of fairness constraints with respect to multiple non-disjoint and multi-valued sensitive attributes, and which comes with provable guarantees. In particular, our algorithm can handle non-convex ""linear fractional"" constraints (which includes fairness constraints such as predictive parity) for which no prior algorithm was known. Key to our results is an algorithm for a family of classification problems with convex constraints along with a reduction from classification problems with linear fractional constraints to this family. Empirically, we observe that our algorithm is fast, can achieve near-perfect fairness with respect to various fairness metrics, and the loss in accuracy due to the imposed fairness constraints is often small.","['L. Elisa Celis', 'Lingxiao Huang', 'Vijay Keswani', 'Nisheeth K. Vishnoi']","['Yale University', 'EPFL', 'EPFL', 'Yale University']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
"Clear Sanctions, Vague Rewards: How China's Social Credit System Currently Defines ""Good"" and ""Bad"" Behavior","China's Social Credit System (SCS, 社会信用体系 or shehui xinyong tixi) is expected to become the first digitally-implemented nationwide scoring system with the purpose to rate the behavior of citizens, companies, and other entities. Thereby, in the SCS, ""good"" behavior can result in material rewards and reputational gain while ""bad"" behavior can lead to exclusion from material resources and reputational loss. Crucially, for the implementation of the SCS, society must be able to distinguish between behaviors that result in reward and those that lead to sanction. In this paper, we conduct the first transparency analysis of two central administrative information platforms of the SCS to understand how the SCS currently defines ""good"" and ""bad"" behavior. We analyze 194,829 behavioral records and 942 reports on citizens' behaviors published on the official Beijing SCS website and the national SCS platform ""Credit China"", respectively. By applying a mixed-method approach, we demonstrate that there is a considerable asymmetry between information provided by the so-called Redlist (information on ""good"" behavior) and the Blacklist (information on ""bad"" behavior). At the current stage of the SCS implementation, the majority of explanations on blacklisted behaviors includes a detailed description of the causal relation between inadequate behavior and its sanction. On the other hand, explanations on redlisted behavior, which comprise positive norms fostering value internalization and integration, are less transparent. Finally, this first SCS transparency analysis suggests that socio-technical systems applying a scoring mechanism might use different degrees of transparency to achieve particular behavioral engineering goals.","['Severin Engelmann', 'Mo Chen', 'Felix Fischer', 'Ching-yu Kao', 'Jens Grossklags']","['Technical University of Munich', 'Technical University of Munich', 'Technical University of Munich and IF, London', 'Fraunhofer Institute for Applied and Integrated Security', 'Technical University of Munich']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing,"Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source. In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.","['Inioluwa Deborah Raji', 'Andrew Smart', 'Rebecca N. White', 'Margaret Mitchell', 'Timnit Gebru', 'Ben Hutchinson', 'Jamila Smith-Loud', 'Daniel Theron', 'Parker Barnes']","['Partnership on AI', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI,"Many organizations have published principles intended to guide the ethical development and deployment of AI systems; however, their abstract nature makes them difficult to operationalize. Some organizations have therefore produced AI ethics checklists, as well as checklists for more specific concepts, such as fairness, as applied to AI systems. But unless checklists are grounded in practitioners’ needs, they may be misused. To understand the role of checklists in AI ethics, we conducted an iterative co-design process with 48 practitioners, focusing on fairness. We co-designed an AI fairness checklist and identified desiderata and concerns for AI fairness checklists in general. We found that AI fairness checklists could provide organizational infrastructure for formalizing ad-hoc processes and empowering individual advocates. We discuss aspects of organizational culture that may impact the efficacy of such checklists, and highlight future research directions.","['Michael Madaio', 'Luke Stark', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,CHI Conference on Human Factors in Computing Systems,2020-03-01,TRUE
Collaborative Human-AI (CHAI): Evidence-Based Interpretable Melanoma Classification in Dermoscopic Images,"Automated dermoscopic image analysis has witnessed rapid growth in diagnostic performance. Yet adoption faces resistance, in part, because no evidence is provided to support decisions. In this work, an approach for evidence-based classification is presented. A feature embedding is learned with CNNs, triplet-loss, and global average pooling, and used to classify via kNN search. Evidence is provided as both the discovered neighbors, as well as localized image regions most relevant to measuring distance between query and neighbors. To ensure that results are relevant in terms of both label accuracy and human visual similarity for any skill level, a novel hierarchical triplet logic is implemented to jointly learn an embedding according to disease labels and non-expert similarity. Results are improved over baselines trained on disease labels alone, as well as standard multiclass loss. Quantitative relevance of results, according to non-expert similarity, as well as localized image regions, are also significantly improved.","N. C. F. Codella, C.-C. Lin, A. Halpern, M. Hind, R. Feris and J. R. Smith",IBM,MICCAI (2018),2018,TRUE
Collaborative Machine Learning with Incentive-Aware Model Rewards,"Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party's contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party's reward based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy fairness and trade off between the desirable properties via an adjustable parameter. The value of each party's model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets.","Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, Bryan Kian Hsiang Low",National University of Singapore,ICML 2020,2020-07,FALSE
Companion Robots: the Hallucinatory Danger of Human-Robot Interactions,"The advent of the so-called Companion Robots is raising many ethical concerns among scholars and in the public opinion. Focusing mainly on robots caring for the elderly, in this paper we analyze these concerns to distinguish which are directly ascribable to robotic, and which are instead pre-existent. One of these is the ""deception objection"", namely the ethical unacceptability of deceiving the user about the simulated nature of the robot's behaviors. We argue on the inconsistency of this charge, as today formulated. After that, we underline the risk, for human-robot interaction, to become a hallucinatory relation where the human would subjectify the robot in a dynamic of meaning-overload. Finally, we analyze the definition of ""quasi-other"" relating to the notion of ""uncanny"". The goal of this paper is to argue that the main concern about Companion Robots is the simulation of a human-like interaction in the absence of an autonomous robotic horizon of meaning. In addition, that absence could lead the human to build a hallucinatory reality based on the relation with the robot.","['Piercosma Bisconti Lucidi', 'Daniele Nardi']","['Sapienza University of Rome, Rome, Italy', 'Sapienza University of Rome, Rome, Italy']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Compensation at the Crossroads: Autonomous Vehicles and Alternative Victim Compensation Schemes,"Over the last five years, a small but growing number of vehicle accidents involving fully or partially autonomous vehicles have raised a new and profoundly novel legal issue: who should be liable (if anyone) and how victims should be compensated (if at all) when a vehicle controlled by an algorithm rather than a human driver causes injury. The answer to this question has implications far beyond the resolution of individual autonomous vehicle crash cases. Whether the American legal system is capable of handling these cases fairly and efficiently implicates the likelihood that (a) consumers will adopt autonomous vehicles, and (b) the rate at which they will (or will not) do so. These implications should concern law and policy makers immensely. If autonomous cars stand to drastically reduce the number of fatalities and injuries on U.S. roadways-and virtually every scholar believes that they will-getting the adjudication and compensation aspect of autonomous vehicle injuries ""wrong,"" so to speak, risks stymieing adoption of this technology and leaving more Americans at risk of dying at the hands of human drivers.",['Tracy Hresko Pearl'],"['Texas Tech University School of Law, Lubbock, TX, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Computational Perspectives on Social Good and Access to Opportunity,"Computational techniques show immense promise to both deepen our understanding of socioeconomic inequality and inform interventions to alleviate it. With the increased prevalence of collaborations across disciplines and the availability of large data-sets, there is a wealth of areas in which nuanced questions and novel techniques can reveal powerful observations and point towards innovative solutions. In this piece, we highlight ways for using algorithmic, computational, and network-based techniques, in conjunction with insights from the social sciences, to improve access to opportunity for historically disadvantaged and under-served communities. We underline opportunities for work at the interface of these disciplines using examples from health, housing, and economic inequality.",['Rediet Abebe'],"['Cornell University, Ithaca, NY, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
"Computer Science Communities: Who is Speaking, and Who is Listening to the Women? Using an Ethics of Care to Promote Diverse Voices","Those working on policy, digital ethics and governance often refer to issues in 'computer science', that includes, but is not limited to, common subfields such as Artificial Intelligence (AI), Computer Science (CS) Computer Security (InfoSec), Computer Vision (CV), Human Computer Interaction (HCI), Information Systems, (IS), Machine Learning (ML), Natural Language Processing (NLP) and Systems Architecture. Within this framework, this paper is a preliminary exploration of two hypotheses, namely 1) Each community has differing inclusion of minoritised groups (using women as our test case, by identifying female-sounding names); and 2) Even where women exist in a community, they are not published representatively. Using data from 20,000 research records, totalling 503,318 names, preliminary data supported our hypothesis. We argue that ACM has an ethical duty of care to its community to increase these ratios, and to hold individual computing communities to account in order to do so, by providing incentives and a regular reporting system, in order to uphold its own Code.","['Marc Cheong', 'Kobi Leins', 'Simon Coghlan']","['Centre for AI and Digital Ethics, University of Melbourne Parkville, VIC, Australia', 'Centre for AI and Digital Ethics, University of Melbourne Parkville, VIC, Australia', 'Centre for AI and Digital Ethics, University of Melbourne Parkville, VIC, Australia']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Conservative Agency via Attainable Utility Preservation,"Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.","['Alexander Matt Turner', 'Dylan Hadfield-Menell', 'Prasad Tadepalli']","['Oregon State University, Corvallis, OR, USA', 'University of California, Berkeley, Berkeley, CA, USA', 'Oregon State University, Corvallis, OR, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Contextual Analysis of Social Media: The Promise and Challenge of Eliciting Context in Social Media Posts with Natural Language Processing,"While natural language processing affords researchers an opportunity to automatically scan millions of social media posts, there is growing concern that automated computational tools lack the ability to understand context and nuance in human communication and language. This article introduces a critical systematic approach for extracting culture, context and nuance in social media data. The Contextual Analysis of Social Media (CASM) ap-proach considers and critiques the gap between inadequacies in natural language processing tools and differences in geographic, cultural, and age-related variance of social media use and communication. CASM utilizes a team-based approach to analysis of social media data, explicitly informed by community expertise. We use of CASM to analyze Twitter posts from gang-involved youth in Chicago. We designed a set of experiments to evaluate the performance of a support vector machine us-ing CASM hand-labeled posts against a distant model. We found that the CASM-informed hand-labeled data outperforms the baseline distant labels, indicating that the CASM labels capture additional dimensions of information that content-only methods lack. We then question whether this is helpful or harmful for gun violence prevention.","['Desmond U. Patton', 'William R. Frey', 'Kyle A. McGregor', 'Fei-Tzin Lee', 'Kathleen McKeown', 'Emanuel Moss']","['Columbia University, New York, NY, USA', 'Columbia University, New York, NY, USA', 'Lankenau Institute for Medical Research, Philadelphia, PA, USA', 'Columbia University, New York, NY, USA', 'Columbia University, New York, NY, USA', 'City University of New York, New York, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Controlling Polarization in Personalization: An Algorithmic Framework,"Personalization is pervasive in the online space as it leads to higher efficiency for the user and higher revenue for the platform by individualizing the most relevant content for each user. However, recent studies suggest that such personalization can learn and propagate systemic biases and polarize opinions; this has led to calls for regulatory mechanisms and algorithms that are constrained to combat bias and the resulting echo-chamber effect. We propose a versatile framework that allows for the possibility to reduce polarization in personalized systems by allowing the user to constrain the distribution from which content is selected. We then present a scalable algorithm with provable guarantees that satisfies the given constraints on the types of the content that can be displayed to a user, but -- subject to these constraints -- will continue to learn and personalize the content in order to maximize utility. We illustrate this framework on a curated dataset of online news articles that are conservative or liberal, show that it can control polarization, and examine the trade-off between decreasing polarization and the resulting loss to revenue. We further exhibit the flexibility and scalability of our approach by framing the problem in terms of the more general diverse content selection problem and test it empirically on both a News dataset and the MovieLens dataset.","['L. Elisa Celis', 'Sayash Kapoor', 'Farnood Salehi', 'Nisheeth Vishnoi']","['Yale University', 'IIT Kanpur', 'École Polytechnique Fédérale de Lausanne (EPFL)', 'Yale University']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Corporate Social Responsibility via Multi-Armed Bandits,"We propose a multi-armed bandit setting where each arm corresponds to a subpopulation, and pulling an arm is equivalent to granting an opportunity to this subpopulation. In this setting the decision-maker's fairness policy governs the number of opportunities each subpopulation should receive, which typically depends on the (unknown) reward from granting an opportunity to this subpopulation. The decision-maker can decide whether to provide these opportunities, or pay a pre-defined monetary value for every withheld opportunity. The decision-maker's objective is to maximize her utility, which is the sum of rewards minus the cost paid for withheld opportunities. We provide a no-regret algorithm that maximizes the decision-maker's utility and complement our analysis with an almost-tight lower bound. Finally, we discuss the fairness policy and demonstrate its downstream implications on the utility and opportunities via simulations.","['Tom Ron', 'Omer Ben-Porat', 'Uri Shalit']","['Technion - Israel Institute of Technology', 'Tel-Aviv University', 'Technion - Israel Institute of Technology']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Costs and Benefits of Fair Representation Learning,"Machine learning algorithms are increasingly used to make or support important decisions about people's lives. This has led to interest in the problem of fair classification, which involves learning to make decisions that are non-discriminatory with respect to a sensitive variable such as race or gender. Several methods have been proposed to solve this problem, including fair representation learning, which cleans the input data used by the algorithm to remove information about the sensitive variable. We show that using fair representation learning as an intermediate step in fair classification incurs a cost compared to directly solving the problem, which we refer to as thecost of mistrust. We show that fair representation learning in fact addresses a different problem, which is of interest when the data user is not trusted to access the sensitive variable. We quantify the benefits of fair representation learning, by showing that any subsequent use of the cleaned data will not be too unfair. The benefits we identify result from restricting the decisions of adversarial data users, while the costs are due to applying those same restrictions to other data users.","['Daniel McNamara', 'Cheng Soon Ong', 'Robert C. Williamson']","['Australian National University & CSIRO Data61, Canberra, Australia', 'Australian National University & CSIRO Data61, Canberra, Australia', 'Australian National University & CSIRO Data61, Canberra, Australia']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Counterfactual Fairness in Text Classification through Robustness,"In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that ""Some people are gay"" is toxic while ""Some people are straight"" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.","['Sahaj Garg', 'Vincent Perot', 'Nicole Limtiaco', 'Ankur Taly', 'Ed H. Chi', 'Alex Beutel']","['Stanford University, Stanford, CA, USA', 'Google AI, New York, NY, USA', 'Google AI, New York, NY, USA', 'Google AI, Mountain View, CA, USA', 'Google AI, Mountain View, CA, USA', 'Google AI, New York, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,TRUE
"Counterfactual risk assessments, evaluation, and fairness","Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome. Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may---and as we show empirically, do---induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.","['Amanda Coston', 'Alan Mishler', 'Edward H. Kennedy', 'Alexandra Chouldechova']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Creating Fair Models of Atherosclerotic Cardiovascular Disease Risk,"Guidelines for the management of atherosclerotic cardiovascular disease (ASCVD) recommend the use of risk stratification models to identify patients most likely to benefit from cholesterol-lowering and other therapies. These models have differential performance across race and gender groups with inconsistent behavior across studies, potentially resulting in an inequitable distribution of beneficial therapy. In this work, we leverage adversarial learning and a large observational cohort extracted from electronic health records (EHRs) to develop a ""fair"" ASCVD risk prediction model with reduced variability in error rates across groups. We empirically demonstrate that our approach is capable of aligning the distribution of risk predictions conditioned on the outcome across several groups simultaneously for models built from high-dimensional EHR data. We also discuss the relevance of these results in the context of the empirical trade-off between fairness and model performance.","['Stephen Pfohl', 'Ben Marafino', 'Adrien Coulet', 'Fatima Rodriguez', 'Latha Palaniappan', 'Nigam H. Shah']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University & Université de Lorraine, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
"Crowdsourcing with Fairness, Diversity and Budget Constraints","Recent studies have shown that the labels collected from crowdworkers can be discriminatory with respect to sensitive attributes such as gender and race. This raises questions about the suitability of using crowdsourced data for further use, such as for training machine learning algorithms. In this work, we address the problem of fair and diverse data collection from a crowd under budget constraints. We propose a novel algorithm which maximizes the expected accuracy of the collected data, while ensuring that the errors satisfy desired notions of fairness. We provide guarantees on the performance of our algorithm and show that the algorithm performs well in practice through experiments on a real dataset.","['Naman Goel', 'Boi Faltings']","['Ecole Polytechnique Fédérale de Lausanne, Lausanne, Switzerland', 'Ecole Polytechnique Fédérale de Lausanne, Lausanne, Switzerland']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Data Augmentation for Discrimination Prevention and Bias Disambiguation,"Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an ""ideal world'' dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.","['Shubham Sharma', 'Yunfeng Zhang', 'Jesús M. Ríos Aliaga', 'Djallel Bouneffouf', 'Vinod Muthusamy', 'Kush R. Varshney']","['IBM Research & University of Texas at Austin, Austin, TX, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Austin, TX, USA', 'IBM Research, Yorktown Heights, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
Data Driven Techniques for Organizing Scientific Articles Relevant to Biomimicry,"Life on earth presents elegant solutions to many of the challenges innovators and entrepreneurs across disciplines face every day. To facilitate innovations inspired by nature, there is an emerging need for systems that bring relevant biological information to this application-oriented market. In this paper, we discuss our approach to assembling a system that uses machine learning techniques to assess a scientific article's potential usefulness to innovators, and classifies these articles in a way that helps innovators find information relevant to the challenges they are attempting to solve.","['Yuanshuo Zhao', 'Ioana Baldini', 'Prasanna Sattigeri', 'Inkit Padhi', 'Yoong Keok Lee', 'Ethan Smith']","['Georgia Institute of Technology, Atlanta, GA, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, IBM Research, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'Biomimicry Institute, Missoula, MT, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,TRUE
Data in New Delhi's predictive policing system,"In 2015, Delhi Police announced plans for predictive policing. The Crime Mapping, Analytics and Predictive System (CMAPS) would be implemented in India's capital, for live spatial hotspot mapping of crime, criminal behavior patterns and suspect analysis. Four years later, there is little known about the effect of CMAPS due to the lack of public accountability mechanisms and large exceptions for law enforcement under India's Right to Information Act. Through an ethnographic study of Delhi Police's data collection practices, and analysing the institutional and legal reality within which CMAPS will function, this paper presents one of the first accounts of smart policing in India. Through our findings and discussion we show what kinds of biases are present within Delhi Police's data collection practices currently and how they translate and transfer into initiatives like CMAPS. We further discuss what the biases in CMAPS can teach us about future public sector deployment of socio-technical systems in India and other global South geographies. We also offer methodological considerations for studying AI deployments in non-western contexts. We conclude with a set of recommendations for civil society and social justice actors to consider when engaging with opaque systems implemented in the public sector.","['Vidushi Marda', 'Shivangi Narayan']","['Article 19', 'Jawaharlal Nehru University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Data Leverage: A Framework for Empowering the Public in its Relationship with Technology Companies,"Many powerful computing technologies rely on implicit and explicit data contributions from the public. This dependency suggests a potential source of leverage for the public in its relationship with technology companies: by reducing, stopping, redirecting, or otherwise manipulating data contributions, the public can reduce the effectiveness of many lucrative technologies. In this paper, we synthesize emerging research that seeks to better understand and help people action this data leverage. Drawing on prior work in areas including machine learning, human-computer interaction, and fairness and accountability in computing, we present a framework for understanding data leverage that highlights new opportunities to change technology company behavior related to privacy, economic inequality, content moderation and other areas of societal concern. Our framework also points towards ways that policymakers can bolster data leverage as a means of changing the balance of power between the public and tech companies.","['Nicholas Vincent', 'Hanlin Li', 'Nicole Tilly', 'Stevie Chancellor', 'Brent Hecht']","['Northwestern University', 'Northwestern University', 'Northwestern University', 'University of Minnesota', 'Northwestern University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Data preprocessing to mitigate bias: A maximum entropy based approach,"Data containing human or social features may over- or under-represent groups with respect to salient social attributes such as gender or race, which can lead to biases in downstream applications. Prior approaches towards preprocessing data to mitigate such biases either reweigh the points in the dataset or set up a constrained optimization problem on the domain to minimize a metric of bias. However, the former do not learn a distribution over the entire domain and the latter do not scale well with the domain size. This paper presents an optimization framework that can be used as a data preprocessing method towards mitigating bias: It can learn distributions over large domains and controllably adjust the representation rates of protected groups and/or achieve target fairness metrics such as statistical parity, yet remains close to the empirical distribution induced by the given dataset. Our approach appeals to the principle of maximum entropy, which states that amongst all distributions satisfying a given set of constraints, we should choose the one closest in KL-divergence to a given prior. While maximum entropy distributions can succinctly encode distributions over large domains, they can be difficult to compute. Our main technical contribution is an instantiation of the maximum entropy framework for our set of constraints and priors, which encode our bias mitigation goals, that runs in time polynomial in the dimension of the data. Empirically, we observe that samples from the learned distribution have desired representation rates and statistical rates, and when used for training a classifier incurs only a slight loss in accuracy while maintaining fairness properties.","Elisa Celis, Vijay Keswani, Nisheeth Vishnoi",Yale University,ICML 2020,2020-07,FALSE
DeBayes: a Bayesian method for debiasing network embeddings,"As machine learning algorithms are increasingly deployed for high-impact automated decision making, ethical and increasingly also legal standards demand that they treat all individuals fairly, without discrimination based on their age, gender, race or other sensitive traits. In recent years much progress has been made on ensuring fairness and reducing bias in standard machine learning settings. Yet, for network embedding, with applications in vulnerable domains ranging from social network analysis to recommender systems, current options remain limited both in number and performance. We thus propose DeBayes: a conceptually elegant Bayesian method that is capable of learning debiased embeddings by using a biased prior. Our experiments show that these representations can then be used to perform link prediction that is significantly more fair in terms of popular metrics such as demographic parity and equalized opportunity.","Maarten Buyl, Tijl De Bie ",Ghent University,ICML 2020,2020-07,FALSE
Debiasing Embeddings for Fairer Text Classification,"(Bolukbasi et al., 2016) demonstrated that pre-trained  word embeddings  can  inherit  gender bias from the data they were trained on.  We investigate  how  this  bias  affects  downstream classification  tasks,  using  the  case  study  of occupation  classification  (De-Arteaga  et  al.,2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy  channel  for  communicating  gender information.   With  a  relatively  minor  adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and obtain high classification accuracy.","['Flavien Prost', 'Nithum Thain', 'Tolga Bolukbasi']",Google,1st ACL Workshop on Gender Bias for Natural Language Processing (2019),2019,TRUE
Decoupled Classifiers for Group-Fair and Efficient Machine Learning,"When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group. ","Cynthia Dwork, Nicole Immorlica, Adam Tauman Kalai, Max Leiserson","[""Harvard"", ""Microsoft Research"", ""University of Maryland""]",FAT* 2018,2018,TRUE
Deep determinantal generative classifier: robustness on noisy and adversarial samples,"Large-scale datasets may contain significant proportions of noisy (incorrect) class labels, and it is well-known that modern deep neural networks (DNNs) poorly generalize from such noisy training datasets. To mitigate the issue, we propose a novel inference method, termed Robust Generative classifier (RoG), applicable to any discriminative (e.g., softmax) neural classifier pre-trained on noisy datasets. In particular, we induce a generative classifier on top of hidden feature spaces of the pre-trained DNNs, for obtaining a more robust decision boundary. By estimating the parameters of generative classifier using the minimum covariance determinant estimator, we significantly improve the classification accuracy with neither re-training of the deep model nor changing its architectures. With the assumption of Gaussian distribution for features, we prove that RoG generalizes better than baselines under noisy labels. Finally, we propose the ensemble version of RoG to improve its performance by investigating the layer-wise characteristics of DNNs. Our extensive experimental results demonstrate the superiority of RoG given different learning models optimized by several training techniques to handle diverse scenarios of noisy labels.",[],Google,ICML (2019),2019,TRUE
Deep Weighted Averaging Classifiers,"Recent advances in deep learning have achieved impressive gains in classification accuracy on a variety of types of data, including images and text. Despite these gains, however, concerns have been raised about the calibration, robustness, and interpretability of these models. In this paper we propose a simple way to modify any conventional deep architecture to automatically provide more transparent explanations for classification decisions, as well as an intuitive notion of the credibility of each prediction. Specifically, we draw on ideas from nonparametric kernel regression, and propose to predict labels based on a weighted sum of training instances, where the weights are determined by distance in a learned instance-embedding space. Working within the framework of conformal methods, we propose a new measure of nonconformity suggested by our model, and experimentally validate the accompanying theoretical expectations, demonstrating improved transparency, controlled error rates, and robustness to out-of-domain data, without compromising on accuracy or calibration.","['Dallas Card', 'Michael Zhang', 'Noah A. Smith']","['Machine Learning Department, Carnegie Mellon University, Pittsburgh, Pennsylvania', 'Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, Washington', 'Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, Washington and Allen Institute for Artificial, Intelligence, Seattle, Washington']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Deepfakes for Medical Video De-Identification: Privacy Protection and Diagnostic Information Preservation,"Data sharing for medical research has been difficult as open-sourcing clinical data may violate patient privacy. Traditional methods for face de-identification wipe out facial information entirely, making it impossible to analyze facial behavior. Recent advancements on whole-body keypoints detection also rely on facial input to estimate body keypoints. Both facial and body keypoints are critical in some medical diagnoses, and keypoints invariability after de-identification is of great importance. Here, we propose a solution using deepfake technology, the face swapping technique. While this swapping method has been criticized for invading privacy and portraiture right, it could conversely protect privacy in medical video: patients' faces could be swapped to a proper target face and become unrecognizable. However, it remained an open question that to what extent the swapping de-identification method could affect the automatic detection of body keypoints. In this study, we apply deepfake technology to Parkinson's disease examination videos to de-identify subjects, and quantitatively show that: face-swapping as a de-identification approach is reliable, and it keeps the keypoints almost invariant, significantly better than traditional methods. This study proposes a pipeline for video de-identification and keypoint preservation, clearing up some ethical restrictions for medical data sharing. This work could make open-source high quality medical video datasets more feasible and promote future medical research that benefits our society.","['Bingquan Zhu', 'Hao Fang', 'Yanan Sui', 'Luming Li']","['Tsinghua University, Beijing, China', 'Tsinghua University, Beijing, China', 'Tsinghua University, Beijing, China', 'Tsinghua University, Beijing, China']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Defensive Quantization: When Efficiency Meets Robustness,"Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack.",Chuang Gan,IBM,ICLR (2019),2019,TRUE
Defining AI in Policy versus Practice,"Recent concern about harms of information technologies motivate consideration of regulatory action to forestall or constrain certain developments in the field of artificial intelligence (AI). However, definitional ambiguity hampers the possibility of conversation about this urgent topic of public concern. Legal and regulatory interventions require agreed-upon definitions, but consensus around a definition of AI has been elusive, especially in policy conversations. With an eye towards practical working definitions and a broader understanding of positions on these issues, we survey experts and review published policy documents to examine researcher and policy-maker conceptions of AI. We find that while AI researchers favor definitions of AI that emphasize technical functionality, policy-makers instead use definitions that compare systems to human thinking and behavior. We point out that definitions adhering closely to the functionality of AI systems are more inclusive of technologies in use today, whereas definitions that emphasize human-like capabilities are most applicable to hypothetical future technologies. As a result of this gap, ethical and regulatory efforts may overemphasize concern about future technologies at the expense of pressing issues with existing deployed technologies.","['P. M. Krafft', 'Meg Young', 'Michael Katell', 'Karen Huang', 'Ghislain Bugingo']","['University of Oxford, Oxford, United Kingdom', 'University of Washington, Seattle, WA, USA', 'University of Washington, Seattle, WA, USA', 'Harvard University, Cambridge, MA, USA', 'University of Washington, Seattle, WA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Degenerate Feedback Loops in Recommender Systems,"Machine learning is used extensively in recommender systems deployed in products. The decisions made by these systems can influence user beliefs and preferences which in turn affect the feedback the learning system receives - thus creating a feedback loop. This phenomenon can give rise to the so-called ""echo chambers"" or ""filter bubbles"" that have user and societal implications. In this paper, we provide a novel theoretical analysis that examines both the role of user dynamics and the behavior of recommender systems, disentangling the echo chamber from the filter bubble effect. In addition, we offer practical solutions to slow down system degeneracy. Our study contributes toward understanding and developing solutions to commonly cited issues in the complex temporal scenario, an area that is still largely unexplored.","['Ray Jiang', 'Silvia Chiappa', 'Tor Lattimore', 'András György', 'Pushmeet Kohli']","['DeepMind, London, United Kingdom', 'DeepMind, London, United Kingdom', 'DeepMind, London, United Kingdom', 'DeepMind, London, United Kingdom', 'DeepMind, London, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,TRUE
Deontological Ethics By Monotonicity Shape Constraints,"We demonstrate how easy it is for modern machine-learned systems to violate common deontological ethical principles and social norms such as ""favor the less fortunate,"" and ""do not penalize good attributes."" We propose that in some cases such ethical principles can be incorporated into a machine-learned model by adding shape constraints that constrain the model to respond only positively to relevant inputs. We analyze the relationship between these deontological constraints that act on individuals and the consequentialist group-based fairness goals of one-sided statistical parity and equal opportunity. This strategy works with sensitive attributes that are Boolean or real-valued such as income and age, and can help produce more responsible and trustworthy AI.",['Serena Wang'],Google,AISTATS (2020),2020,TRUE
Designing Accountable Systems,"Accountability is an often called for property of technical systems. It is a requirement for algorithmic decision systems, autonomous cyber-physical systems, and for software systems in general. As a concept, accountability goes back to the early history of Liberalism and is suggested as a tool to limit the use of power. This long history has also given us many, often slightly differing, definitions of accountability. The problem that software developers now face is to understand what accountability means for their systems and how to reflect it in a system's design. To enable the rigorous study of accountability in a system, we need models that are suitable for capturing such a varied concept. In this paper, we present a method to express and compare different definitions of accountability using Structural Causal Models. We show how these models can be used to evaluate a system's design and present a small use case based on an autonomous car.","['Severin Kacianka', 'Alexander Pretschner']","['Technical University of Munich, Garching, Germany', 'Technical University of Munich, Garching, Germany']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Designing an Online Infrastructure for Collecting AI Data From People With Disabilities,"AI technology offers opportunities to expand virtual and physical access for people with disabilities. However, an important part of bringing these opportunities to fruition is ensuring that upcoming AI technology works well for people with a wide range of abilities. In this paper, we identify the lack of data from disabled populations as one of the challenges to training and benchmarking fair and inclusive AI systems. As a potential solution, we envision an online infrastructure that can enable large-scale, remote data contributions from disability communities. We investigate the motivations, concerns, and challenges that people with disabilities might experience when asked to collect and upload various forms of AI-relevant data through a semi-structured interview and an online survey that simulated a data contribution process by collecting example data files through an online portal. Based on our findings, we outline design guidelines for developers creating online infrastructures for gathering data from people with disabilities.","['Joon Sung Park', 'Danielle Bragg', 'Ece Kamar', 'Meredith Ringel Morris']","['Microsoft Research - Redmond, Stanford University, Stanford, CA, USA', 'Microsoft Research - New England Cambridge, MA, USA', 'Microsoft Research - Redmond Redmond, WA, USA', 'Microsoft Research - Redmond Redmond, WA, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Designing Non-greedy Reinforcement Learning Agents with Diminishing Reward Shaping,"This paper intends to address an issue in RL that when agents possessing varying capabilities, most resources may be acquired by stronger agents, leaving the weaker ones ""starving"". We introduce a simple method to train non-greedy agents in multi-agent reinforcement learning scenarios with nearly no extra cost. Our model can achieve the following goals in designing the non-greedy agent:non-homogeneous equality, only need local information, cost-effective, generalizable and configurable. We propose the idea of diminishing reward that makes the agent feel less satisfied for consecutive rewards obtained. This idea allows the agents to behave less greedy with-out the need to explicitly coding any ethical pattern nor monitor other agents' status. Given our framework, resources distributed more equally without running the risk of reaching homogeneous equality. We designed two games, Gathering Game and Hunter Prey to evaluate the quality of the model.","['Fan-Yun Sun', 'Yen-Yu Chang', 'Yueh-Hua Wu', 'Shou-De Lin']","['National Taiwan University, Taipei, Taiwan ROC', 'National Taiwan University, Taipei, Taiwan ROC', 'National Taiwan University, Taipei, Taiwan ROC', 'National Taiwan University, Taipei, Taiwan ROC']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Designing Unbiased Surveys for HCI Research,"Surveys are a commonly used method within HCI research. While it initially appears easy and inexpensive to conduct surveys, overlooking key considerations in questionnaire design and the survey research process can yield skewed, biased, or entirely invalid survey results. Fortunately decades of academic research and analysis exist on optimizing the validity and reliability of survey data, from which this course will draw. To enable the creation of unbiased surveys, this course demonstrates questionnaire design biases and pitfalls, provides best practices for minimizing these, and reviews different uses of surveys within HCI.","['Hendrik Müller', 'Aaron Sedley']",Google,CHI '14 Extended Abstracts on Human Factors in Computing Systems (2014),2014,TRUE
Detecting Bias with Generative Counterfactual Face  Attribute Augmentation,"We introduce a simple framework for identifying biases of a smiling attribute classifier. Our method poses counterfactual questions of the form: how would the prediction change if this face characteristic had been different? We leverage recent advances in generative adversarial networks to build a realistic generative model of faces that affords controlled manipulation of specific facial characteristics. Empirically, we identify several different factors of variation (that we believe should be in-dependent of a smiling) that  affect the predictions of a smiling classifier trained on CelebA.",['Ben Hutchinson'],Google,"Fairness, Accountability, Transparency and Ethics in Computer Vision Workshop (in conjunction with CVPR) (2019)",2019,TRUE
Detecting discriminatory risk through data annotation based on Bayesian inferences,"Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination.","['Elena Beretta', 'Antonio Vetrò', 'Bruno Lepri', 'Juan Carlos De Martin']","['Nexa Center for Internet & Society, Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy Fondazione Bruno Kessler Trento, Italy', 'Nexa Center for Internet & Society, Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy', 'Fondazione Bruno Kessler, Trento, Italy', 'Nexa Center for Internet & Society, Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Detecting Out-of-Distribution Examples with Gram Matrices,"When presented with Out-of-Distribution (OOD) examples, deep neural networks yield confident, incorrect predictions. Detecting OOD examples is challenging, and the potential risks are high. In this paper, we propose to detect OOD examples by identifying inconsistencies between activity patterns and predicted class. We find that characterizing activity patterns by Gram matrices and identifying anomalies in Gram matrix values can yield high OOD detection rates. We identify anomalies in the Gram matrices by simply comparing each value with its respective range observed over the training data. Unlike many approaches, this can be used with any pre-trained softmax classifier and neither requires access to OOD data for fine-tuning hyperparameters, nor does it require OOD access for inferring parameters. We empirically demonstrate applicability across a variety of architectures and vision datasets and, for the important and surprisingly hard task of detecting far out-of-distribution examples, it generally performs better than or equal to state-of-the-art OOD detection methods (including those that do assume access to OOD examples).","Chandramouli Shama Sastry, Sageev Oore ",Dalhousie University/Vector Institute,ICML 2020,2020-07,FALSE
"Different ""Intelligibility"" for Different Folks","Many arguments have concluded that our autonomous technologies must be intelligible, interpretable, or explainable, even if that property comes at a performance cost. In this paper, we consider the reasons why some property like these might be valuable, we conclude that there is not simply one kind of 'intelligibility', but rather different types for different individuals and uses. In particular, different interests and goals require different types of intelligibility (or explanations, or other related notion). We thus provide a typography of 'intelligibility' that distinguishes various notions, and draw methodological conclusions about how autonomous technologies should be designed and deployed in different ways, depending on whose intelligibility is required.","['Yishan Zhou', 'David Danks']","['University of California San Diego, San Diego, CA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Differential Tweetment: Mitigating Racial Dialect Bias in Harmful Tweet Detection,"Automated systems for detecting harmful social media content are afflicted by a variety of biases, some of which originate in their training datasets. In particular, some systems have been shown to propagate racial dialect bias: they systematically classify content aligned with the African American English (AAE) dialect as harmful at a higher rate than content aligned with White English (WE). This perpetuates prejudice by silencing the Black community. Towards this problem we adapt and apply two existing bias mitigation approaches: preferential sampling pre-processing and adversarial debiasing in-processing. We analyse the impact of our interventions on model performance and propagated bias. We find that when bias mitigation is employed, a high degree of predictive accuracy is maintained relative to baseline, and in many cases bias against AAE in harmful tweet predictions is reduced. However, the specific effects of these interventions on bias and performance vary widely between dataset contexts. This variation suggests the unpredictability of autonomous harmful content detection outside of its development context. We argue that this, and the low performance of these systems at baseline, raise questions about the reliability and role of such systems in high-impact, real-world settings.","['Ari Ball-Burack', 'Michelle Seng Ah Lee', 'Jennifer Cobbe', 'Jatinder Singh']","['Compliant & Accountable Systems Group University of Cambridge, UK', 'Compliant & Accountable Systems Group University of Cambridge, UK', 'Compliant & Accountable Systems Group University of Cambridge, UK', 'Compliant & Accountable Systems Group University of Cambridge, UK']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Differentially Private Distributed Data Summarization under Covariate Shift,"We envision AI marketplaces to be platforms where consumers, with very less data for a target task, can obtain a relevant model by accessing many private data sources with vast number of data samples. One of the key challenges is to construct a training dataset that matches a target task without compromising on privacy of the data sources. To this end, we consider the following distributed data summarizataion problem. Given K private source datasets denoted by [Di]i∈[K] and a small target validation set Dv, which may involve a considerable covariate shift with respect to the sources, compute a summary dataset Ds⊆⋃i∈[K]Di such that its statistical distance from the validation dataset Dv is minimized. We use the popular Maximum Mean Discrepancy as the measure of statistical distance. The non-private problem has received considerable attention in prior art, for example in prototype selection (Kim et al., NIPS 2016). Our work is the first to obtain strong differential privacy guarantees while ensuring the quality guarantees of the non-private version. We study this problem in a Parsimonious Curator Privacy Model, where a trusted curator coordinates the summarization process while minimizing the amount of private information accessed. Our central result is a novel protocol that (a) ensures the curator accesses at most O(K13|Ds|+|Dv|) points (b) has formal privacy guarantees on the leakage of information between the data owners and (c) closely matches the best known non-private greedy algorithm. Our protocol uses two hash functions, one inspired by the Rahimi-Recht random features method and the second leverages state of the art differential privacy mechanisms. We introduce a novel noiseless differentially private auctioning protocol for winner notification and demonstrate the efficacy of our protocol using real-world datasets.","Kanthi Sarpatwar, Karthikeyan Shanmugam, Venkata Sitaramagiridharganesh Ganapavarapu, Ashish Jagmohan, Roman Vaculin",IBM,NeurIPS (2019),2019,TRUE
Differentially Private Fair Learning,"Motivated by settings in which predictive models may be required to be non-discriminatory with respect to certain attributes (such as race), but even collecting the sensitive attribute may be forbidden or restricted, we initiate the study of fair learning under the constraint of differential privacy. We design two learning algorithms that simultaneously promise differential privacy and equalized odds, a 'fairness' condition that corresponds to equalizing false positive and negative rates across protected groups. Our first algorithm is a private implementation of the equalized odds post-processing approach of [Hardt et al., 2016]. This algorithm is appealingly simple, but must be able to use protected group membership explicitly at test time, which can be viewed as a form of 'disparate treatment'. Our second algorithm is a differentially private version of the oracle-efficient in-processing approach of [Agarwal et al., 2018] that can be used to find the optimal fair classifier, given access to a subroutine that can solve the original (not necessarily fair) learning problem. This algorithm is more complex but need not have access to protected group membership at test time. We identify new tradeoffs between fairness, accuracy, and privacy that emerge only when requiring all three properties, and show that these tradeoffs can be milder if group membership may be used at test time. We conclude with a brief experimental evaluation. ","Matthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron Roth, Saeed Sharifi-Malvajerdi, Jonathan Ullman","[""University of Pennsylvania"", ""Northeastern University""]",ICML 2019,2019-06,FALSE
Diminishing Returns Shape Constraints for Interpretability and Regularization,"We investigate machine learning models that can provide diminishing returns
and accelerating returns guarantees to capture prior knowledge or policies
about how outputs should depend on inputs. We show that one can build
flexible, nonlinear, multi-dimensional models using lattice functions with any
combination of concavity/convexity and monotonicity constraints on any
subsets of features, and compare to new shape-constrained neural networks.
We demonstrate on real-world examples that these shape constrained models
can provide tuning-free regularization and improve model understandability.","['Dara Bahri', 'Andy Cotter', 'Kevin Canini']",Google,NIPS 2018 (2018),2018,TRUE
Direct Uncertainty Prediction for Medical Second Opinions,"The issue of disagreements amongst human experts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresponds to doctor disagreements on a patient diagnosis. In this work, we show that machine learning models can be successfully trained to give uncertainty scores to data instances that result in high expert disagreements. In particular, they can identify patient cases that would benefit most from a medical second opinion. Our central methodological finding is that Direct Uncertainty Prediction (DUP), training a model to predict an uncertainty score directly from the raw patient features, works better than Uncertainty Via Classification, the two step process of training a classifier and postprocessing the output distribution to give an uncertainty score. We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging application.","['Maithra Raghu', 'Rory Abbott Sayres']",Google,ICML (2019),2019,TRUE
Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability,"Nowadays, the use of machine learning models is becoming a utility in many applications. Companies deliver pre-trained models encapsulated as application programming interfaces (APIs) that developers combine with third-party components and their own models and data to create complex data products to solve specific problems. The complexity of such products and the lack of control and knowledge of the internals of each component used unavoidable cause effects, such as lack of transparency, difficulty in auditability, and the emergence of potential uncontrolled risks. They are effectively black-boxes. Accountability of such solutions is a challenge for the auditors and the machine learning community. In this work, we propose a wrapper that given a black-box model enriches its output prediction with a measure of uncertainty when applied to a target domain. To develop the wrapper, we follow these steps: Modeling the distribution of the output. In a text classification setting, the output is a probability distribution p(y|X, w*) over the different classes to predict, y, given an input text X and the pre-trained model with parameters w*. We model this output by a random variable to measure the variability that the data noise causes in the output. Here we consider the output distribution coming from a Dirichlet probability density function, thus p(y|X, w*) ~ Dir(α). Decomposition of the Dirichlet concentration parameter. To relate the output of the classifier with the concentration parameter in the Dirichlet distribution, we propose a decomposition of the concentration parameter in two terms: α = βy. The role of this scalar β is to control the spread of the distribution around the expected value, i.e. the original prediction y. Training the wrapper. Sentences are represented as the average value of their word embeddings. This representation feeds a neural network that outputs a single regression value that models the parameter β. For each input, we combine β and the black-box prediction to obtain the corresponding distribution for the output ym,i ~ Dir(αi). By using Monte Carlo sampling, we approximate the expected value of the classification probabilities, [EQUATION] and we train the model applying a cross-entropy loss over the predictions and the labels. Obtaining an uncertainty score from the wrapper. To obtain a numerical value for the uncertainty of a prediction, we draw samples from the resulting Dir(α) to evaluate the predictive entropy with [EQUATION], thus obtaining a numerical score for the uncertainty of each prediction. Using uncertainty for rejection. Based on this wrapper, we provide an actionable mechanism to mitigate risk in the form of decision rejection: once equipped with a value for the uncertainty of a given prediction, we can choose not to issue that prediction when the risk or uncertainty in that decision is significant. This results in a rejection system that selects the more confident predictions, discards those more uncertain, and leads to an improvement in the trustability of the resulting system. We showcase the proposed technique and methodology in a practical scenario where we apply a simulated sentiment analysis API based on NLP to different domains. On each experiment, we train a sentiment classifier using text reviews of products in a source domain. We apply the pre-trained black-box to obtain the predictions for the reviews from a target domain. The tuples of review plus black-box predictions are then used for training the wrapper to obtain the uncertainty. Finally, we use the uncertainty score to sort the predictions from more to less uncertain, and we search for a rejection point that maximizes the three performance measures: non-rejected accuracy, and classification and rejection quality. Experiments demonstrate the effectiveness of the uncertainty measure computed by the wrapper and shows its high correlation to bad quality predictions and misclassifications. In all the cases, the uncertainty metric here proposed outperforms traditional uncertainty measures.","['José Mena Roldán', 'Oriol Pujol Vila', 'Jordi Vitrià Marca']","['Universitat de Barcelona', 'Universitat de Barcelona', 'Universitat de Barcelona']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Discovering User Bias in Ordinal Voting Systems,"Crowdsourcing systems increasingly rely on users to provide more
subjective ground truth for intelligent systems - e.g. ratings, aspect
of quality and perspectives on how expensive or lively a place feels,
etc. We focus on the ubiquitous implementation of online user ordinal voting (e.g 1-5, 1 star-4 stars) on some aspect of an entity, to
extract a relative truth, measured by a selected metric such as vote
plurality or mean. We argue that this methodology can aggregate
results that yield little information to the end user. In particular,
ordinal user rankings often converge to a indistinguishable rating.
This is demonstrated by the trend in certain cities for the majority of restaurants to all have a 4 star rating. Similarly, the rating of an establishment can be significantly affected by a few users.
User bias in voting is not spam, but rather a preference that can
be harnessed to provide more information to users. We explore
notions of both global skew and user bias. Leveraging these bias
and preference concepts, the paper suggests explicit models for
better personalization and more informative ratings.","['Alyssa Whitlock Lees', 'Chris Welty']",Google,"SAD-2019: Workshop on Subjectivity, Ambiguity and Disagreement",2019,TRUE
Discrimination in Online Advertising: A Multidisciplinary Inquiry,"We explore ways in which discrimination may arise in the targeting of job-related advertising, noting the potential for multiple parties to contribute to its occurrence. We then examine the statutes and case law interpreting the prohibition on advertisements that indicate a preference based on protected class, and consider its application to online advertising. We focus on its interaction with Section 230 of the Communications Decency Act, which provides interactive computer services with immunity for providing access to information created by a third party. We argue that such services can lose that immunity if they target ads toward or away from protected classes without explicit instructions from advertisers to do so. ","Amit Datta, Anupam Datta, Jael Makagon, Deirdre K. Mulligan, Michael Carl Tschantz","[""Carnegie Mellon"", ""Berkeley"", ""International Computer Science Institute""]",FAT* 2018,2018,FALSE
Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments,"Despite vigorous debates about the technical characteristics of risk assessments being deployed in the U.S. criminal justice system, remarkably little research has studied how these tools affect actual decision-making processes. After all, risk assessments do not make definitive decisions---they inform judges, who are the final arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a first step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk. We found several behaviors that call into question the supposed efficacy and fairness of risk assessments: our study participants 1) underperformed the risk assessment even when presented with its predictions, 2) could not effectively evaluate the accuracy of their own or the risk assessment's predictions, and 3) exhibited behaviors fraught with ""disparate interactions,"" whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants. These results suggest the need for a new ""algorithm-in-the-loop"" framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract. If risk assessments are to be used at all, they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential.","['Ben Green', 'Yiling Chen']","['Harvard University', 'Harvard University']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Dissecting Racial Bias in an Algorithm that Guides Health Decisions for 70 Million People,"A single algorithm drives an important health care decision for over 70 million people in the US. When health systems anticipate that a patient will have especially complex and intensive future health care needs, she is enrolled in a 'care management' program, which provides considerable additional resources: greater attention from trained providers and help with coordination of her care. To determine which patients will have complex future health care needs, and thus benefit from program enrollment, many systems rely on an algorithmically generated commercial risk score. In this paper, we exploit a rich dataset to study racial bias in a commercial algorithm that is deployed nationwide today in many of the US's most prominent Accountable Care Organizations (ACOs). We document significant racial bias in this widely used algorithm, using data on primary care patients at a large hospital. Blacks and whites with the same algorithmic risk scores have very different realized health. For example, the highest-risk black patients (those at the threshold where patients are auto-enrolled in the program), have significantly more chronic illnesses than white enrollees with the same risk score. We use detailed physiological data to show the pervasiveness of the bias: across a range of biomarkers, from HbA1c levels for diabetics to blood pressure control for hypertensives, we find significant racial health gaps conditional on risk score. This bias has significant material consequences for patients: it effectively means that white patients with the same health as black patients are far more likely be enrolled in the care management program, and benefit from its resources. If we simulated a world without this gap in predictions, blacks would be auto-enrolled into the program at more than double the current rate. An unusual aspect of our dataset is that we observe not just the risk scores but also the input data and objective function used to construct it. This provides a unique window into the mechanisms by which bias arises. The algorithm is given a data frame with (1) Yit (label), total medical expenditures ('costs') in year t; and (2) Xi,t--1 (features), fine-grained care utilization data in year t -- 1 (e.g., visits to cardiologists, number of x-rays, etc.). The algorithm's predicted risk of developing complex health needs is thus in fact predicted costs. And by this metric, one could easily call the algorithm unbiased: costs are very similar for black and white patients with the same risk scores. So far, this is inconsistent with algorithmic bias: conditional on risk score, predictions do not favor whites or blacks. The fundamental problem we uncover is that when thinking about 'health care needs,' hospitals and insurers focus on costs. They use an algorithm whose specific objective is cost prediction, and from this perspective, predictions are accurate and unbiased. Yet from the social perspective, actual health -- not just costs -- also matters. This is where the problem arises: costs are not the same as health. While costs are a reasonable proxy for health (the sick do cost more, on average), they are an imperfect one: factors other than health can drive cost -- for example, race. We find that blacks cost more than whites on average; but this gap can be decomposed into two countervailing effects. First, blacks bear a different and larger burden of disease, making them costlier. But this difference in illness is offset by a second factor: blacks cost less, holding constant their exact chronic conditions, a force that dramatically reduces the overall cost gap. Perversely, the fact that blacks cost less than whites conditional on health means an algorithm that predicts costs accurately across racial groups will necessarily also generate biased predictions on health. The root cause of this bias is not in the procedure for prediction, or the underlying data, but the algorithm's objective function itself. This bias is akin to, but distinct from, 'mis-measured labels': it arises here from the choice of labels, not their measurement, which is in turn a consequence of the differing objective functions of private actors in the health sector and society. From the private perspective, the variable they focus on -- cost -- is being appropriately optimized. But our results hint at how algorithms may amplify a fundamental problem in health care as a whole: externalities produced when health care providers focus too narrowly on financial motives, optimizing on costs to the detriment of health. In this sense, our results suggest that a pervasive problem in health care -- incentives that induce health systems to focus on dollars rather than health -- also has consequences for the way algorithms are built and monitored.","['Ziad Obermeyer', 'Sendhil Mullainathan']","['UC Berkeley, Berkeley, CA', 'University of Chicago, Chicago, IL']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation,"Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque. We propose Distill-and-Compare, an approach to audit such models without probing the black-box model API or pre-defining features to audit. To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by the black-box models. We compare the mimic model trained with distillation to a second, un-distilled transparent model trained on ground truth outcomes, and use differences between the two models to gain insight into the black-box model. We demonstrate the approach on four data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. We also propose a statistical test to determine if a data set is missing key features used to train the black-box model. Our test finds that the ProPublica data is likely missing key feature(s) used in COMPAS.","['Sarah Tan', 'Rich Caruana', 'Giles Hooker', 'Yin Lou']","['Cornell University, Ithaca, NY, USA', 'Microsoft Research, Redmond, WA, USA', 'Cornell University, Ithaca, NY, USA', 'Ant Financial, San Mateo, CA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,TRUE
Diversity and Inclusion Metrics for Subset Selection,"The concept of fairness has recently been applied in machine learning settings to describe a wide range of constraints and objectives. When applied to ranking, recommendation, or subset selection problems for an individual, it becomes less clear that fairness goals are more applicable than goals that prioritize diverse outputs and instances that represent the individual's goals well.  In this work, we discuss the relevance of the concept of fairness to the concepts of diversity and inclusion, and introduce metrics that quantify the diversity and inclusion of an instance or set.  Diversity and inclusion metrics can be used in tandem, including additional fairness constraints, or may be used separately, and we detail how the different metrics interact.  Results from human subject experiments demonstrate that the proposed criteria for diversity and inclusion are consistent with social notions of these two concepts, and human judgments on the diversity and inclusion of example instances are correlated with the defined metrics.","['Dylan Baker', 'Ben Hutchinson', 'Alex Hanna']",Google,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES) (2020)",2020,TRUE
Diversity and Inclusion Metrics in Subset Selection,"The ethical concept of fairness has recently been applied in machine learning (ML) settings to describe a wide range of constraints and objectives. When considering the relevance of ethical concepts to subset selection problems, the concepts of diversity and inclusion are additionally applicable in order to create outputs that account for social power and access differentials. We introduce metrics based on these concepts, which can be applied together, separately, and in tandem with additional fairness constraints. Results from human subject experiments lend support to the proposed criteria. Social choice methods can additionally be leveraged to aggregate and choose preferable sets, and we detail how these may be applied.","['Margaret Mitchell', 'Dylan Baker', 'Nyalleng Moorosi', 'Emily Denton', 'Ben Hutchinson', 'Alex Hanna', 'Timnit Gebru', 'Jamie Morgenstern']","['Google Research, Seattle, WA, USA', 'Google Research, Seattle, WA, USA', 'Google Research, Accra, Ghana', 'Google Research, New York, NJ, USA', 'Google Research, San Francisco, CA, USA', 'Google Research, Mountain View, CA, USA', 'Google Research, Mountain View, CA, USA', 'Google Research & University of Washington, Seattle, WA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
Diversity-Sensitive Conditional Generative Adversarial Networks,"We propose a simple yet highly effective method that addresses the mode-collapse
problem in the Conditional Generative Adversarial Network (cGAN). Although
conditional distributions are multi-modal (i.e., having many modes) in practice,
most cGAN approaches tend to learn an overly simplified distribution where an
input is always mapped to a single output regardless of variations in latent code.
To address such issue, we propose to explicitly regularize the generator to produce
diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives.
Additionally, explicit regularization on generator allows our method to control a
balance between visual quality and diversity. We demonstrate the effectiveness
of our method on three conditional generation tasks: image-to-image translation,
image inpainting, and future video prediction. We show that simple addition of
our regularization to existing models leads to surprisingly diverse generations,
substantially outperforming the previous approaches for multi-modal conditional
generation specifically designed in each individual task.",[],Google,ICLR (2019),2019,TRUE
Doctor XAI: an ontology-based approach to black-box sequential data classification explanations,"Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.","['Cecilia Panigutti', 'Alan Perotti', 'Dino Pedreschi']","['Scuola Normale Superiore', 'ISI foundation', 'University of Pisa']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Documenting Computer Vision Datasets: An Invitation to Reflexive Data Practices,"In industrial computer vision, discretionary decisions surrounding the production of image training data remain widely undocumented. Recent research taking issue with such opacity has proposed standardized processes for dataset documentation. In this paper, we expand this space of inquiry through fieldwork at two data processing companies and thirty interviews with data workers and computer vision practitioners. We identify four key issues that hinder the documentation of image datasets and the effective retrieval of production contexts. Finally, we propose reflexivity, understood as a collective consideration of social and intellectual factors that lead to praxis, as a necessary precondition for documentation. Reflexive documentation can help to expose the contexts, relations, routines, and power structures that shape data.","['Milagros Miceli', 'Tianling Yang', 'Laurens Naudts', 'Martin Schuessler', 'Diana Serbanescu', 'Alex Hanna']","['Technische Universität Berlin', 'Technische Universität Berlin', 'Centre for IT & IP Law (CiTiP), KU Leuven', 'Technische Universität Berlin', 'Technische Universität Berlin', 'Google Research']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Does AI Qualify for the Job?: A Bidirectional Model Mapping Labour and AI Intensities,"In this paper we present a setting for examining the relation be-tween the distribution of research intensity in AI research and the relevance for a range of work tasks (and occupations) in current and simulated scenarios. We perform a mapping between labourand AI using a set of cognitive abilities as an intermediate layer. This setting favours a two-way interpretation to analyse (1) what impact current or simulated AI research activity has or would have on labour-related tasks and occupations, and (2) what areas of AI research activity would be responsible for a desired or undesired effect on specific labour tasks and occupations. Concretely, in our analysis we map 59 generic labour-related tasks from several worker surveys and databases to 14 cognitive abilities from the cognitive science literature, and these to a comprehensive list of 328 AI benchmarks used to evaluate progress in AI techniques. We provide this model and its implementation as a tool for simulations. We also show the effectiveness of our setting with some illustrative examples.","['Fernando Martínez-Plumed', 'Songül Tolan', 'Annarosa Pesole', 'José Hernández-Orallo', 'Enrique Fernández-Macías', 'Emilia Gómez']","['Universitat Politècnica de València, Valencia, Spain', 'European Commission (JRC), Seville, Spain', 'European Commission (JRC), Seville, Italy', 'Universitat Politècnica de València, Valencia, Spain', 'European Commission (JRC), Seville, Spain', 'European Commission (JRC), Seville, Germany']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Downstream Effects of Affirmative Action,"We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions. We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. We show that both goals can be achieved when the college does not report grades. On the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy.","['Sampath Kannan', 'Aaron Roth', 'Juba Ziani']","['University of Pennsylvania', 'University of Pennsylvania', 'California Institute of Technology']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples,"Recent studies have highlighted the vulnerability of deep neural networks (DNNs) to adversarial examples - a visually indistinguishable adversarial image can easily be crafted to cause a well-trained model to misclassify. Existing methods for crafting adversarial examples are based on L2 and L∞ distortion metrics. However, despite the fact that L1 distortion accounts for the total variation and encourages sparsity in the perturbation, little has been developed for crafting L1-based adversarial examples. In this paper, we formulate the process of attacking DNNs via adversarial examples as an elastic-net regularized optimization problem. Our elastic-net attacks to DNNs (EAD) feature L1-oriented adversarial examples and include the state-of-the-art L2 attack as a special case. Experimental results on MNIST, CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial examples with small L1 distortion and attains similar attack performance to the state-of-the-art methods in different attack scenarios. More importantly, EAD leads to improved attack transferability and complements adversarial training for DNNs, suggesting novel insights on leveraging L1 distortion in adversarial machine learning and security implications of DNNs.","Pin-Yu Chen, Y. Sharma, H. Zhang,  J. Yi, Cho-Jui Hsieh",IBM,AAAI (2018),2018,TRUE
Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making,"Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.","['Yunfeng Zhang', 'Q. Vera Liao', 'Rachel K. E. Bellamy']","['IBM Research AI', 'IBM Research AI', 'IBM Research AI']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Efficient Neural Network Robustness Certification with General Activation Functions," Finding minimum distortion of adversarial examples and thus certifying robustness in neural networks classifiers is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for \textit{general} activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to the four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by \textit{adaptively} selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan. To the best of our knowledge, CROWN is the first framework that can efficiently certify non-trivial robustness for general activation functions in neural networks.",Huan Zhang Pin-Yu Chen,IBM,NeurIPS (2018),2018,TRUE
Efficient Search for Diverse Coherent Explanations,"This paper proposes new search algorithms for counterfactual explanations based upon mixed integer programming. We are concerned with complex data in which variables may take any value from a contiguous range or an additional set of discrete states. We propose a novel set of constraints that we refer to as a ""mixed polytope"" and show how this can be used with an integer programming solver to efficiently find coherent counterfactual explanations i.e. solutions that are guaranteed to map back onto the underlying data structure, while avoiding the need for brute-force enumeration. We also look at the problem of diverse explanations and show how these can be generated within our framework.",['Chris Russell'],['The University of Surrey and The Alan Turing Institute'],"FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
"Embodiment, Anthropomorphism, and Intellectual Property Rights for AI Creations","Computational creativity is an emerging branch of artificial intelligence (AI) concerned with algorithms that can create novel and high-quality ideas or artifacts, either autonomously or semi-autonomously in collaboration with people. Quite simply, such algorithms may be described as artificial innovation engines. These technologies raise questions of authorship/inventorship and of agency, which become further muddled by the social context induced by AI that may be physically-embodied or anthropomorphized. These questions are fundamentally intertwined with the provision of appropriate incentives for conducting and commercializing computational creativity research through intellectual property regimes. This paper reviews current understanding of intellectual property rights for AI, and explores possible framings for intellectual property policy in social context.","['Deepak Somaya', 'Lav R. Varshney']","['University of Illinois at Urbana-Champaign, Urbana, IL, USA', 'University of Illinois at Urbana-Champaign, Urbana, IL, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Ensuring Fairness in Machine Learning to Advance Health Equity,"A central promise of machine learning (ML) is to use historical data to project the future trajectories of patients.  Will they have a good or bad outcome? What diagnoses will they have? What treatments should they be given?  But in many cases, we do not want the future to look like the past, especially when the past contains patterns of human or structural biases against vulnerable populations.","['Alvin Rishi Rajkomar', 'Greg Corrado', 'Michael Howell']",Google,Annals of Internal Medicine (2018),2018,TRUE
Epistemic Therapy for Bias in Automated Decision-Making,"Despite recent interest in both the critical and machine learning literature on ""bias"" in artificial intelligence (AI) systems, the nature of specific biases stemming from the interaction of machines, humans, and data remains ambiguous. Influenced by Gendler's work on human cognitive biases, we introduce the concept of alief-discordant belief, the tension between the intuitive moral dispositions of designers and the explicit representations generated by algorithms. Our discussion of alief-discordant belief diagnoses the ethical concerns that arise when designing AI systems atop human biases. We furthermore codify the relationship between data, algorithms, and engineers as components of this cognitive discordance, comprising a novel epistemic framework for ethics in AI.","['Thomas Krendl Gilbert', 'Yonatan Mintz']","['University of California, Berkeley, Berkeley, CA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Epistemic values in feature importance methods: Lessons from feminist epistemology,"As the public seeks greater accountability and transparency from machine learning algorithms, the research literature on methods to explain algorithms and their outputs has rapidly expanded. Feature importance methods form a popular class of explanation methods. In this paper, we apply the lens of feminist epistemology to recent feature importance research. We investigate what epistemic values are implicitly embedded in feature importance methods and how or whether they are in conflict with feminist epistemology. We offer some suggestions on how to conduct research on explanations that respects feminist epistemic values, taking into account the importance of social context, the epistemic privileges of subjugated knowers, and adopting more interactional ways of knowing","['Leif Hancox-Li', 'I. Elizabeth Kumar']","['Capital One, New York, New York, USA', 'University of Utah, Salt Lake City, UT, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Equality of Voice: Towards Fair Representation in Crowdsourced Top-K Recommendations,"To help their users to discover important items at a particular time, major websites like Twitter, Yelp, TripAdvisor or NYTimes provide Top-K recommendations (e.g., 10 Trending Topics, Top 5 Hotels in Paris or 10 Most Viewed News Stories), which rely on crowdsourced popularity signals to select the items. However, different sections of a crowd may have different preferences, and there is a large silent majority who do not explicitly express their opinion. Also, the crowd often consists of actors like bots, spammers, or people running orchestrated campaigns. Recommendation algorithms today largely do not consider such nuances, hence are vulnerable to strategic manipulation by small but hyper-active user groups. To fairly aggregate the preferences of all users while recommending top-K items, we borrow ideas from prior research on social choice theory, and identify a voting mechanism called Single Transferable Vote (STV) as having many of the fairness properties we desire in top-K item (s)elections. We develop an innovative mechanism to attribute preferences of silent majority which also make STV completely operational. We show the generalizability of our approach by implementing it on two different real-world datasets. Through extensive experimentation and comparison with state-of-the-art techniques, we show that our proposed approach provides maximum user satisfaction, and cuts down drastically on items disliked by most but hyper-actively promoted by a few users.","['Abhijnan Chakraborty', 'Gourab K. Patro', 'Niloy Ganguly', 'Krishna P. Gummadi', 'Patrick Loiseau']","['MPI for Software Systems, Germany IIT Kharagpur, India', 'IIT Kharagpur, India', 'IIT Kharagpur, India', 'MPI for Software Systems, Germany', 'Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG & MPI SWS']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Equalized Odds Implies Partially Equalized Outcomes Under Realistic Assumptions,"Equalized odds -- where the true positive rates and false positive rates are equal across groups (e.g. racial groups) -- is a common quantitative measure of fairness. Equalized outcomes -- where the difference in predicted outcomes between groups is less than the difference observed in the training data -- is more contentious, because it is incompatible with perfectly accurate predictions. We formalize and quantify the relationship between these two important but seemingly distinct notions of fairness. We show that under realistic assumptions, equalized odds implies partially equalized outcomes. We prove a comparable result for approximately equalized odds. In addition, we generalize a well-known previous result about the incompatibility of equalized odds and another definition of fairness known as calibration, by showing that partially equalized outcomes implies non-calibration. Our results highlight the risks of using trends observed across groups to make predictions about individuals.",['Daniel McNamara'],"['Australian National University & CSIRO Data61, Canberra, Australia']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Ethical Challenges in Data-Driven Dialogue Systems,"The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.","['Peter Henderson', 'Koustuv Sinha', 'Nicolas Angelard-Gontier', 'Nan Rosemary Ke', 'Genevieve Fried', 'Ryan Lowe', 'Joelle Pineau']","['McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'Polytechnique Montréal, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Ethically Aligned Opportunistic Scheduling for Productive Laziness,"In artificial intelligence (AI) mediated workforce management systems (e.g., crowdsourcing), long-term success depends on workers accomplishing tasks productively and resting well. This dual objective can be summarized by the concept of productive laziness. Existing scheduling approaches mostly focus on efficiency but overlook worker wellbeing through proper rest. In order to enable workforce management systems to follow the IEEE Ethically Aligned Design guidelines to prioritize worker wellbeing, we propose a distributed Computational Productive Laziness (CPL) approach in this paper. It intelligently recommends personalized work-rest schedules based on local data concerning a worker's capabilities and situational factors to incorporate opportunistic resting and achieve superlinear collective productivity without the need for explicit coordination messages. Extensive experiments based on a real-world dataset of over 5,000 workers demonstrate that CPL enables workers to spend 70% of the effort to complete 90% of the tasks on average, providing more ethically aligned scheduling than existing approaches.","['Han Yu', 'Chunyan Miao', 'Yongqing Zheng', 'Lizhen Cui', 'Simon Fauvel', 'Cyril Leung']","['Nanyang Technological University, Singapore, Singapore', 'Nanyang Technological University, Singapore, Singapore', 'Shandong University, Jinan, China', 'Shandong University, Jinan, China', 'Nanyang Technological University, Singapore, Singapore', 'The University of British Columbia, Vancouver, Canada']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Ethics by Design: Necessity or Curse?,"Ethics by Design concerns the methods, algorithms and tools needed to endow autonomous agents with the capability to reason about the ethical aspects of their decisions, and the methods, tools and formalisms to guarantee that an agent's behavior remains within given moral bounds. In this context some questions arise: How and to what extent can agents understand the social reality in which they operate, and the other intelligences (AI, animals and humans) with which they co-exist? What are the ethical concerns in the emerging new forms of society, and how do we ensure the human dimension is upheld in interactions and decisions by autonomous agents?. But overall, the central question is: ""Can we, and should we, build ethically-aware agents?"" This paper presents initial conclusions from the thematic day of the same name held at PRIMA2017, on October 2017.","['Virginia Dignum', 'Matteo Baldoni', 'Cristina Baroglio', 'Maurizio Caon', 'Raja Chatila', 'Louise Dennis', 'Gonzalo Génova', 'Galit Haim', 'Malte S. Kließ', 'Maite Lopez-Sanchez', 'Roberto Micalizio', 'Juan Pavón', 'Marija Slavkovik', 'Matthijs Smakman', 'Marlies van Steenbergen', 'Stefano Tedeschi', 'Leon van der Toree', 'Serena Villata', 'Tristan de Wildt']","['Umeå University, Umeå, Sweden', 'Università degli Studi di Torino, Torino, Italy', 'Università degli Studi di Torino, Torino, Italy', 'University of Applied Sciences and Arts Western Switzerland, Fribourg, Switzerland', 'Sorbonne Université, Paris, France', 'University of Liverpool, Liverpool, United Kingdom', 'Universidad Carlos III de Madrid, Leganés, Spain', 'The College of Management Academic Studies, Rishon Lezion, Israel', 'Deft Technical University, Delft, Netherlands', 'Universitat de Barcelona, Barcelona, Spain', 'Università degli Studi di Torino, Torino, Italy', 'Universidad Complutense de Madrid, Madrid, Spain', 'University of Bergen, Bergen, Norway', 'HU University of Applied Sciences Utrecht, Utrecht, Netherlands', 'HU University of Applied Sciences, Utrecht, Utrecht, Netherlands', 'Università degli Studi di Torino, Torino, Italy', 'University of Luxembourg, Luxembourg, Luxembourg', ""Université Côte d'Azur, CNRS, Inria, I3S, Sophia Antipolis Cedex, France"", 'Delft University of Technology, Delft, Netherlands']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Ethics for AI Writing: The Importance of Rhetorical Context,"Implicit in any rhetorical interaction-between humans or between humans and machines-are ethical codes that shape the rhetorical context, the social situation in which communication happens and also the engine that drives communicative interaction. Such implicit codes are usually invisible to AI writing systems because the social factors shaping communication (the why and how of language, not the what) are not usually explicitly evident in databases the systems use to produce discourse. Can AI writing systems learn to learn rhetorical context, particularly the implicit codes for communication ethics? We see evidence that some systems do address issues of rhetorical context, at least in rudimentary ways. But we critique the information transfer communication model supporting many AI writing systems, arguing for a social context model that accounts for rhetorical context-what is, in a sense, ""not there"" in the data corpus but that is critical for the production of meaningful, significant, and ethical communication. We offer two ethical principles to guide design of AI writing systems: transparency about machine presence and critical data awareness, a methodological reflexivity about rhetorical context and omissions in the data that need to be provided by a human agent or accounted for in machine learning.","['Heidi A. McKee', 'James E. Porter']","['Miami University, Oxford, OH, USA', 'Miami University, Oxford, OH, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Ethics in Norm Decision Making,"Norms are an instrument to coordinate societies, but deciding which norms to enact is a difficult task. Not only norms might have incom- patibilities between themselves, such as norms contradicting other norms, but also the cost of implementation can be an important aspect to consider. Furthermore, due to the growing social inter- est in ethics and the ethical impact norms can have, this ethical implications should also be examined during the decision making process.",['Marc Serramia'],"['Universitat de Barcelona, Barcelona, Spain']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Ethics of Food Recommender Applications,"The recent unprecedented popularity of food recommender applications has raised several issues related to the ethical, societal and legal implications of relying on these applications. In this paper, in order to assess the relevant ethical issues, we rely on the emerging principles across the AI & Ethics community and define them tailored context specifically. Considering the popular Food Recommender Systems (henceforth F-RS) in the European market cannot be regarded as personalised F-RS, we show how merely this lack of feature shifts the relevance of the focal ethical concerns. We identify the major challenges and propose a scheme for how explicit ethical agendas should be explained. We also argue how a multi-stakeholder approach is indispensable to ensure producing long-term benefits for all stakeholders. After proposing eight ethical desiderata points for F-RS, we present a case-study and assess it based on our proposed desiderata points.","['Daniel Karpati', 'Amro Najjar', 'Diego Agustin Ambrossio']","['University of Luxembourg, Esch-sur-Alzette, Luxembourg', 'University of Luxembourg, Esch-Sur-Alzette, Luxembourg', 'University of Luxembourg, Esch-Sur-Alzette, Luxembourg']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Evading the Curse of Dimensionality in Unconstrained Private Generalized Linear Problems,"Differentially private gradient descent (DP-GD) has been extremely effective both theoretically, and in practice, for solving private empirical risk minimization (ERM) problems. In this paper, we focus on understanding the impact of the clipping norm, a critical component of DP-GD, on its convergence. We provide the first formal convergence analysis of clipped DP-GD.","['Shuang Song', 'Om Thakkar']",Google,24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021) (2020),2020,TRUE
Evaluating Fairness of Machine Learning Models Under Uncertain and Incomplete Information,"Training and evaluation of fair classifiers is a challenging problem. This is partly due to the fact that most fairness metrics of interest depend on both the sensitive attribute information and label information of the data points. In many scenarios it is not possible to collect large datasets with such information. An alternate approach that is commonly used is to separately train an attribute classifier on data with sensitive attribute information, and then use it later in the ML pipeline to evaluate the bias of a given classifier. While such decoupling helps alleviate the problem of demographic scarcity, it raises several natural questions such as: how should the attribute classifier be trained?, and how should one use a given attribute classifier for accurate bias estimation? In this work we study this question from both theoretical and empirical perspectives. We first experimentally demonstrate that the test accuracy of the attribute classifier is not always correlated with its effectiveness in bias estimation for a downstream model. In order to further investigate this phenomenon, we analyze an idealized theoretical model and characterize the structure of the optimal classifier. Our analysis has surprising and counter-intuitive implications where in certain regimes one might want to distribute the error of the attribute classifier as unevenly as possible among the different subgroups. Based on our analysis we develop heuristics for both training and using attribute classifiers for bias estimation in the data scarce regime. We empirically demonstrate the effectiveness of our approach on real and simulated data.","['Pranjal Awasthi', 'Alex Beutel', 'Matthäus Kleindessner', 'Jamie Morgenstern', 'Xuezhi Wang']","['Rutgers University & Google', 'Google', 'Amazon', 'University of Washington & Google', 'Google']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Evaluating Stochastic Rankings with Expected Exposure,"We introduce the concept of expected exposure as the average attention ranked items receive from users over repeated samples of the same query. Furthermore, we advocate for the adoption of the principle of equal expected exposure: given a fixed information need, no item receive more or less expected exposure compared to any other item of the same relevance grade. We argue that this principle is desirable for many retrieval objectives and scenarios, including topical diversity and fair ranking. Leveraging user models from existing retrieval metrics, we propose a general evaluation methodology based on expected exposure and draw connections to related metrics in information retrieval evaluation. Importantly, this methodology relaxes classic information retrieval assumptions, allowing a system, in response to a query, to produce a distribution over rankings instead of a single fixed ranking. We study the behavior of the expected exposure metric and stochastic rankers across a variety of information access conditions, including ad hoc retrieval and recommendation. We believe that measuring and optimizing expected exposure metrics using randomization opens a new area for retrieval algorithm development and progress.","['Fernando Diaz', 'Bhaskar Mitra', 'Michael D. Ekstrand', 'Asia J. Biega', 'Ben Carterette']",Microsoft,Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM),2020-07-01,TRUE
Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach,"The robustness of neural networks to adversarial examples has received great attention due to security implications. Despite various attack approaches to crafting visually imperceptible adversarial examples, little has been developed towards a comprehensive measure of robustness. In this paper, we provide a theoretical justification for converting robustness analysis into a local Lipschitz constant estimation problem, and propose to use the Extreme Value Theory for efficient evaluation. Our analysis yields a novel robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness. The proposed CLEVER score is attack-agnostic and computationally feasible for large neural networks. Experimental results on various networks, including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned with the robustness indication measured by the ℓ2 and ℓ∞ norms of adversarial examples from powerful attacks, and (ii) defended networks using defensive distillation or bounded ReLU indeed achieve better CLEVER scores. To the best of our knowledge, CLEVER is the first attack-independent robustness metric that can be applied to any neural network classifier.","Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, Luca Daniel",IBM,ICLR (2018),2018,TRUE
Evolutionary Search for Adversarially Robust Neural Networks,"We explore the use of evolutionary neural network architecture search in order to arrive at architectures which achieve high accuracy both on benign and adversarial samples when trained adversarially. On MNIST, our best model obtains 99.5% accuracy on benign samples and 96.19% under an untargeted attack with maximum `∞ perturbation 0.3, substantially improving over the state-of-the-art. On CIFAR-10, our most robust model achieves 49.08% accuracy under an untargeted attack with maximum `∞ perturbation 8/255, and 82.88% on benign samples. We were able to synthesize a model obtaining 93.2% accuracy on benign samples and 49.0% under a PGD(10) attack, however, it appears to have overfitted to the attack configuration used during the adversarial training. Compared to state-of-the-art robust architectures, our models have about 50% less trainable parameters.","Mathieu Sinn, Martin Wistuba, Beat Buesser, Maria-Irina Nicolae, Minh Tran",IBM,ICLR (2019),2019,TRUE
Expectation-Aware Planning: A Unifying Framework for Synthesizing and Executing Self-Explaining Plans for Human-Aware Planning,"In this work, we present a new planning formalism called expectation-aware planning for decision making with humans in the loop where the human's expectations about an agent may differ from the agent's own model. We show how this formulation allows agents to not only leverage existing strategies for handling model differences but can also exhibit novel behaviors that are generated through the combination of these different strategies. Our formulation also reveals a deep connection to existing approaches in epistemic planning. Specifically, we show how we can leverage classical planning compilations for epistemic planning to solve expectation-aware planning problems. To the best of our knowledge, the proposed formulation is the first complete solution to decision-making in the presence of diverging user expectations that is amenable to a classical planning compilation while successfully combining previous works on explanation and explicability. We empirically show how our approach provides a computational advantage over existing approximate approaches that unnecessarily try to search in the space of models while also failing to facilitate the full gamut of behaviors enabled by our framework.","Sarath Sreedharan, Tathagata Chakraborti, Christian Muise, Subbarao Kambhampati",IBM,AAAI (2020),2020,TRUE
Explainability fact sheets: a framework for systematic assessment of explainable approaches,"Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.","['Kacper Sokol', 'Peter Flach']","['University of Bristol, Bristol, United Kingdom', 'University of Bristol, Bristol, United Kingdom']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Explainable machine learning in deployment,"Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.","['Umang Bhatt', 'Alice Xiang', 'Shubham Sharma', 'Adrian Weller', 'Ankur Taly', 'Yunhan Jia', 'Joydeep Ghosh', 'Ruchir Puri', 'José M. F. Moura', 'Peter Eckersley']","['Carnegie Mellon University and Partnership on AI and University of Cambridge and Leverhulme CFI', 'Partnership on AI', 'University of Texas at Austin', 'University of Cambridge and Leverhulme CFI and The Alan Turing Institute', 'Fiddler Labs', 'Baidu', 'University of Texas at Austin and CognitiveScale', 'IBM Research', 'Carnegie Mellon University', 'Partnership on AI']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Explaining Deep Neural Networks using Unsupervised Clustering,"We propose a novel method to explain trained deep neural networks (DNNs), by distilling them into surrogate models using unsupervised clustering. Our method can be flexibly applied to any subset of layers of a DNN architecture and can incorporate low-level and high-level information. On image datasets given pre-trained DNNs, we demonstrate strength of our method in finding similar training samples, and shedding light on the concepts the DNN bases its decision on. Via user studies, we show that our model can improve user trust in modelâs prediction.",['Sercan Arik'],Google,2020 Workshop on Human Interpretability in Machine Learning (2020),2020,TRUE
Explaining Explanations in AI,"Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that ""All models are wrong but some are useful."" We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a ""do it yourself kit"" for explanations, allowing a practitioner to directly answer ""what if questions"" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.","['Brent Mittelstadt', 'Chris Russell', 'Sandra Wachter']","['University of Oxford, The Alan Turing Institute', 'University of Surrey, The Alan Turing Institute', 'University of Oxford, The Alan Turing Institute']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Explaining machine learning classifiers through diverse counterfactual explanations,"Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.","['Ramaravind K. Mothilal', 'Amit Sharma', 'Chenhao Tan']","['Microsoft Research India', 'Microsoft Research India', 'University of Colorado Boulder']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
"Explanatory Dialogs: Towards Actionable, Interactive Explanations","Adoption of AI systems in high-stakes domains (e.g., transportation, law, and healthcare) demands that human users trust these systems. A desiderata for establishing trust is that the users understand the system's decision process. However, a high-performing system may use a complex decision process, which may not be interpretable by itself. We argue that existing solutions for generating interpretable explanations have limitations and as a solution, propose developing new explanation systems that enable interactive and actionable dialogs between the user and the system.",['Gagan Bansal'],"['University of Washington, Seattle, WA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Explanatory Interactive Machine Learning,"Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.","['Stefano Teso', 'Kristian Kersting']","['KU Leuven, Leuven, Belgium', 'TU Darmstadt, Darmstadt, Germany']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Exploiting Moral Values to Choose the Right Norms,"Norms constitute regulative mechanisms extensively enacted in groups, organisations, and societies. However, 'choosing the right norms to establish' constitutes an open problem that requires the consideration of a number of constraints (such as norm relations) and preference criteria (e.g over involved moral values). This paper advances the state of the art in the Normative Multiagent Systems literature by formally defining this problem and by proposing its encoding as a linear program so that it can be automatically solved.","['Marc Serramia', 'Maite Lopez-Sanchez', 'Juan A. Rodriguez-Aguilar', 'Javier Morales', 'Michael Wooldridge', 'Carlos Ansotegui']","['Universitat de Barcelona, Barcelona, Spain', 'Universitat de Barcelona, Barcelona, Spain', 'Artificial Intelligence Research Institute (IIIA-CSIC), Cerdanyola del Valles, Spain', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom', 'Universitat de Lleida, Lleida, Spain']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Exploring AI Futures Through Role Play,"We present an innovative methodology for studying and teaching the impacts of AI through a role-play game. The game serves two primary purposes: 1) training AI developers and AI policy professionals to reflect on and prepare for future social and ethical challenges related to AI and 2) exploring possible futures involving AI technology development, deployment, social impacts, and governance. While the game currently focuses on the inter-relations between short-, mid- and long-term impacts of AI, it has potential to be adapted for a broad range of scenarios, exploring in greater depths issues of AI policy research and affording training within organizations. The game presented here has undergone two years of development and has been tested through over 30 events involving between 3 and 70 participants. The game is under active development, but preliminary findings suggest that role-play is a promising methodology for both exploring AI futures and training individuals and organizations in thinking about, and reflecting on, the impacts of AI and strategic mistakes that can be avoided today.","['Shahar Avin', 'Ross Gruetzemacher', 'James Fox']","['University of Cambridge, Cambridge, United Kingdom', 'Auburn University, Auburn, AL, USA', 'University of Oxford, Oxford, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Exploring the Hyperparameter Landscape of Adversarial Robustness,"Adversarial training shows promise as an approach for training models that are robust towards adversarial perturbation. In this paper, we explore some of the practical challenges of adversarial training. We present a sensitivity analysis that illustrates that the effectiveness of adversarial training hinges on the settings of a few salient hyperparameters. We show that the robustness surface that emerges across these salient parameters can be surprisingly complex and that therefore no effective one-size-fits-all parameter settings exist. We then demonstrate that we can use the same salient hyperparameters as tuning knob to navigate the tension that can arise between robustness and accuracy. Based on these findings, we present a practical approach that leverages hyperparameter optimization techniques for tuning adversarial training to maximize robustness while keeping the loss in accuracy within a defined budget.","Evelyn Duesterwald, Anupama Murthi, Ganesh Venkataraman, Mathieu Sinn, Deepak Vijaykeerthy",IBM,ICLR (2019),2019,TRUE
FACE: Feasible and Actionable Counterfactual Explanations,"Work in Counterfactual Explanations tends to focus on the principle of ""the closest possible world"" that identifies small changes leading to the desired outcome. In this paper we argue that while this approach might initially seem intuitively appealing it exhibits shortcomings not addressed in the current literature. First, a counterfactual example generated by the state-of-the-art systems is not necessarily representative of the underlying data distribution, and may therefore prescribe unachievable goals (e.g., an unsuccessful life insurance applicant with severe disability may be advised to do more sports). Secondly, the counterfactuals may not be based on a ""feasible path"" between the current state of the subject and the suggested one, making actionable recourse infeasible (e.g., low-skilled unsuccessful mortgage applicants may be told to double their salary, which may be hard without first increasing their skill level). These two shortcomings may render counterfactual explanations impractical and sometimes outright offensive. To address these two major flaws, first of all, we propose a new line of Counterfactual Explanations research aimed at providing actionable and feasible paths to transform a selected instance into one that meets a certain goal. Secondly, we propose FACE: an algorithmically sound way of uncovering these ""feasible paths"" based on the shortest path distances defined via density-weighted metrics. Our approach generates counterfactuals that are coherent with the underlying data distribution and supported by the ""feasible paths"" of change, which are achievable and can be tailored to the problem at hand.","['Rafael Poyiadzi', 'Kacper Sokol', 'Raul Santos-Rodriguez', 'Tijl De Bie', 'Peter Flach']","['University of Bristol, Bristol, United Kingdom', 'University of Bristol, Bristol, United Kingdom', 'University of Bristol, Bristol, United Kingdom', 'Ghent University, Ghent, Belgium', 'University of Bristol, Bristol, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
FACT: A Diagnostic for Group Fairness Trade-offs,"Group fairness, a class of fairness notions that measure how different groups of individuals are treated differently according to their protected attributes, has been shown to conflict with one another, often with a necessary cost in loss of model's predictive performance. We propose a general diagnostic that enables systematic characterization of these trade-offs in group fairness. We observe that the majority of group fairness notions can be expressed via the fairness-confusion tensor, which is the confusion matrix split according to the protected attribute values. We frame several optimization problems that directly optimize both accuracy and fairness objectives over the elements of this tensor, which yield a general perspective for understanding multiple trade-offs including group fairness incompatibilities. It also suggests an alternate post-processing method for designing fair classifiers. On synthetic and real datasets, we demonstrate the use cases of our diagnostic, particularly on understanding the trade-off landscape between accuracy and fairness. ","Joon Kim, Jiahao Chen, Ameet Talwalkar","[""Carnegie Mellon"", ""JP Morgan AI Research"", ""Determined AI""]",ICML 2020,2020-07,TRUE
"FACTS-IR: Fairness, Accountability, Confidentiality, Transparency, and Safety in Information Retrieval","The purpose of the SIGIR 2019 workshop on Fairness, Accountability, Confidentiality, Transparency, and Safety (FACTS-IR) was to explore challenges in responsible information retrieval system development and deployment. To this end, the workshop aimed to crowdsource from the larger SIGIR community and draft an actionable research agenda on five key dimensions of responsible information retrieval: fairness, accountability, confidentiality, transparency, and safety. Such an agenda can guide others in the community that are interested in pursuing FACTS-IR research, as well as inform potential funders about relevant research avenues. The workshop brought together a diverse set of researchers and practitioners interested in contributing to the development of a technical research agenda for responsible information retrieval.","['Alexandra Olteanu', 'Jean Garcia-Gathright', 'Maarten de Rijke', 'Michael D. Ekstrand']",Microsoft,ACM SIGIR Forum,2019-12-02,TRUE
Failure Modes of Variational Inference for Decision Making,"In this paper we highlight the risks of relying on
mean-field variational inference to learn models
that are used as simulators for decision making.
We study the role of accurate inference for latent
variable models in terms of cumulative reward
performance. We show how naive mean-field
variational inference at test time can lead to poor
decisions in basic but fundamental quadratic control problems with continuous actions, as relevant
correlations in the latent space are ignored. We
then extend these examples to a more complex
non-linear scenario with asymmetric costs, where
regret is even more significant.","['Carlos Riquelme', 'Matt Hoffman']",Google,ICML Workshop (2018),2018,TRUE
Fair Algorithms for Learning in Allocation Problems,"Settings such as lending and policing can be modeled by a centralized agent allocating a scarce resource (e.g. loans or police officers) amongst several groups, in order to maximize some objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such problems fairness is also a concern. One natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource in question, the probability of actually receiving it is approximately independent of the individual's group. For example, in lending this would mean that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it would mean that two individuals committing the same crime in different districts would have roughly equal chances of being arrested. In this paper, we formalize this general notion of fairness for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the allocator does not know the frequency of candidates (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates in each group. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to and do not learn about crimes committed if the police presence in a district is low. As an application of our framework and algorithm, we consider the predictive policing problem, in which the resource being allocated to each group is the number of police officers assigned to each district. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. In this case, the fairness constraint asks that the probability that an individual who has committed a crime is arrested should be independent of the district in which they live. We investigate the performance of our learning algorithm on the Philadelphia Crime Incidents dataset.","['Hadi Elzayn', 'Shahin Jabbari', 'Christopher Jung', 'Michael Kearns', 'Seth Neel', 'Aaron Roth', 'Zachary Schutzman']","['University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Fair Allocation through Competitive Equilibrium from Generic Incomes,"Two food banks catering to populations of different sizes with different needs must divide among themselves a donation of food items. What constitutes a ""fair"" allocation of the items among them? Competitive equilibrium from equal incomes (CEEI) is a classic solution to the problem of fair and efficient allocation of goods among equally entitled agents [Foley 1967, Varian 1974]. Every agent (foodbank) receives an equal endowment of artificial currency with which to ""purchase"" bundles of goods (food items). Prices for the goods are set high enough such that the agents can simultaneously get their favorite within-budget bundle, and low enough such that all goods are allocated (no waste). A CEEI satisfies mathematical notions of fairness like fair-share, and also has built-in transparency -- prices can be published so the agents can verify they're being treated equally. However, a CEEI is not guaranteed to exist when the items are indivisible. We study competitive equilibrium from generic incomes (CEGI), which is based on the idea of slightly perturbed endowments, and enjoys similar fairness, efficiency and transparency properties as CEEI. We show that when the two agents have almost equal endowments and additive preferences for the items, a CEGI always exists. We then consider agents who are a priori non-equal (like different-sized foodbanks); we formulate a new notion of fair allocation among non-equals satisfied by CEGI, and show existence in cases of interest (like when the agents have identical preferences). Experiments on simulated and Spliddit data (a popular fair division website) indicate more general existence. Our results open opportunities for future research on fairness through generic endowments, and on fair treatment of non-equals.","['Moshe Babaioff', 'Noam Nisan', 'Inbal Talgam-Cohen']","['Microsoft Research', 'Hebrew University of Jerusalem', 'Technion']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Fair Allocation through Selective Information Acquisition,"Public and private institutions must often allocate scarce resources under uncertainty. Banks, for example, extend credit to loan applicants based in part on their estimated likelihood of repaying a loan. But when the quality of information differs across candidates (e.g., if some applicants lack traditional credit histories), common lending strategies can lead to disparities across groups. Here we consider a setting in which decision makers---before allocating resources---can choose to spend some of their limited budget further screening select individuals. We present a computationally efficient algorithm for deciding whom to screen that maximizes a standard measure of social welfare. Intuitively, decision makers should screen candidates on the margin, for whom the additional information could plausibly alter the allocation. We formalize this idea by showing the problem can be reduced to solving a series of linear programs. Both on synthetic and real-world datasets, this strategy improves utility, illustrating the value of targeted information acquisition in such decisions. Further, when there is social value for distributing resources to groups for whom we have a priori poor information---like those without credit scores---our approach can substantially improve the allocation of limited assets.","['William Cai', 'Johann Gaebler', 'Nikhil Garg', 'Sharad Goel']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Fair classification and social welfare,"Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Δ'e perturbations to a fairness parameter 'e in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of ""fairness-to-welfare"" solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring ""more fair"" classifiers does not abide by the Pareto Principle---a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.","['Lily Hu', 'Yiling Chen']","['Harvard University', 'Harvard University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Fair Classification with Group-Dependent Label Noise,"This work examines how to train fair classifiers in settings where training labels are corrupted with random noise, and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup. Heterogeneous label noise models systematic biases towards particular groups when generating annotations. We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures, without accounting for heterogeneous and group-dependent error rates, can decrease both the accuracy and the fairness of the resulting classifier. Our experiments demonstrate these issues arise in practice as well. We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise. We provide both theoretical and empirical justifications for the efficacy of our methods. We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good.","['Jialu Wang', 'Yang Liu', 'Caleb Levy']","['UC Santa Cruz, Santa Cruz, CA, USA', 'UC Santa Cruz, Santa Cruz, CA, USA', 'UC Santa Cruz Santa Cruz, CA, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Fair Clustering via Equitable Group Representations,"What does it mean for a clustering to be fair? One popular approach seeks to ensure that each cluster contains groups in (roughly) the same proportion in which they exist in the population. The normative principle at play is balance: any cluster might act as a representative of the data, and thus should reflect its diversity. But clustering also captures a different form of representativeness. A core principle in most clustering problems is that a cluster center should be representative of the cluster it represents, by being ""close"" to the points associated with it. This is so that we can effectively replace the points by their cluster centers without significant loss in fidelity, and indeed is a common ""use case"" for clustering. For such a clustering to be fair, the centers should ""represent"" different groups equally well. We call such a clustering a group-representative clustering. In this paper, we study the structure and computation of group-representative clusterings. We show that this notion naturally parallels the development of fairness notions in classification, with direct analogs of ideas like demographic parity and equal opportunity. We demonstrate how these notions are distinct from and cannot be captured by balance-based notions of fairness. We present approximation algorithms for group representative k-median clustering and couple this with an empirical evaluation on various real-world data sets. We also extend this idea to facility location, motivated by the current problem of assigning polling locations for voting","['Mohsen Abbasi', 'Aditya Bhaskara', 'Suresh Venkatasubramanian']","['University of Utah', 'University of Utah', 'University of Utah']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Fair decision making using privacy-protected data,"Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a trade-off between protecting privacy and the accuracy of decisions, we initiate a first-of-its-kind study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making. We empirically investigate novel tradeoffs on two real-world decisions made using U.S. Census data (allocation of federal funds and assignment of voting rights benefits) as well as a classic apportionment problem. Our results show that if decisions are made using an ∈-differentially private version of the data, under strict privacy constraints (smaller ∈), the noise added to achieve privacy may disproportionately impact some groups over others. We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities. We also explore improved algorithms to remedy the unfairness observed.","['David Pujol', 'Ryan McKenna', 'Satya Kuppam', 'Michael Hay', 'Ashwin Machanavajjhala', 'Gerome Miklau']","['Duke University', 'University of Massachusetts, Amherst', 'University of Massachusetts, Amherst', 'Colgate University', 'Duke University', 'University of Massachusetts, Amherst']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Fair Forests: Regularized Tree Induction to Minimize Model Bias,"The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly, there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable, non-linear, and easy-to-use. In this paper we develop, to our knowledge, the first technique for the induction of fair decision trees.We show that our ""Fair Forest"" retains the benefits of the tree-based approach, while providing both greater accuracy and fairness than other alternatives, for both ""group fairness'' and ""individual fairness.'' We also introduce new measures for fairness which are able to handle multinomial and continues attributes as well as regression problems, as opposed to binary attributes and labels only. Finally, we demonstrate a new, more robust evaluation procedure for algorithms that considers the dataset in its entirety rather than only a specific protected attribute.","['Edward Raff', 'Jared Sylvester', 'Steven Mills']","['Booz Allen Hamilton, Columbia, MD, USA', 'Booz Allen Hamilton, Columbia, MD, USA', 'Booz Allen Hamilton, Columbia, MD, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,TRUE
Fair k-Centers via Maximum Matching,"The field of algorithms has seen a push for fairness, or the removal of inherent bias, in recent history. In data summarization, where a much smaller subset of a data set is chosen to represent the whole of the data, fairness can be introduced by guaranteeing each ""demographic group"" a specific portion of the representative subset. Specifically, this paper examines this fair variant of the k-centers problem, where a subset of the data with cardinality k is chosen to minimize distance to the rest of the data. Previous papers working on this problem presented both a 3-approximation algorithm with a super-linear runtime and a linear-time algorithm whose approximation factor is exponential in the number of demographic groups. This paper combines the best parts of each algorithm , by presenting a linear-time algorithm with a guaranteed 3-approximation factor, and provides empirical evidence of both the algorithm's runtime and effectiveness.","Matthew Jones, Thy Nguyen, Huy Nguyen ",Northeastern University,ICML 2020,2020-07,FALSE
Fair Learning with Private Demographic Data,"Sensitive attributes such as race are rarely available to learners in real world settings as their collection is often restricted by laws and regulations. We give a scheme that allows individuals to release their sensitive information privately while still allowing any downstream entity to learn non-discriminatory predictors. We show how to adapt non-discriminatory learners to work with privatized protected attributes giving theoretical guarantees on performance. Finally, we highlight how the methodology could apply to learning fair predictors in settings where protected attributes are only available for a subset of the data.","Hussein Mozannar, Mesrob I. Ohannessian, Nathan Srebro","[""MIT"", ""UIC"", ""Toyota Technological Institute at Chicago""]",ICML 2020,2020-07,FALSE
Fair Regression: Quantitative Definitions and Reduction-Based Algorithms,"In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems fair regression. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness--accuracy frontiers on several standard datasets. ","Alekh Agarwal, Miroslav Dudík, Zhiwei Steven Wu","[""Microsoft Research"", ""University of Minnesota""]",ICML 2019,2019-06,TRUE
Fair Transfer Learning with Missing Protected Attributes,"Risk assessment is a growing use for machine learning models. When used in high-stakes applications, especially ones regulated by anti-discrimination laws or governed by societal norms for fairness, it is important to ensure that learned models do not propagate and scale any biases that may exist in training data. In this paper, we add on an additional challenge beyond fairness: unsupervised domain adaptation to covariate shift between a source and target distribution. Motivated by the real-world problem of risk assessment in new markets for health insurance in the United States and mobile money-based loans in East Africa, we provide a precise formulation of the machine learning with covariate shift and score parity problem. Our formulation focuses on situations in which protected attributes are not available in either the source or target domain. We propose two new weighting methods: prevalence-constrained covariate shift (PCCS) which does not require protected attributes in the target domain and target-fair covariate shift (TFCS) which does not require protected attributes in the source domain. We empirically demonstrate their efficacy in two applications.","['Amanda Coston', 'Karthikeyan Natesan Ramamurthy', 'Dennis Wei', 'Kush R. Varshney', 'Skyler Speakman', 'Zairah Mustahsan', 'Supriyo Chakraborty']","['IBM Research & Carnegie Mellon University, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Nairobi, Kenya', 'IBM Watson AI Platform, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Fairness and Abstraction in Sociotechnical Systems,"A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce ""fair"" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five ""traps"" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.","['Andrew D. Selbst', 'Danah Boyd', 'Sorelle A. Friedler', 'Suresh Venkatasubramanian', 'Janet Vertesi']","['Data & Society Research Institute, New York, NY', 'Microsoft Research and Data & Society Research Institute, New York, NY', 'Haverford College, Haverford, PA', 'University of Utah, Salt Lake City, UT', 'Princeton University, Princeton, NJ']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Fairness and utilization in allocating resources with uncertain demand,"Resource allocation problems are a fundamental domain in which to evaluate the fairness properties of algorithms. The trade-offs between fairness and utilization have a long history in this domain. A recent line of work has considered fairness questions for resource allocation when the demands for the resource are distributed across multiple groups and drawn from probability distributions. In such cases, a natural fairness requirement is that individuals from different groups should have (approximately) equal probabilities of receiving the resource. A largely open question in this area has been to bound the gap between the maximum possible utilization of the resource and the maximum possible utilization subject to this fairness condition. Here, we obtain some of the first provable upper bounds on this gap. We obtain an upper bound for arbitrary distributions, as well as much stronger upper bounds for specific families of distributions that are typically used to model levels of demand. In particular, we find --- somewhat surprisingly --- that there are natural families of distributions (including Exponential and Weibull) for which the gap is non-existent: it is possible to simultaneously achieve maximum utilization and the given notion of fairness. Finally, we show that for power-law distributions, there is a non-trivial gap between the solutions, but this gap can be bounded by a constant factor independent of the parameters of the distribution.","['Kate Donahue', 'Jon Kleinberg']","['Cornell University', 'Cornell University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Fairness GAN: Generating Datasets with Fairness Properties using a Generative Adversarial Network,"We introduce the Fairness GAN, an approach for generating a dataset that is plausibly similar to a given multimedia dataset, but is more fair with respect to protected attributes in decision making. We propose a novel auxiliary classifier GAN that strives for demographic parity or equality of opportunity and show empirical results on several datasets, including the CelebFaces Attributes (CelebA) dataset, the Quick, Draw! dataset, and a dataset of soccer player images and the offenses they were called for. The proposed formulation is well-suited to absorbing unlabeled data; we leverage this to augment the soccer dataset with the much larger CelebA dataset. The methodology tends to improve demographic parity and equality of opportunity while generating plausible images.","Prasanna Sattigeri, Samuel Hoffman, Vijil Chenthamarakshan, Kush Varshney",IBM,ICLR (2019),2019,TRUE
Fairness in Deceased Organ Matching,"As algorithms are given responsibility to make decisions that impact our lives, there is increasing awareness of the need to ensure the fairness of these decisions. One of the first challenges then is to decide what fairness means in a particular context. We consider here fairness in deciding how to match organs donated by deceased donors to patients. Due to the increasing age of patients on the waiting list, and of organs being donated, the current ""first come, first served'' mechanism used in Australia is under review to take account of age of patients and of organs. We consider how to revise the mechanism to take account of age fairly. We identify a number of different types of fairness, such as to patients, to regions and to blood types and consider how they can be achieved.","['Nicholas Mattei', 'Abdallah Saffidine', 'Toby Walsh']","['IBM Research, Yorktown Heights, NY, USA', 'UNSW Sydney, Sydney, Australia', 'UNSW Sydney, Data61, & TU Berlin, Sydney, Australia']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,TRUE
Fairness in Machine Learning: Lessons from Political Philosophy,"What does it mean for a machine learning model to be 'fair', in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise 'fairness' in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning. ",Reuben Binns,Oxford,FAT* 2018,2018,FALSE
Fairness in Recommendation Ranking through Pairwise Comparisons,"Recommender systems are one of the most pervasive applications of machine learning in industry, with many services using them to match users to products or information.  As such it is important to ask: what are the possible fairness risks, how can we quantify them, and how should we address them?","['Alex Beutel', 'Zhe Zhao', 'Ed H. Chi']",Google,KDD (2019),2019,TRUE
Fairness in Relational Domains,"AI and machine learning tools are being used with increasing frequency for decision making in domains that affect peoples' lives such as employment, education, policing and loan approval. These uses raise concerns about biases of algorithmic discrimination and have motivated the development of fairness-aware machine learning. However, existing fairness approaches are based solely on attributes of individuals. In many cases, discrimination is much more complex, and taking into account the social, organizational, and other connections between individuals is important. We introduce new notions of fairness that are able to capture the relational structure in a domain. We use first-order logic to provide a flexible and expressive language for specifying complex relational patterns of discrimination. Furthermore, we extend an existing statistical relational learning framework, probabilistic soft logic (PSL), to incorporate our definition of relational fairness. We refer to this fairness-aware framework FairPSL. FairPSL makes use of the logical definitions of fairnesss but also supports a probabilistic interpretation. In particular, we show how to perform maximum a posteriori(MAP) inference by exploiting probabilistic dependencies within the domain while avoiding violation of fairness guarantees. Preliminary empirical evaluation shows that we are able to make both accurate and fair decisions.","['Golnoosh Farnadi', 'Behrouz Babaki', 'Lise Getoor']","['UC Santa Cruz, Santa Cruz, CA, USA', 'Polytechnique Montreal, Montreal, PQ, Canada', 'UC Santa Cruz, Santa Cruz, CA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Fairness in Risk Assessment Instruments: Post-Processing to Achieve Counterfactual Equalized Odds,"In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed. The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria. In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.","['Alan Mishler', 'Edward H. Kennedy', 'Alexandra Chouldechova']","['Department of Statistics, Carnegie Mellon University', 'Department of Statistics, Carnegie Mellon University', 'Heinz College Carnegie Mellon University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Fairness Indicators Demo: Scalable Infrastructure for Fair ML Systems,"The rise of machine learning around the globe in fields like medicine, education, employment, credit lending, and criminal sentencing has the potential to reflect and reinforce societal biases at large scale through the models deployed. While fairness concerns are multifaceted, technical evaluations and improvements of models are a critical aspect of a developer's role. And, for these considerations to truly scale, they must integrate into existing processes. In particular, we focus on seamlessly integrating known technical methods with existing libraries used for the training, evaluation, and deployment of models.  To showcase the suite of tools built in Tensorflow, we present an interactive case study demo in conjunction with Conversation AI, an ML research initiative to make online conversations more inclusive.",['Manasi N Joshi'],Google,MLSys 2020,2020,TRUE
Fairness is not static: deeper understanding of long term fairness via simulation studies,"As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets. To address this structural difficulty in the field, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.","[""Alexander D'Amour"", 'Hansa Srinivasan', 'James Atwood', 'Pallavi Baljekar', 'D. Sculley', 'Yoni Halpern']","['Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
"Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives","How should we decide which fairness criteria or
definitions to adopt in machine learning systems?
To answer this question, we must study the fair-
ness preferences of actual users of machine learn-
ing systems. Stringent parity constraints on treat-
ment or impact can come with trade-offs, and
may not even be preferred by the social groups
in question (Zafar et al., 2017). Thus it might
be beneficial to elicit what the groupâs prefer-
ences are, rather than rely on a priori defined
mathematical fairness constraints. Simply asking
for self-reported rankings of users is challenging
because research has shown that there are often
gaps between peopleâs stated and actual prefer-
ences(Bernheim et al., 2013).",['Ben Hutchinson'],Google,Proceedings of ICML 2020 Workshop on Participatory Approaches to Machine Learning,2020,TRUE
Fairness risk measures,"Ensuring that classifiers are non-discriminatory or fair with respect to a sensitive feature (e.g., race or gender) is a topical problem. Progress in this task requires fixing a definition of fairness, and there have been several proposals in this regard over the past few years. Several of these, however, assume either binary sensitive features (thus precluding categorical or real-valued sensitive groups), or result in non-convex objectives (thus adversely affecting the optimisation landscape).  In this paper, we propose a new definition of fairness that generalises some existing proposals, while allowing for generic sensitive features and resulting in a convex objective. The key idea is to enforce that the expected losses (or risks) across each subgroup induced by the sensitive feature are commensurate. We show how this relates to the rich literature on risk measures from mathematical finance. As a special case, this leads to a new convex fairness-aware objective based on minimising the conditional value at risk (CvaR).","Robert C Williamson, Aditya Menon","[""Australian National Laboratory"", ""Google Research""]",ICML 2019,2019-06,TRUE
Fairness Sample Complexity and the Case for Human Intervention,"With the aim of building machine learning systems that incorporate standards of fairness and accountability, we explore explicit subgroup sample complexity bounds. The work is motivated by the observation that classifier predictions for real world datasets often demonstrate drastically different metrics, such as accuracy, when subdivided by specific sensitive variable subgroups.  The reasons for these discrepancies are varied and not limited to the influence of mitigating variables, institutional bias, underlying population distributions as well as selection bias. Among the numerous definitions of fairness that exist, we argue that at a minimum, principled ML practices should ensure that classification predictions are able to mirror the underlying sub-population distributions as a prelude to bias mitigation, and not amplify discrepancies due to sampling/selection bias. However, as the number of sensitive variables grow, populations meeting at the intersectionality of these variables may simply not exist or be large enough to accurately sample from. In theses increasingly likely scenarios, the case for human intervention and applying situational and individual definitions of fairness should be made..    In this paper we explore, setting Pareto-efficient subgroup sample complexity lower bounds based on the complexity of the ML classifier using VC dimension and Rademacher complexity.  We demonstrate that for a classifier to approach a definition of fairness in terms of specific sensitive variables, adequate subgroup population samples need to exist and the model dimensionality has to be aligned with subgroup population distributions.  In cases where this is not feasible human intervention is explored.  We look at two commonly explored UCI datasets under this lens.",['Alyssa Whitlock Lees'],Google,Where is the Human? Bridging the Gap Between AI and HCI at Chi (2019),2019,TRUE
Fairness through Causal Awareness: Learning Causal Latent-Variable Models for Biased Data,"How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.","['David Madras', 'Elliot Creager', 'Toniann Pitassi', 'Richard Zemel']","['University of Toronto, Vector Institute', 'University of Toronto, Vector Institute', 'University of Toronto, Vector Institute', 'University of Toronto, Vector Institute']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning,"Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness","['Vedant Nanda', 'Samuel Dooley', 'Sahil Singla', 'Soheil Feizi', 'John P. Dickerson']","['University of Maryland MPI-SWS', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Fairness Under Unawareness: Assessing Disparity When Protected Class Is Unobserved,"Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.","['Jiahao Chen', 'Nathan Kallus', 'Xiaojie Mao', 'Geoffry Svacha', 'Madeleine Udell']","['', 'Cornell Tech, New York, New York, USA', 'Cornell Tech, New York, New York, USA', '', 'Cornell University, Ithaca, New York, USA']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Fairness Violations and Mitigation under Covariate Shift,"We study the problem of learning fair prediction models for unseen test sets distributed differently from the train set. Stability against changes in data distribution is an important mandate for responsible deployment of models. The domain adaptation literature addresses this concern, albeit with the notion of stability limited to that of prediction accuracy. We identify sufficient conditions under which stable models, both in terms of prediction accuracy and fairness, can be learned. Using the causal graph describing the data and the anticipated shifts, we specify an approach based on feature selection that exploits conditional independencies in the data to estimate accuracy and fairness metrics for the test set. We show that for specific fairness definitions, the resulting model satisfies a form of worst-case optimality. In context of a healthcare task, we illustrate the advantages of the approach in making more equitable decisions.","['Harvineet Singh', 'Rina Singh', 'Vishwali Mhasawade', 'Rumi Chunara']","['Center for Data Science, New York University, New York City, NY, USA', 'Tandon School of Engineering, New York University, New York City, NY, USA', 'Tandon School of Engineering, New York University, New York City, NY, USA', 'Tandon School of Engineering; School of Global Public Health, New York University, New York City, NY, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Fairness warnings and fair-MAML: learning fairly with minimal data,"Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.","['Dylan Slack', 'Sorelle A. Friedler', 'Emile Givental']","['University of California, Irvine', 'Haverford College', 'Haverford College']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Fairness without Harm: Decoupled Classifiers with Preference Guarantees,"In domains such as medicine, it can be acceptable for machine learning models to include sensitive attributes such as gender and ethnicity. In this work, we argue that when there is this kind of treatment disparity, then it should be in the best interest of each group. Drawing on ethical principles such as beneficence (""do the best"") and non-maleficence (""do no harm""), we show how to use sensitive attributes to train decoupled classifiers that satisfy preference guarantees. These guarantees ensure the majority of individuals in each group prefer their assigned classifier to (i) a pooled model that ignores group membership (rationality), and (ii) the model assigned to any other group (envy-freeness). We introduce a recursive procedure that adaptively selects group attributes for decoupling, and present formal conditions to ensure preference guarantees in terms of generalization error. We validate the effectiveness of the procedure on real-world datasets, showing that it improves accuracy without violating preference guarantees on test data. ","Berk Ustun, Yang Liu, David Parkes","[""Harvard University"", ""UC Santa Cruz""]",ICML 2019,2019-06,FALSE
Fairness-Aware Learning for Continuous Attributes and Treatments,"We address the problem of algorithmic fairness: ensuring that the outcome of a classifier is not biased towards certain values of sensitive variables such as age, race or gender. As common fairness metrics can be expressed as measures of (conditional) independence between variables, we propose to use the Rényi maximum correlation coefficient to generalize fairness measurement to continuous variables. We exploit Witsenhausen's characterization of the Rényi correlation coefficient to propose a differentiable implementation linked to f-divergences. This allows us to generalize fairness-aware learning to continuous variables by using a penalty that upper bounds this coefficient. Theses allows fairness to be extented to variables such as mixed ethnic groups or financial status without thresholds effects. This penalty can be estimated on mini-batches allowing to use deep nets. Experiments show favorable comparisons to state of the art on binary variables and prove the ability to protect continuous ones","Jeremie Mary, Clément Calauzčnes, Noureddine El Karoui","[""Criteo AI Lab"", ""Berkeley""]",ICML 2019,2019-06,TRUE
Fairness-Aware Programming,"Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness. We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature, as well as others. As the decision-making program executes, the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated, reporting such violations to the developer. The advantages of this approach are two fold: (i) Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness, as the programmer does not have to write ad hoc code for maintaining statistics. (ii) Compared to existing techniques for checking and ensuring fairness, our approach monitors a decision-making program in the wild, which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested. We describe an implementation of our proposed methodology as a library in the Python programming language and illustrate its use on case studies from the algorithmic fairness literature.","['Aws Albarghouthi', 'Samuel Vinitsky']","['University of Wisconsin-Madison', 'University of Wisconsin-Madison']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
"Fairness, Equality, and Power in Algorithmic Decision-Making","Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same ""merit."" Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by ""merit;"" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.","['Maximilian Kasy', 'Rediet Abebe']","['University of Oxford, Department of Economics', 'University of California, Berkeley, Department of Electrical Engineering & Computer Sciences']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
"Fairness, Welfare, and Equity in Personalized Pricing","We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a ""triple bottom line"": personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.","['Nathan Kallus', 'Angela Zhou']","['Cornell University and Cornell Tech', 'Cornell University and Cornell Tech']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Faithful and Customizable Explanations of Black Box Models,"As predictive models increasingly assist human experts (e.g., doctors) in day-to-day decision making, it is crucial for experts to be able to explore and understand how such models behave in different feature subspaces in order to know if and when to trust them. To this end, we propose Model Understanding through Subspace Explanations (MUSE), a novel model agnostic framework which facilitates understanding of a given black box model by explaining how it behaves in subspaces characterized by certain features of interest. Our framework provides end users (e.g., doctors) with the flexibility of customizing the model explanations by allowing them to input the features of interest. The construction of explanations is guided by a novel objective function that we propose to simultaneously optimize for fidelity to the original model, unambiguity and interpretability of the explanation. More specifically, our objective allows us to learn, with optimality guarantees, a small number of compact decision sets each of which captures the behavior of a given black box model in unambiguous, well-defined regions of the feature space. Experimental evaluation with real-world datasets and user studies demonstrate that our approach can generate customizable, highly compact, easy-to-understand, yet accurate explanations of various kinds of predictive models compared to state-of-the-art baselines.","['Himabindu Lakkaraju', 'Ece Kamar', 'Rich Caruana', 'Jure Leskovec']","['Harvard University, Boston, MA, USA', 'Microsoft Research, Redmond, WA, USA', 'Microsoft Research, Redmond, WA, USA', 'Stanford University, Stanford, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Feature Noise Induces Loss Discrepancy Across Groups," It has been observed that the performance of standard learning procedures differs widely across groups. Recent studies usually attribute this loss discrepancy to an information deficiency for one group (e.g., one group has less data). In this work, we point to a more subtle source of loss discrepancy---feature noise. Our main result is that even when there is no information deficiency specific to one group (e.g., both groups have infinite data), adding the same amount of feature noise to all individuals leads to loss discrepancy. For linear regression, we characterize this loss discrepancy in terms of the amount of noise and difference between moments of the two groups. We then study the time it takes for an estimator to adapt to a shift in the population that makes the groups have the same mean. We finally validate our results on three real-world datasets.","Fereshte Khani, Percy Liang ",Stanford University,ICML 2020,2020-07,FALSE
Federated Heavy Hitters with Differential Privacy,"The discovery of heavy hitters (most frequent items) in user-generated data streams drives improvements in the app and web ecosystems, but can incur substantial privacy risks if not done with care. To address these risks, we propose a distributed and privacy-preserving algorithm for discovering the heavy hitters in a population of user-generated data streams. We leverage the sampling property of our distributed algorithm to prove that it is inherently differentially private, without requiring additional noise. We also examine the trade-off between privacy and utility, and show that our algorithm provides excellent utility while also achieving strong privacy guarantees. A significant advantage of this approach is that it eliminates the need to centralize raw data while also avoiding the significant loss in utility incurred by local differential privacy. We validate our findings both theoretically, using worst-case analyses, and practically, using a Twitter dataset with 1.6M tweets and over 650k users. Finally, we carefully compare our approach to Apple's local differential privacy method for discovering heavy hitters.","['Wennan Zhu', 'Peter Kairouz', 'Brendan McMahan']",Google,International Conference on Artificial Intelligence and Statistics (AISTATS) 2020,2020,TRUE
Fifty Shades of Grey: In Praise of a Nuanced Approach Towards Trustworthy Design,"Environmental data science is uniquely placed to respond to essentially complex and fantastically worthy challenges related to arresting planetary destruction. Trust is needed for facilitating collaboration between scientists who may share datasets and algorithms, and for crafting appropriate science-based policies. Achieving this trust is particularly challenging because of the numerous complexities, multi-scale variables, interdependencies and multi-level uncertainties inherent in environmental data science. Virtual Labs---easily accessible online environments provisioning access to datasets, analysis and visualisations---are socio-technical systems which, if carefully designed, might address these challenges and promote trust in a variety of ways. In addition to various system properties that can be utilised in support of effective collaboration, certain features which are commonly seen to benefit trust---transparency and provenance in particular---appear applicable to promoting trust in and through Virtual Labs. Attempting to realise these features in their design reveals, however, that their implementation is more nuanced and complex than it would appear. Using the lens of affordances, we argue for the need to carefully articulate these features, with consideration of multiple stakeholder needs on balance, so that these Virtual Labs do in fact promote trust. We argue that these features not be conceived as widgets that can be imported into a given context to promote trust; rather, whether they promote trust is a function of how systematically designers consider various (potentially conflicting) stakeholder trust needs.","['Lauren Thornton', 'Bran Knowles', 'Gordon Blair']","['Lancaster University, Lancaster, UK', 'Lancaster University, Lancaster, UK', 'Lancaster University, Lancaster, UK']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Financial Forecasting and Analysis for Low-Wage Workers,"Despite the plethora of financial services and products on the market nowadays, there is a lack of such services and products designed especially for the low-wage population. Approximately 30% of the U.S. working population engage in low-wage work, and many of them lead a paycheck-topaycheck lifestyle. Financial planning advice needs to explicitly address their financial instability. In this paper, we propose a system of data mining techniques on small-scale transactions data to improve automatic and personalized financial planning advice to low-wage workers. We propose robust methods for accurate prediction of bank account balances and automatic extraction of recurring transactions and unexpected large expenses. We formulate a hybrid method consisting of historical data averaging and a regularized regression framework for prediction. To uncover recurring transactions, we use a heuristic approach that capitalizes on transaction descriptions. Our methods achieve higher performance compared to conventional approaches and stateof-the-art predictive methods in real financial transactions data. The proposed methods will upgrade the functionalities in WageGoal, Neighborhood Trust Financial Partners’ web-based application that provides budgeting and cash flow management services to a user base comprising mostly lowincome individuals. The proposed methods will therefore have a direct impact on the individuals who are or will be connected to the product.","Wenyu Zhang, Raya Horesh, Karthikeyan NatesanRamamurthy, Lingfei Wu, Jinfeng Yi, Kryn Anderson, Kush R. Varshney",IBM,Data for Good Exchange (2018),2018,TRUE
Flexibly Fair Representation Learning by Disentanglement,"We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also \emph&lbrace;flexibly fair&rbrace;, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder---which does not require the sensitive attributes for inference---allows for the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.",['Kevin Jordan Swersky'],Google,ICML (2019),2019,TRUE
FlipTest: fairness testing via optimal transport,"We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.","['Emily Black', 'Samuel Yeom', 'Matt Fredrikson']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods,"As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.","['Dylan Slack', 'Sophie Hilgard', 'Emily Jia', 'Sameer Singh', 'Himabindu Lakkaraju']","['University of California, Irvine, Irvine, CA, USA', 'Harvard University, Cambridge, MA, USA', 'Harvard University, Cambridge, MA, USA', 'University of California, Irvine, Irvine, CA, USA', 'Harvard University, Cambridge, MA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
"Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI","Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.","['Alon Jacovi', 'Ana Marasović', 'Tim Miller', 'Yoav Goldberg']","['Bar Ilan University', 'Allen Institute for Artificial Intelligence, University of Washington', 'School of Computing and Information Systems, The University of Melbourne', 'Bar Ilan University, Allen Institute for Artificial Intelligence']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
FR-Train: A mutual information-based approach to fair and robust training,"Trustworthy AI is a critical issue in machine learning where, in addition to training a model that is accurate, one must consider both fair and robust training in the presence of data bias and poisoning. However, the existing model fairness techniques mistakenly view poisoned data as an additional bias, resulting in severe performance degradation. To fix this problem, we propose FR-Train, which holistically performs fair and robust model training. We provide a mutual information-based interpretation of an existing adversarial training-based fairness-only method, and apply this idea to architect an additional discriminator that can identify poisoned data using a clean validation set and reduce its influence. In our experiments, FR-Train shows almost no decrease in fairness and accuracy in the presence of data poisoning by both mitigating the bias and defending against poisoning. We also demonstrate how to construct clean validation sets using crowdsourcing, and release new benchmark datasets.","Yuji Roh, Kangwook Lee, Steven Whang, Changho Suh ","[""Korea Advanced Institute ofScience and Technology"", ""University of Wisconsin-Madison""]",ICML 2020,2020-07,FALSE
Framing Artificial Intelligence in American Newspapers,"Publics' perceptions of new scientific advances such as AI are often informed and influenced by news coverage. To understand how artificial intelligence (AI) was framed in U.S. newspapers, a content analysis based on framing theory in journalism and science communication was conducted. This study identified the dominant topics and frames, as well as the risks and benefits of AI covered in five major American newspapers from 2009 to 2018. Results indicated that business and technology were the primary topics in news coverage of AI. The benefits of AI were discussed more frequently than its risks, but risks of AI were generally discussed with greater specificity. Additionally, episodic issue framing and societal impact framing were more frequently used.","['Ching-Hua Chuan', 'Wan-Hsiu Sunny Tsai', 'Su Yeon Cho']","['University of Miami, Coral Gables, FL, USA', 'University of Miami, Coral Gables, FL, USA', 'University of Miami, Coral Gables, FL, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy,"The word 'ethics' is under siege in technology policy circles. Weaponized in support of deregulation, self-regulation or handsoff governance, ""ethics"" is increasingly identified with technology companies' self-regulatory efforts and with shallow appearances of ethical behavior. So-called ""ethics washing"" by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech community at large. In parallel to the growth of ethics washing, its condemnation has led to a tendency to engage in ""ethics bashing."" This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards, self-governance schemes or stakeholder groups. The misunderstandings underlying ethics bashing are at least threefold: (a) philosophy and ""ethics"" are seen as a communications strategy and as a form of instrumentalized cover-up or façade for unethical behavior, (b) philosophy is understood in opposition and as alternative to political representation and social organizing and (c) the role and importance of moral philosophy is downplayed and portrayed as mere ""ivory tower"" intellectualization of complex problems that need to be dealt with in practice. This paper argues that the rhetoric of ethics and morality should not be reductively instrumentalized, either by the industry in the form of ""ethics washing,"" or by scholars and policy-makers in the form of ""ethics bashing."" Grappling with the role of philosophy and ethics requires moving beyond both tendencies and seeing ethics as a mode of inquiry that facilitates the evaluation of competing tech policy strategies. In other words, we must resist narrow reductivism of moral philosophy as instrumentalized performance and renew our faith in its intrinsic moral value as a mode of knowledgeseeking and inquiry. Far from mandating a self-regulatory scheme or a given governance structure, moral philosophy in fact facilitates the questioning and reconsideration of any given practice, situating it within a complex web of legal, political and economic institutions. Moral philosophy indeed can shed new light on human practices by adding needed perspective, explaining the relationship between technology and other worthy goals, situating technology within the human, the social, the political. It has become urgent to start considering technology ethics also from within and not only from outside of ethics.",['Elettra Bietti'],['Harvard Law School'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
From Fair Decision Making To Social Equality,"The study of fairness in intelligent decision systems has mostly ignored long-term influence on the underlying population. Yet fairness considerations (e.g. affirmative action) have often the implicit goal of achieving balance among groups within the population. The most basic notion of balance is eventual equality between the qualifications of the groups. How can we incorporate influence dynamics in decision making? How well do dynamics-oblivious fairness policies fare in terms of reaching equality? In this paper, we propose a simple yet revealing model that encompasses (1) a selection process where an institution chooses from multiple groups according to their qualifications so as to maximize an institutional utility and (2) dynamics that govern the evolution of the groups' qualifications according to the imposed policies. We focus on demographic parity as the formalism of affirmative action. We first give conditions under which an unconstrained policy reaches equality on its own. In this case, surprisingly, imposing demographic parity may break equality. When it doesn't, one would expect the additional constraint to reduce utility, however, we show that utility may in fact increase. In real world scenarios, unconstrained policies do not lead to equality. In such cases, we show that although imposing demographic parity may remedy it, there is a danger that groups settle at a worse set of qualifications. As a silver lining, we also identify when the constraint not only leads to equality, but also improves all groups. These cases and trade-offs are instrumental in determining when and how imposing demographic parity can be beneficial in selection processes, both for the institution and for society on the long run.","['Hussein Mouzannar', 'Mesrob I. Ohannessian', 'Nathan Srebro']","['American University of Beirut', 'Toyota Technological Institute at Chicago', 'Toyota Technological Institute at Chicago']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
From Optimizing Engagement to Measuring Value,"Most recommendation engines today are based on predicting user engagement, e.g. predicting whether a user will click on an item or not. However, there is potentially a large gap between engagement signals and a desired notion of value that is worth optimizing for. We use the framework of measurement theory to (a) confront the designer with a normative question about what the designer values, (b) provide a general latent variable model approach that can be used to operationalize the target construct and directly optimize for it, and (c) guide the designer in evaluating and revising their operationalization. We implement our approach on the Twitter platform on millions of users. In line with established approaches to assessing the validity of measurements, we perform a qualitative evaluation of how well our model captures a desired notion of ""value"".","['Smitha Milli', 'Luca Belli', 'Moritz Hardt']","['UC Berkeley', 'Twitter', 'UC Berkeley']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
"From Papers to Programs: Courts, Corporations, Clinics and the Battle over Computerized Psychological Testing","This paper examines the role of technology firms in computerizing personality tests from the early 1960s to late 1980s. It focuses on the National Computer Systems (NCS) and their development of an automated interpretation for the Minnesota Multiphasic Personality inventory (MMPI). NCS trumpeted their computerized interpretation as a way to free up clerical labor and mitigate human bias. Yet psychologists cautioned that proprietary algorithms risked obscuring decision rules. I show how clinics, courtrooms, and businesses all had competing interests in the use of computerized personality tests. As I argue, the development of computerized psychological tests was shaped both by business concerns about intellectual property and profits and psychologists' concerns with validity and access to algorithms. Across these domains, the common claim was that computerized psychological testing could provide a technical fix for bias. This paper contributes to histories of computing emphasizing the importance of IP, the relationship between labor, technology, and expertise, and to histories of algorithms.",['Kira Lussier'],['University of Toronto'],"FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
From Soft Classifiers to Hard Decisions: How fair can we be?,"A popular methodology for building binary decision-making classifiers in the presence of imperfect information is to first construct a calibrated non-binary ""scoring"" classifier, and then to post-process this score to obtain a binary decision. We study various fairness (or, error-balance) properties of this methodology, when the non-binary scores are calibrated over all protected groups, and with a variety of post-processing algorithms. Specifically, we show: First, there does not exist a general way to post-process a calibrated classifier to equalize protected groups' positive or negative predictive value (PPV or NPV). For certain ""nice"" calibrated classifiers, either PPV or NPV can be equalized when the post-processor uses different thresholds across protected groups. Still, when the post-processing consists of a single global threshold across all groups, natural fairness properties, such as equalizing PPV in a nontrivial way, do not hold even for ""nice"" classifiers. Second, when the post-processing stage is allowed to defer on some decisions (that is, to avoid making a decision by handing off some examples to a separate process), then for the non-deferred decisions, the resulting classifier can be made to equalize PPV, NPV, false positive rate (FPR) and false negative rate (FNR) across the protected groups. This suggests a way to partially evade the impossibility results of Chouldechova and Kleinberg et al., which preclude equalizing all of these measures simultaneously. We also present different deferring strategies and show how they affect the fairness properties of the overall system. We evaluate our post-processing techniques using the COMPAS data set from 2016.","['Ran Canetti', 'Aloni Cohen', 'Nishanth Dikkala', 'Govind Ramnarayan', 'Sarah Scheffler', 'Adam Smith']","['Boston University and Tel Aviv University', 'MIT', 'MIT', 'MIT', 'Boston University', 'Boston University']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
"Garbage in, garbage out?: do machine learning application papers in social computing report where human-labeled training data comes from?","Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper's authors labeling the data themselves. Such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. In this paper, we investigate to what extent a sample of machine learning application papers in social computing --- specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data --- give specific details about whether such best practices were followed. Our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: Does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. We find a wide divergence in whether such practices were followed and documented. Much of machine learning research and education focuses on what is done once a ""gold standard"" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.","['R. Stuart Geiger', 'Kevin Yu', 'Yanlai Yang', 'Mindy Dai', 'Jie Qiu', 'Rebekah Tang', 'Jenny Huang']","['University of California', 'University of California', 'University of California', 'University of California', 'University of California', 'University of California', 'University of California']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,"Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms. ","Joy Buolamwini, Timnit Gebru","[""MIT Media Lab"", ""Microsoft Research""]",FAT* 2018,2018,TRUE
Giving AI a Theory of Mind,"Effective collaboration between humans and artificially intelligent agents will require that the two are equipped to build a sense of mutual understanding with each other. When humans have an intuitive understanding of the motives and intentions of other humans, it is known as Theory of Mind. My work revolves around designing artificial intelligence to leverage this capacity to improve human collaborations with artificial agents.",['Bobbie Eicher'],"['Georgia Institute of Technology, Atlanta, GA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Global Explanations of Neural Networks: Mapping the Landscape of Predictions,"A barrier to the wider adoption of neural networks is their lack of interpretability. While local explanation methods exist for one prediction, most global attributions still reduce neural network decisions to a single set of features. In response, we present an approach for generating global attributions called GAM, which explains the landscape of neural network predictions across subpopulations. GAM augments global explanations with the proportion of samples that each attribution best explains and specifies which samples are described by each attribution. Global explanations also have tunable granularity to detect more or fewer subpopulations. We demonstrate that GAM's global explanations 1) yield the known feature importances of simulated data, 2) match feature weights of interpretable statistical models on real data, and 3) are intuitive to practitioners through user studies. With more transparent predictions, GAM can help ensure neural network decisions are generated for the right reasons.","['Mark Ibrahim', 'Melissa Louie', 'Ceena Modarres', 'John Paisley']","['Center for Machine Learning, Capital One, New York, NY, USA', 'Center for Machine Learning, Capital One, New York, NY, USA', 'Center for Machine Learning, Capital One, New York, NY, USA', 'Columbia University, New York, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,TRUE
Good Explanation for Algorithmic Transparency,"Machine learning algorithms have gained widespread usage across a variety of domains, both in providing predictions to expert users and recommending decisions to everyday users. However, these AI systems are often black boxes, and end-users are rarely provided with an explanation. The critical need for explanation by AI systems has led to calls for algorithmic transparency, including the ""right to explanation'' in the EU General Data Protection Regulation (GDPR). These initiatives presuppose that we know what constitutes a meaningful or good explanation, but there has actually been surprisingly little research on this question in the context of AI systems. In this paper, we (1) develop a generalizable framework grounded in philosophy, psychology, and interpretable machine learning to investigate and define characteristics of good explanation, and (2) conduct a large-scale lab experiment to measure the impact of different factors on people's perceptions of understanding, usage intention, and trust of AI systems. The framework and study together provide a concrete guide for managers on how to present algorithmic prediction rationales to end-users to foster trust and adoption, and elements of explanation and transparency to be considered by AI researchers and engineers in designing, developing, and deploying transparent or explainable algorithms.","['Joy Lu', 'Dokyun (DK) Lee', 'Tae Wan Kim', 'David Danks']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy,"Purpose
Use adjudication to quantify errors in diabetic retinopathy (DR) grading based on individual graders and majority decision, and to train an improved automated algorithm for DR grading.","['Jonathan Krause', 'Varun Gulshan', 'Kasumi Widner', 'Greg Corrado', 'Lily Peng', 'Dale Webster']",Google,Ophthalmology (2018),2018,TRUE
Group Fairness: Independence Revisited,"This paper critically examines arguments against independence, a measure of group fairness also known as statistical parity and as demographic parity. In recent discussions of fairness in computer science, some have maintained that independence is not a suitable measure of group fairness. This position is at least partially based on two influential papers (Dwork et al., 2012, Hardt et al., 2016) that provide arguments against independence. We revisit these arguments, and we find that the case against independence is rather weak. We also give arguments in favor of independence, showing that it plays a distinctive role in considerations of fairness. Finally, we discuss how to balance different fairness considerations.",['Tim Räz'],"['Institute of Philosophy, University of Bern, Switzerland Institute of Biomedical Ethics and History of Medicine, University of Zürich, Switzerland']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Guiding Prosecutorial Decisions with an Interpretable Statistical Model,"After a felony arrest, many American jurisdictions hold individuals for several days while police officers investigate the incident and prosecutors decide whether to press criminal charges. This pre-arraignment detention can both preserve public safety and reduce the need for officers to seek out and re-arrest individuals who are ultimately charged with a crime. Such detention, however, also comes at a high social and financial cost to those who are never charged but still incarcerated. In one of the first large-scale empirical analyses of pre-arraignment detention, we examine police reports and charging decisions for approximately 30,000 felony arrests in a major American city between 2012 and 2017. We find that 45% of arrested individuals are never charged for any crime but still typically spend one or more nights in jail before being released. In an effort to reduce such incarceration, we develop a statistical model to help prosecutors identify cases soon after arrest that are likely to be ultimately dismissed. By carrying out an early review of five such candidate cases per day, we estimate that prosecutors could potentially reduce pre-arraignment incarceration for ultimately dismissed cases by 35%. To facilitate implementation and transparency, our model to prioritize cases for early review is designed as a simple, weighted checklist. We show that this heuristic strategy achieves comparable performance to traditional, black-box machine learning models.","['Zhiyuan Lin', 'Alex Chohlas-Wood', 'Sharad Goel']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,"The implementation of AI systems has led to new forms of harm in various sensitive social domains. We analyze these as problems How to address these harms remains at the center of controversial debate. In this paper, we discuss the inherent normative uncertainty and political debates surrounding the safety of AI systems.of vagueness to illustrate the shortcomings of current technical approaches in the AI Safety literature, crystallized in three dilemmas that remain in the design, training and deployment of AI systems. We argue that resolving normative uncertainty to render a system 'safe' requires a sociotechnical orientation that combines quantitative and qualitative methods and that assigns design and decision power across affected stakeholders to navigate these dilemmas through distinct channels for dissent. We propose a set of sociotechnical commitments and related virtues to set a bar for declaring an AI system 'human-compatible', implicating broader interdisciplinary design approaches.","['Roel I.J. Dobbe', 'Thomas Krendl Gilbert', 'Yonatan Mintz']","['New York University, New York, NY, USA', 'University of California, Berkeley, Berkeley, CA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Hiding Images Within Images,"We present a system to hide a full color image inside another of the
same size with minimal quality loss to either image.  Deep neural
networks are simultaneously trained to create the hiding and revealing
processes and are designed to specifically work as a pair.  The system
is trained on images drawn randomly from the ImageNet database, and
works well on natural images from a wide variety of sources.  Beyond
demonstrating the successful application of deep learning to hiding
images, we examine how the result is achieved and apply numerous
transformations to analyze if image quality in the host and hidden
image can be maintained.  These transformation range from simple image
manipulations to sophisticated machine learning-based adversaries.
Two extensions to the basic system are presented that mitigate the
possibility of discovering the content of the hidden image.  With
these extensions, not only can the hidden information be kept secure,
but the system can be used to hide even more than a single image.
Applications for this technology include image authentication,
digital watermarks, finding exact regions of image manipulation, and
storing meta-information about image rendering and content.",['Shumeet Baluja'],Google,IEEE Transactions on Pattern Analysis and Machine Intelligence (2019),2019,TRUE
High Dimensional Model Explanations: An Axiomatic Approach,"Complex black-box machine learning models are regularly used in critical decision-making domains. This has given rise to several calls for algorithmic explainability. Many explanation algorithms proposed in literature assign importance to each feature individually. However, such explanations fail to capture the joint effects of sets of features. Indeed, few works so far formally analyze high dimensional model explanations. In this paper, we propose a novel high dimension model explanation method that captures the joint effect of feature subsets. We propose a new axiomatization for a generalization of the Banzhaf index; our method can also be thought of as an approximation of a black-box model by a higher-order polynomial. In other words, this work justifies the use of the generalized Banzhaf index as a model explanation by showing that it uniquely satisfies a set of natural desiderata and that it is the optimal local approximation of a black-box model. Our empirical evaluation of our measure highlights how it manages to capture desirable behavior, whereas other measures that do not satisfy our axioms behave in an unpredictable manner.","['Neel Patel', 'Martin Strobel', 'Yair Zick']","['University of Southern California', 'National University of Singapore', 'University of Massachusetts, Amherst']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
How can I choose an explainer?: An Application-grounded Evaluation of Post-hoc Explanations,"There have been several research works proposing new Explainable AI (XAI) methods designed to generate model explanations having specific properties, or desiderata, such as fidelity, robustness, or human-interpretability. However, explanations are seldom evaluated based on their true practical impact on decision-making tasks. Without that assessment, explanations might be chosen that, in fact, hurt the overall performance of the combined system of ML model + end-users. This study aims to bridge this gap by proposing XAI Test, an application-grounded evaluation methodology tailored to isolate the impact of providing the end-user with different levels of information. We conducted an experiment following XAI Test to evaluate three popular XAI methods - LIME, SHAP, and TreeInterpreter - on a real-world fraud detection task, with real data, a deployed ML model, and fraud analysts. During the experiment, we gradually increased the information provided to the fraud analysts in three stages: Data Only, i.e., just transaction data without access to model score nor explanations, Data + ML Model Score, and Data + ML Model Score + Explanations. Using strong statistical analysis, we show that, in general, these popular explainers have a worse impact than desired. Some of the conclusion highlights include: i) showing Data Only results in the highest decision accuracy and the slowest decision time among all variants tested, ii) all the explainers improve accuracy over the Data + ML Model Score variant but still result in lower accuracy when compared with Data Only; iii) LIME was the least preferred by users, probably due to its substantially lower variability of explanations from case to case.","['Sérgio Jesus', 'Catarina Belém', 'Vladimir Balayan', 'João Bento', 'Pedro Saleiro', 'Pedro Bizarro', 'João Gama']","['Feedzai, DCC-FCUP, Universidade do Porto', 'Feedzai', 'Feedzai', 'Feedzai', 'Feedzai', 'Feedzai', 'LIAAD, INESCTEC, Universidade do Porto']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
How Do Fairness Definitions Fare?: Examining Public Attitudes Towards Algorithmic Definitions of Fairness,"What is the best way to define algorithmic fairness? While many definitions of fairness have been proposed in the computer science literature, there is no clear agreement over a particular definition. In this work, we investigate ordinary people's perceptions of three of these fairness definitions. Across two online experiments, we test which definitions people perceive to be the fairest in the context of loan decisions, and whether fairness perceptions change with the addition of sensitive information (i.e., race of the loan applicants). Overall, one definition (calibrated fairness) tends to be more pre- ferred than the others, and the results also provide support for the principle of affirmative action.","['Nripsuta Ani Saxena', 'Karen Huang', 'Evan DeFilippis', 'Goran Radanovic', 'David C. Parkes', 'Yang Liu']","['University of Southern California, Los Angeles, CA, USA', 'Harvard University, Cambridge , MA, USA', 'Harvard University, Cambridge, MA, USA', 'Harvard University, Cambridge, MA, USA', 'Harvard University, Cambridge, MA, USA', 'University of California, Santa Cruz, Santa Cruz, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
How Technological Advances Can Reveal Rights,"Over recent decades, technological development has been accompanied by the proposal of new rights by various groups and individuals: the right to public anonymity, the right to be forgotten, and the right to disconnect, for example. Although there is widespread acknowledgment of the motivation behind these proposed rights, there is little agreement about their actual normative status. One potential challenge is that the claims only arise in contingent social-technical contexts, which may affect how we conceive of them ethically (albeit, not necessarily in terms of policy). What sort of morally legitimate rights claims depend on such contingencies? Our paper investigates the grounds on which such proposals might be considered ""actual"" rights. The full paper can be found at http://www.andrew.cmu.edu/user/cgparker/Parker_Danks_RevealedRights.pdf. We propose the notion of a revealed right, a right that only imposes duties -- and thus is only meaningfully revealed -- in certain technological contexts. Our framework is based on an interest theory approach to rights, which understands rights in terms of a justificatory role: morally important aspects of a person's well-being (interests) ground rights, which then justify holding someone to a duty that promotes or protects that interest. Our framework uses this approach to interpret the conflicts that lead to revealed rights in terms of how technological developments cause shifts in the balance of power to promote particular interests. Different parties can have competing or conflicting interests. It is also generally accepted that some interests are more normatively important than others (even if only within a particular framework). We can refer to this difference in importance by saying that the former interest has less ""moral weight"" than the latter interest (in that context). The moral weight of an interest is connected to its contribution to the interest-holder's overall well-being, and thereby determines the strength of the reason that a corresponding right provides to justify a duty. Improved technology can offer resources that grant one party increased causal power to realize its interests to the detriment of another's capacity to do so, even while the relative moral weight of their interests remain the same. Such changes in circumstance can make the importance of protecting a particular interest newly salient. If that interest's moral weight justifies establishing a duty to protect it, thereby limiting the threat posed by the new socio-technical context, then a right is revealed. Revealed rights justify realignment between the moral weight and causal power orderings so that people with weightier interests have greater power to protect those interests. In the extended paper, we show how this account can be applied to the interpretation of two recently proposed ""rights"": the right to be forgotten, and the right to disconnect. Since we are focused on making sense of revealed rights, not any particular substantive theory of interests or well-being, the characterization of 'weights' is a free parameter in this account. Our framework alone cannot provide means to resolve the question of whether specific rights exist, but it can be used to identify empirical questions that need to be answered to decide the existence or non-existence of such rights. The emergence of a revealed right depends on a number of factors, including: whether the plausible uses of the technology could potentially impede another's well-being or interests; whether the technology is sufficiently common to have a wider, social impact; and whether the technology has actually changed the balance of power sufficiently to yield a frequent possibility for misalignment between causal power and moral weight. This approach confronts the question of how, in principle, such rights could be justified, without requiring specific commitments on the ontology of rights. Our account explains why the rhetoric of ""new rights"" is both accurate (since the rights were not previously recognized) and inaccurate (since the rights were present all along, but without corresponding duties). Further, it explains the rights without grounding their normative status in considerations related to right-holders' capacities to rationally waive or assert claims. This is especially important given that many of the relevant disruptive technological developments pose challenges to understanding by affected parties for the same reasons they pose threats to those parties' well-being. In the course of our discussion, we confront a number of potential objections to the account. We argue that our framework's ability to accommodate highly specific or derivative-seeming rights is un-problematic. We also head off worries that our use of interest theory makes the account likely to recognize absurd rights claims.","['Jack Parker', 'David Danks']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
How to Solve Fair k-Center in Massive Data Models,"Fueled by massive data, important decision making is being automated with the help of algorithms, therefore, fairness in algorithms has become an especially important research topic. In this work, we design new streaming and distributed algorithms for the fair k-center problem that models fair data summarization. The streaming and distributed models of computation have an attractive feature of being able to handle massive data sets that do not fit into main memory. Our main contributions are: (a) the first distributed algorithm; which has provably constant approximation ratio and is extremely parallelizable, and (b) a two-pass streaming algorithm with a provable approximation guarantee matching the best known algorithm (which is not a streaming algorithm). Our algorithms have the advantages of being easy to implement in practice, being fast with linear running times, having very small working memory and communication, and outperforming existing algorithms on several real and synthetic data sets. To complement our distributed algorithm, we also give a hardness result for natural distributed algorithms, which holds for even the special case of k-center.","Ashish Chiplunkar, Sagar Kale, Sivaramakrishnan Natarajan Ramamoorthy ","[""IIT Delhi"", ""University of Vienna"", ""University of Washington""]",ICML 2020,2020-07,FALSE
Human Comprehension of Fairness in Machine Learning,"Bias in machine learning has manifested injustice in several areas, with notable examples including gender bias in job-related ads [4], racial bias in evaluating names on resumes [3], and racial bias in predicting criminal recidivism [1]. In response, research into algorithmic fairness has grown in both importance and volume over the past few years. Different metrics and approaches to algorithmic fairness have been proposed, many of which are based on prior legal and philosophical concepts [2]. The rapid expansion of this field makes it difficult for professionals to keep up, let alone the general public. Furthermore, misinformation about notions of fairness can have significant legal implications. Computer scientists have largely focused on developing mathematical notions of fairness and incorporating them in fielded ML systems. A much smaller collection of studies has measured public perception of bias and (un)fairness in algorithmic decision-making. However, one major question underlying the study of ML fairness remains unanswered in the literature: Does the general public understand mathematical definitions of ML fairness and their behavior in ML applications? We take a first step towards answering this question by studying non-expert comprehension and perceptions of one popular definition of ML fairness, demographic parity [5]. Specifically, we developed an online survey to address the following: (1) Does a non-technical audience comprehend the definition and implications of demographic parity? (2) Do demographics play a role in comprehension? (3) How are comprehension and sentiment related? (4) Does the application scenario affect comprehension?","['Debjani Saha', 'Candice Schumann', 'Duncan C. McElfresh', 'John P. Dickerson', 'Michelle L. Mazurek', 'Michael Carl Tschantz']","['University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'International Computer Science Institute, Berkeley, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Human Trust Measurement Using an Immersive Virtual Reality Autonomous Vehicle Simulator,"Recent studies indicate that people are negatively predisposed toward utilizing autonomous systems. These findings highlight the necessity of conducting research to better understand the evolution of trust between humans and growing autonomous technologies such as self-driving cars (SDC). This research presents a new approach for real-time trust measurement between passengers and SDCs. We utilized a new structured data collection approach along with a virtual reality SDC simulator to understand how various autonomous driving scenarios can increase or decrease human trust and how trust can be re-built in the case of incidental failures. To verify our methodology, we designed and conducted an empirical experiment on 50 human subjects. The results of this experiment indicated that most subjects could rebuild trust during a reasonable time frame after the system demonstrated faulty behavior. Our analysis showed that this approach is highly effective for collecting real-time data from human subjects and lays the foundation for more-involved future research in the domain of human trust and autonomous driving.","['Shervin Shahrdar', 'Corey Park', 'Mehrdad Nojoumian']","['Florida Atlantic University, Boca Raton, FL, USA', 'Florida Atlantic University, Boca Raton, FL, USA', 'Florida Atlantic University, Boca Raton, FL, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Human-AI Learning Performance in Multi-Armed Bandits,"People frequently face challenging decision-making problems in which outcomes are uncertain or unknown. Artificial intelligence (AI) algorithms exist that can outperform humans at learning such tasks. Thus, there is an opportunity for AI agents to assist people in learning these tasks more effectively. In this work, we use a multi-armed bandit as a controlled setting in which to explore this direction. We pair humans with a selection of agents and observe how well each human-agent team performs. We find that team performance can beat both human and agent performance in isolation. Interestingly, we also find that an agent's performance in isolation does not necessarily correlate with the human-agent team's performance. A drop in agent performance can lead to a disproportionately large drop in team performance, or in some settings can even improve team performance. Pairing a human with an agent that performs slightly better than them can make them perform much better, while pairing them with an agent that performs the same can make them them perform much worse. Further, our results suggest that people have different exploration strategies and might perform better with agents that match their strategy. Overall, optimizing human-agent team performance requires going beyond optimizing agent performance, to understanding how the agent's suggestions will influence human decision-making.","['Ravi Pandya', 'Sandy H. Huang', 'Dylan Hadfield-Menell', 'Anca D. Dragan']","['University of California, Berkeley, Berkeley, CA, USA', 'University of California, Berkeley, Berkeley, CA, USA', 'University of California, Berkeley, Berkeley, CA, USA', 'University of California, Berkeley, Berkeley, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Human-Centered Tools for Coping with Imperfect Algorithms during Medical Decision-Making,"Machine learning (ML) is increasingly being used in image retrieval systems for medical decision making. One application of ML is to retrieve visually similar medical images from past patients (e.g. tissue from biopsies) to reference when making a medical decision with a new patient. However, no algorithm can perfectly capture an expert's ideal notion of similarity for every case: an image that is algorithmically determined to be similar may not be medically relevant to a doctor's specific diagnostic needs. In this paper, we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm, and developed tools that empower users to cope with the search algorithm on-the-fly, communicating what types of similarity are most important at different moments in time. In two evaluations with pathologists, we found that these refinement tools increased the diagnostic utility of images found and increased user trust in the algorithm. The tools were preferred over a traditional interface, without a loss in diagnostic accuracy. We also observed that users adopted new strategies when using refinement tools, re-purposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors. Taken together, these findings inform future human-ML collaborative systems for expert decision-making.","['Carrie Jun Cai', 'Emily Reif', 'Narayan G Hegde', 'Been Kim', 'Daniel Smilkov', 'Martin Wattenberg', 'Fernanda Viégas', 'Greg Corrado']",Google,Conference on Human Factors in Computing Systems (2019),2019,TRUE
Human-in-the-Loop Interpretability Prior,"We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.",['Been Kim'],Google,NeurIPS (Spotlight) (2018),2018,TRUE
"I agree with the decision, but they didn't deserve this: Future Developers' Perception of Fairness in Algorithmic Decisions","While professionals are increasingly relying on algorithmic systems for making a decision, on some occasions, algorithmic decisions may be perceived as biased or not just. Prior work has looked into the perception of algorithmic decision-making from the user's point of view. In this work, we investigate how students in fields adjacent to algorithm development perceive algorithmic decisionmaking. Participants (N=99) were asked to rate their agreement with statements regarding six constructs that are related to facets of fairness and justice in algorithmic decision-making in three separate scenarios. Two of the three scenarios were independent of each other, while the third scenario presented three different outcomes of the same algorithmic system, demonstrating perception changes triggered by different outputs. Quantitative analysis indicates that a) 'agreeing' with a decision does not mean the person 'deserves the outcome', b) perceiving the factors used in the decision-making as 'appropriate' does not make the decision of the system 'fair' and c) perceiving a system's decision as 'not fair' is affecting the participants' 'trust' in the system. In addition, participants found proportional distribution of benefits more fair than other approaches. Qualitative analysis provides further insights into that information the participants find essential to judge and understand an algorithmic decision-making system's fairness. Finally, the level of academic education has a role to play in the perception of fairness and justice in algorithmic decision-making.","['Maria Kasinidou', 'Styliani Kleanthous', 'Pınar Barlas', 'Jahna Otterbacher']","['Cyprus Center for Algorithmic Transparency, Open University of Cyprus, Nicosia, Cyprus', 'Cyprus Center for Algorithmic Transparency, Open University of Cyprus Nicosia, Cyprus', 'Research Centre on Interactive Media, Smart Systems and Emerging Technologies, Nicosia, Cyprus', 'Cyprus Center for Algorithmic Transparency, Open University of Cyprus, Nicosia, Cyprus']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases,"Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition. But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects? We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images. We find that state-of-the-art unsupervised models trained on ImageNet, a popular benchmark image dataset curated from internet images, automatically learn racial, gender, and intersectional biases. We replicate 8 previously documented human biases from social psychology, from the innocuous, as with insects and flowers, to the potentially harmful, as with race and gender. Our results closely match three hypotheses about intersectional bias from social psychology. For the first time in unsupervised computer vision, we also quantify implicit human biases about weight, disabilities, and several ethnicities. When compared with statistical patterns in online image datasets, our findings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web.","['Ryan Steed', 'Aylin Caliskan']","['Carnegie Mellon University, Pittsburgh, Pennsylvania, USA', 'George Washington University, Washington, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
IMLI: An Incremental Framework for MaxSAT-Based Learning of Interpretable Classification Rules,"The wide adoption of machine learning in the critical domains such as medical diagnosis, law, education had propelled the need for interpretable techniques due to the need for end users to understand the reasoning behind decisions due to learning systems. The computational intractability of interpretable learning led practitioners to design heuristic techniques, which fail to provide sound handles to tradeoff accuracy and interpretability. Motivated by the success of MaxSAT solvers over the past decade, recently MaxSAT-based approach, called MLIC, was proposed that seeks to reduce the problem of learning interpretable rules expressed in Conjunctive Normal Form (CNF) to a MaxSAT query. While MLIC was shown to achieve accuracy similar to that of other state of the art black-box classifiers while generating small interpretable CNF formulas, the runtime performance of MLIC is significantly lagging and renders approach unusable in practice. In this context, authors raised the question: Is it possible to achieve the best of both worlds, i.e., a sound framework for interpretable learning that can take advantage of MaxSAT solvers while scaling to real-world instances? In this paper, we take a step towards answering the above question in affirmation. We propose IMLI: an incremental approach to MaxSAT based framework that achieves scalable runtime performance via partition-based training methodology. Extensive experiments on benchmarks arising from UCI repository demonstrate that IMLI achieves up to three orders of magnitude runtime improvement without loss of accuracy and interpretability.","['Bishwamittra Ghosh', 'Kuldeep S. Meel']","['National University of Singapore, Singapore, Singapore', 'National University of Singapore, Singapore, Singapore']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Impacts on Trust of Healthcare AI,"Artificial Intelligence and robotics are rapidly moving into healthcare, playing key roles in specific medical functions, including diagnosis and clinical treatment. Much of the focus in the technology development has been on human-machine interactions, leading to a host of related technology-centric questions. In this paper, we focus instead on the impact of these technologies on human-human interactions and relationships within the healthcare domain. In particular, we argue that trust plays a central role for relationships in the healthcare domain, and the introduction of healthcare AI can potentially have significant impacts on those relations of trust. We contend that healthcare AI systems ought to be treated as assistive technologies that go beyond the usual functions of medical devices. As a result, we need to rethink regulation of healthcare AI systems to ensure they advance relevant values. We propose three distinct guidelines that can be universalized across federal regulatory boards to ensure that patient-doctor trust is not detrimentally affected by the deployment and widespread adoption of healthcare AI technologies.","['Emily LaRosa', 'David Danks']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
"Implications of AI (un-)fairness in higher education admissions: the effects of perceived AI (un-)fairness on exit, voice and organizational reputation","Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.","['Frank Marcinkowski', 'Kimon Kieslich', 'Christopher Starke', 'Marco Lünich']","['University of Düsseldorf, Germany', 'University of Düsseldorf, Germany', 'University of Düsseldorf, Germany', 'University of Düsseldorf, Germany']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Impossible Explanations?: Beyond explainable AI in the GDPR from a COVID-19 use case scenario,"Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI. We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR). Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment. Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.","['Ronan Hamon', 'Henrik Junklewitz', 'Gianclaudio Malgieri', 'Paul De Hert', 'Laurent Beslay', 'Ignacio Sanchez']","['European Commission, Joint Research Centre, Ispra, Italy', 'European Commission, Joint Research Centre, Ispra, Italy', 'Augmented Law Institute, EDHEC Business School, Lille, France', 'Law Science Technology & Society, Vrije Universiteit Brussel, Brussels, Belgium', 'European Commission, Joint Research Centre, Ispra, Italy', 'European Commission, Joint Research Centre, Ispra, Italy']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Improving fairness in machine learning systems: What do industry practitioners need?,"The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams’ challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by industry practitioners and solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address industry practitioners’ needs.","['Ken Holstein', 'Jennifer Wortman Vaughan', 'Hal Daumé III', 'Miro Dudík', 'Hanna Wallach']",Microsoft,2019 ACM CHI Conference on Human Factors in Computing Systems,2019-06-01,TRUE
Improving Simple Models with Confidence Profiles,"In this paper, we propose a new method called ProfWeight for transferring information from a pre-trained deep neural network that has a high test accuracy to a simpler interpretable model or a very shallow network of low complexity and a priori low test accuracy. We are motivated by applications in interpretability and model deployment in severely memory constrained environments (like sensors). Our method uses linear probes to generate confidence scores through flattened intermediate representations. Our transfer method involves a theoretically justified weighting of samples during the training of the simple model using confidence scores of these intermediate layers. The value of our method is first demonstrated on CIFAR-10, where our weighting method significantly improves (3-4%) networks with only a fraction of the number of Resnet blocks of a complex Resnet model. We further demonstrate operationally significant results on a real manufacturing problem, where we dramatically increase the test accuracy of a CART model (the domain standard) by roughly 13%.","A. Dhurandhar, K. Shanmugam, R. Luss and P. Olsen",IBM,NeurIPS (2018),2018,TRUE
Incomplete Contracting and AI Alignment,"We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.","['Dylan Hadfield-Menell', 'Gillian K. Hadfield']","['University of California, Berkeley & Center for Human-Compatible AI, Berkeley, CA, USA', 'University of Toronto & Vector Institute for AI & OpenAI & Center for Human-Compatible AI, Toronto, ON, Canada']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Incorrigibility in the CIRL Framework,"A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. 2015 in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.",['Ryan Carey'],"['Oxford University, Oxford, United Kingdom']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Inferring Work Task Automatability from AI Expert Evidence,"Despite growing alarm about machine learning technologies automating jobs, there is little good evidence on what activities can be automated using such technologies. We contribute the first dataset of its kind by surveying over 150 top academics and industry experts in machine learning, robotics and AI, receiving over 4,500 ratings of how automatable specific tasks are today. We present a probabilistic machine learning model to learn the patterns connecting expert estimates of task automatability and the skills, knowledge and abilities required to perform those tasks. Our model infers the automatability of over 2,000 work activities, and we show how automation differs across types of activities and types of occupations. Sensitivity analysis identifies the specific skills, knowledge and abilities of activities that drive higher or lower automatability. We provide quantitative evidence of what is perceived to be automatable using the state-of-the-art in machine learning technology. We consider the societal impacts of these results and of task-level approaches.","['Paul Duckworth', 'Logan Graham', 'Michael Osborne']","['University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Integrating FATE/critical data studies into data science curricula: where are we going and how do we get there?,"There have been multiple calls for integrating topics related to fairness, accountability, transparency, ethics (FATE) and social justice into Data Science curricula, but little exploration of how this might work in practice. This paper presents the findings of a collaborative auto-ethnography (CAE) engaged in by a MSc Data Science teaching team based at University of Sheffield (UK) Information School where FATE/Critical Data Studies (CDS) topics have been a core part of the curriculum since 2015/16. In this paper, we adopt the CAE approach to reflect on our experiences of working at the intersection of disciplines, and our progress and future plans for integrating FATE/CDS into the curriculum. We identify a series of challenges for deeper FATE/CDS integration related to our own competencies and the wider socio-material context of Higher Education in the UK. We conclude with recommendations for ourselves and the wider FATE/CDS orientated Data Science community.","['Jo Bates', 'David Cameron', 'Alessandro Checco', 'Paul Clough', 'Frank Hopfgartner', 'Suvodeep Mazumdar', 'Laura Sbaffi', 'Peter Stordy', 'Antonio de la Vega de León']","['University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK and Peak Indicators, Chesterfield, UK', 'University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV),"The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of âzebraâ is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.","['Been Kim', 'Martin Wattenberg', 'Carrie Jun Cai', 'James Wexler', 'Fernanda Viegas', 'Rory Abbott Sayres']",Google,ICML (2018),2018,TRUE
Interpretable Active Learning,"Active learning has long been a topic of study in machine learning. However, as increasingly complex and opaque models have become standard practice, the process of active learning, too, has become more opaque. There has been little investigation into interpreting what specific trends and patterns an active learning strategy may be exploring. This work expands on the Local Interpretable Model-agnostic Explanations framework (LIME) to provide explanations for active learning recommendations. We demonstrate how LIME can be used to generate locally faithful explanations for an active learning strategy, and how these explanations can be used to understand how different models and datasets explore a problem space over time. These explanations can also be used to generate batches based on common sources of uncertainty. These regions of common uncertainty can be useful for understanding a model's current weaknesses. In order to quantify the per-subgroup differences in how an active learning strategy queries spatial regions, we introduce a notion of uncertainty bias (based on disparate impact) to measure the discrepancy in the confidence for a model's predictions between one subgroup and another. Using the uncertainty bias measure, we show that our query explanations accurately reflect the subgroup focus of the active learning queries, allowing for an interpretable explanation of what is being learned as points with similar sources of uncertainty have their uncertainty bias resolved. We demonstrate that this technique can be applied to track uncertainty bias over user-defined clusters or automatically generated clusters based on the source of uncertainty. We also measure how the choice of initial labeled examples effects groups over time. ","Richard Phillips, Kyu Hyun Chang, Sorelle A. Friedler","[""Haverford College"", ""Google Inc""]",FAT* 2018,2018,TRUE
Interpretable Approaches to Detect Bias in Black-Box Models,"My dissertation research is grounded in the field of interpretability. I aim to develop methods to explain and interpret predictions from black-box machine learning models to help creators, as well as users, of machine learning models increase their trust and understanding of the models. In this doctoral consortium paper, I summarize my previous and current research projects in interpretability, and describe my future plans for research in this area.",['Sarah Tan'],"['Cornell University, Ithaca, NY, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Interpreting Social Respect:  A Normative Lens for ML Models,"Machine learning is often viewed as an inherently value-neutral process:
statistical tendencies in the training inputs are simply''
used to generalize to new examples. However when models impact social
systems such as interactions between humans, these patterns learned by models
have normative implications. It is important that we ask not onlywhat
patterns exist in the data?'', but also ``how do we want our system 
to impact people?'' In particular, because minority and marginalized
members of society are often statistically underrepresented in data sets, models
may have undesirable disparate impact on such groups. As such, objectives of
social equity and distributive justice require that we develop tools for both
identifying and interpreting harms introduced by models.",['Ben Hutchinson'],Google,Google (2019),2019,TRUE
Interventions for ranking in the presence of implicit bias,"Implicit bias is the unconscious attribution of particular qualities (or lack thereof) to a member from a particular social group (e.g., defined by gender or race). Studies on implicit bias have shown that these unconscious stereotypes can have adverse outcomes in various social contexts, such as job screening, teaching, or policing. Recently, [34] considered a mathematical model for implicit bias and showed the effectiveness of the Rooney Rule as a constraint to improve the utility of the outcome for certain cases of the subset selection problem. Here we study the problem of designing interventions for the generalization of subset selection - ranking - that requires to output an ordered set and is a central primitive in various social and computational contexts. We present a family of simple and interpretable constraints and show that they can optimally mitigate implicit bias for a generalization of the model studied in [34]. Subsequently, we prove that under natural distributional assumptions on the utilities of items, simple, Rooney Rule-like, constraints can also surprisingly recover almost all the utility lost due to implicit biases. Finally, we augment our theoretical results with empirical findings on real-world distributions from the IIT-JEE (2009) dataset and the Semantic Scholar Research corpus.","['L. Elisa Celis', 'Anay Mehrotra', 'Nisheeth K. Vishnoi']","['Yale University', 'IIT Kanpur', 'Yale University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Interventions over Predictions: Reframing the Ethical Debate for Actuarial Risk Assessment,"Actuarial risk assessments are frequently touted as a neutral way to counteract implicit bias and increase the fairness of decisions made at almost every juncture of the criminal justice system, from pretrial release to sentencing, parole and probation. In recent times these assessments have come under increased scrutiny, as critics claim that the statistical techniques underlying them might reproduce existing patterns of discrimination and historical biases that are reflected in the data. Much of this debate is centered around competing notions of fairness and predictive accuracy, which seek to problematize the use of variables that act as ""proxies"" for protected classes, such as race and gender. However, these debates fail to address the core ethical issue at hand - that current risk assessments are ill-equipped to support ethical punishment and rehabilitation practices in the criminal justice system, because they offer only a limited insight into the underlying drivers of criminal behavior. In this paper, we examine the prevailing paradigms of fairness currently under debate and propose an alternative methodology for identifying the underlying social and structural factors that drive criminal behavior. We argue that the core ethical debate surrounding the use of regression in risk assessments is not one of bias or accuracy. Rather, it's one of purpose. If machine learning is operationalized merely in the service of predicting future crime, then it becomes difficult to break cycles of criminalization that are driven by the iatrogenic effects of the criminal justice system itself. We posit that machine learning should not be used for prediction, rather it should be used to surface covariates that are fed into a causal model for understanding the social, structural and psychological drivers of crime. We propose an alternative application of machine learning and causal inference away from predicting risk scores to risk mitigation. ","Chelsea Barabas, Madars Virza, Karthik Dinakar, Joichi Ito, Jonathan Zittrain","[""MIT Media Lab"", ""Harvard""]",FAT* 2018,2018,FALSE
Intriguing Properties of Adversarial Examples,"It is becoming increasingly clear that many machine learning classifiers are vulnerable
to adversarial examples. In attempting to explain the origin of adversarial
examples, previous studies have typically focused on the fact that neural networks
operate on high dimensional data, they overfit, or they are too linear. Here we
argue that the origin of adversarial examples is primarily due to an inherent uncertainty
that neural networks have about their predictions. We show that the functional
form of this uncertainty is independent of architecture, dataset, and training
protocol; and depends only on the statistics of the logit differences of the network,
which do not change significantly during training. This leads to adversarial error
having a universal scaling, as a power-law, with respect to the size of the adversarial
perturbation. We show that this universality holds for a broad range of datasets
(MNIST, CIFAR10, ImageNet, and random data), models (including state-of-theart
deep networks, linear models, adversarially trained networks, and networks
trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated
by these results, we study the effects of reducing prediction entropy on
adversarial robustness. Finally, we study the effect of network architectures on
adversarial sensitivity. To do this, we use neural architecture search with reinforcement
learning to find adversarially robust architectures on CIFAR10. Our
resulting architecture is more robust to white and black box attacks compared to
previous attempts.","['Ekin Dogus Cubuk', 'Quoc V. Le']",Google,ICLR (2018),2018,TRUE
Inverse Norm Conflict Resolution,"In previous work we provided a ""norm conflict resolution"" algorithm allowing agents in stochastic domains (represented by Markov Decision Processes) to ""maximally satisfy"" a set of moral or social norms, where such norms are represented by statements in linear temporal logic (LTL). This required the agent designer to provide weights specifying the relative importance of each norm. In this paper, we propose an ""inverse norm conflict resolution'' algorithm for learning these weights from demonstration. This approach minimizes a cost function based on the relative entropy between a policy encoding the observed behavior and a policy representing optimal norm-following behavior. We demonstrate the effectiveness of the algorithm in a simple GridWorld domain.","['Daniel Kasenberg', 'Matthias Scheutz']","['Tufts University, Medford, MA, USA', 'Tufts University, Medford, MA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Investigating the Effects of Gender Bias on GitHub,"Diversity, including gender diversity, is valued by many software development organizations, yet the field remains dominated by men. One reason for this lack of diversity is gender bias. In this paper, we study the effects of that bias by using an existing framework derived from the gender studies literature. We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub, then evaluate those hypotheses quantitatively. While our results show that effects of gender bias are largely invisible on the GitHub platform itself, there are still signals of women concentrating their work in fewer places and being more restrained in communication than men.",['Emerson Murphy-Hill'],Google,Proceedings of the 2019 International Conference on Software Engineering,2019,TRUE
Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification,"Modern face recognition systems leverage datasets containing images of hundreds of thousands of specific individuals' faces to train deep convolutional neural networks to learn an embedding space that maps an arbitrary individual's face to a vector representation of their identity. The performance of a face recognition system in face verification (1:1) and face identification (1:N) tasks is directly related to the ability of an embedding space to discriminate between identities. Recently, there has been significant public scrutiny into the source and privacy implications of large-scale face recognition training datasets such as MS-Celeb-1M and MegaFace, as many people are uncomfortable with their face being used to train dual-use technologies that can enable mass surveillance. However, the impact of an individual's inclusion in training data on a derived system's ability to recognize them has not previously been studied. In this work, we audit ArcFace, a state-of-the-art, open source face recognition system, in a large-scale face identification experiment with more than one million distractor images. We find a Rank-1 face identification accuracy of 79.71% for individuals present in the model's training data and an accuracy of 75.73% for those not present. This modest difference in accuracy demonstrates that face recognition systems using deep learning work better for individuals they are trained on, which has serious privacy implications when one considers all major open source face recognition training datasets do not obtain informed consent from individuals during their collection.","['Chris Dulhanty', 'Alexander Wong']","['University of Waterloo, Waterloo, ON, Canada', 'University of Waterloo, Waterloo, ON, Canada']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Invisible Influence: Artificial Intelligence and the Ethics of Adaptive Choice Architectures,"For several years, scholars have (for good reason) been largely preoccupied with worries about the use of artificial intelligence and machine learning (AI/ML) tools to make decisions about us. Only recently has significant attention turned to a potentially more alarming problem: the use of AI/ML to influence our decision-making. The contexts in which we make decisions--what behavioral economists call our choice architectures--are increasingly technologically-laden. Which is to say: algorithms increasingly determine, in a wide variety of contexts, both the sets of options we choose from and the way those options are framed. Moreover, artificial intelligence and machine learning (AI/ML) makes it possible for those options and their framings--the choice architectures--to be tailored to the individual chooser. They are constructed based on information collected about our individual preferences, interests, aspirations, and vulnerabilities, with the goal of influencing our decisions. At the same time, because we are habituated to these technologies we pay them little notice. They are, as philosophers of technology put it, transparent to us--effectively invisible. I argue that this invisible layer of technological mediation, which structures and influences our decision-making, renders us deeply susceptible to manipulation. Absent a guarantee that these technologies are not being used to manipulate and exploit, individuals will have little reason to trust them.",['Daniel Susser'],"['Penn State University, State College, PA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Is There a Trade-Off Between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing,"A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that may reflect bias and inequity, and instead, we should be considering accuracy with respect to ideal, unbiased data.","Sanghamitra Dutta, Dennis Wei, Hazar Yueksel, Pin-Yu Chen, Sijia Liu, Kush Varshney ","[""Carnegie Mellon"", ""IBM Research""]",ICML 2020,2020-07,TRUE
Jill Watson Doesn't Care if You're Pregnant: Grounding AI Ethics in Empirical Studies,"Jill Watson is our name for a virtual teaching assistant for a Georgia Tech course on artificial intelligence: Jill answers routine, frequently asked questions on the class discussion forum. In this paper, we outline some of the ethical issues that arose in the development and deployment of the virtual teaching assistant. We posit that experiments such as Jill Watson are critical for deeply understanding AI ethics.","['Bobbie Eicher', 'Lalith Polepeddi', 'Ashok Goel']","['Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Joint Optimization of AI Fairness and Utility: A Human-Centered Approach,"Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.","['Yunfeng Zhang', 'Rachel Bellamy', 'Kush Varshney']","['IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA', 'IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA', 'IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
Justice Beyond Utility in Artificial Intelligence,"The entry of Artificial Intelligence into prominent social and economic environments has brought to the fore concerns about the ethical nature of such agents and tools. Though it is well-known that methods based in utilitarian calculus often fail to account for moral considerations, few alternatives have been adopted in the field of AI. My work advocates for a new approach toward the interaction between AI methods and the social that centers principles of distributive justice. As AI increasingly drives consequential social decision-making, we must consider not only what can be done but what ought to be done. Grappling with the inherently normative nature of these problems requires an orientation towards AI that is able to conceptualize justice beyond utility.",['Lily Hu'],"['Harvard University, Cambridge, MA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Killer Robots and Human Dignity,"Lethal Autonomous Weapon Systems (LAWS) have become the center of an internationally relevant ethical debate. Deontological arguments based on putative legal compliance failures and the creation of accountability gaps along with wide consequentialist arguments based on factors like the ease of engaging in wars have been leveraged by a number of different states and organizations to try and reach global consensus on a ban of LAWS. This paper will focus on one strand of deontological arguments-ones based on human dignity. Merely asserting that LAWS pose a threat to human dignity would be question begging. Independent evidence based on a morally relevant distinction between humans and LAWS is needed. There are at least four reasons to think that the capacity for emotion cannot be a morally relevant distinction. First, if the concept of human dignity is given a subjective definition, whether or not lethal force is administered by humans or LAWS seems to be irrelevant. Second, it is far from clear that human combatants either have the relevant capacity for emotion or that the capacity is exercised in the relevant circumstances. Third, the capacity for emotion can actually be an impediment to the exercising of a combatant's ability to treat an enemy respectfully. Fourth, there is strong inductive evidence to believe that any capacity, when sufficiently well described, can be carried out by artificially intelligent programs.",['Daniel Lim'],"['Duke Kunshan University, Kunshan, China']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Language (Technology) is Power: A Critical Survey of “Bias” in NLP,"We survey 146 papers analyzing “bias” in NLP systems, 
finding that their motivations are often vague, inconsistent, and 
lacking in normative reasoning, despite the fact that analyzing “bias” 
is an inherently normative process. We further find that these papers’ 
proposed quantitative techniques for measuring or mitigating “bias” are 
poorly matched to their motivations and do not engage with the relevant 
literature outside of NLP. Based on these findings, we describe the 
beginnings of a path forward by proposing three recommendations that 
should guide work analyzing “bias” in NLP systems. These recommendations
 rest on a greater recognition of the relationships between language and
 social hierarchies, encouraging researchers and practitioners to 
articulate their conceptualizations of “bias”—i.e., what kinds of system
 behaviors are harmful, in what ways, to whom, and why, as well as the 
normative reasoning underlying these statements—and to center work 
around the lived experiences of members of communities affected by NLP 
systems, while interrogating and reimagining the power relations between
 technologists and such communities.","['Su Lin Blodgett', 'Solon Barocas', 'Hal Daumé III', 'Hanna Wallach']",Microsoft,ACL,2020-06-01,TRUE
Learning and Obeying Conflicting Norms in Stochastic Domains,No abstract available.,['Daniel Kasenberg'],"['Tufts University, Medford, MA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Learning Differentially Private Recurrent Language Models,"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes ""large step"" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","['Brendan McMahan', 'Daniel Ramage', 'Li Zhang']",Google,International Conference on Learning Representations (ICLR) (2018),2018,TRUE
Learning Existing Social Conventions via Observationally Augmented Self-Play,"In order for artificial agents to coordinate effectively with people, they must act consistently with existing conventions (e.g. how to navigate in traffic, which language to speak, or how to coordinate with teammates). A group's conventions can be viewed as a choice of equilibrium in a coordination game. We consider the problem of an agent learning a policy for a coordination game in a simulated environment and then using this policy when it enters an existing group. When there are multiple possible conventions we show that learning a policy via multi-agent reinforcement learning (MARL) is likely to find policies which achieve high payoffs at training time but fail to coordinate with the real group into which the agent enters. We assume access to a small number of samples of behavior from the true convention and show that we can augment the MARL objective to help it find policies consistent with the real group's convention. In three environments from the literature - traffic, communication, and team coordination - we observe that augmenting MARL with a small amount of imitation learning greatly increases the probability that the strategy found by MARL fits well with the existing social convention. We show that this works even in an environment where standard training methods very rarely find the true convention of the agent's partners.","['Adam Lerer', 'Alexander Peysakhovich']","['Facebook AI Research, New York, NY, USA', 'Facebook AI Research, New York, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Learning how to explain neural networks: PatternNet and PatternAttribution,"DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of  simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.","['Pieter-jan Kindermans', 'Dumitru Erhan', 'Been Kim']",Google,ICLR (2018),2018,TRUE
Learning Norms from Stories: A Prior for Value Aligned Agents,"Value alignment is a property of an intelligent agent indicating that it can only pursue goals and activities that are beneficial to humans. Traditional approaches to value alignment use imitation learning or preference learning to infer the values of humans by observing their behavior. We introduce a complementary technique in which a value-aligned prior is learned from naturally occurring stories which encode societal norms. Training data is sourced from the children's educational comic strip, Goofus & Gallant. In this work, we train multiple machine learning models to classify natural language descriptions of situations found in the comic strip as normative or non-normative by identifying if they align with the main characters' behavior. We also report the models' performance when transferring to two unrelated tasks with little to no additional training on the new task.","['Md Sultan Al Nahian', 'Spencer Frazier', 'Mark Riedl', 'Brent Harrison']","['University of Kentucky, Lexington, KY, USA', 'Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA', 'University of Kentucky, Lexington, KY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Learning Occupational Task-Shares Dynamics for the Future of Work,"The recent wave of AI and automation has been argued to differ from previous General Purpose Technologies (GPTs), in that it may lead to rapid change in occupations' underlying task requirements and persistent technological unemployment. In this paper, we apply a novel methodology of dynamic task shares to a large dataset of online job postings to explore how exactly occupational task demands have changed over the past decade of AI innovation, especially across high, mid and low wage occupations. Notably, big data and AI have risen significantly among high wage occupations since 2012 and 2016, respectively. We built an ARIMA model to predict future occupational task demands and showcase several relevant examples in Healthcare, Administration, and IT. Such task demands predictions across occupations will play a pivotal role in retraining the workforce of the future.","['Subhro Das', 'Sebastian Steffen', 'Wyatt Clarke', 'Prabhat Reddy', 'Erik Brynjolfsson', 'Martin Fleming']","['IBM Research, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'IBM Research, Armonk, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'IBM Research, Armonk, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
Learning Optimal Fair Policies,"Systematic discriminatory biases present in our society influence the way data is collected and stored, the way variables are defined, and the way scientific findings are put into practice as policy. Automated decision procedures and learning algorithms applied to such data may serve to perpetuate existing injustice or unfairness in our society. In this paper, we consider how to make optimal but fair decisions, which ""break the cycle of injustice"" by correcting for the unfair dependence of both decisions and outcomes on sensitive features (e.g., variables that correspond to gender, race, disability, or other protected attributes). We use methods from causal inference and constrained optimization to learn optimal policies in a way that addresses multiple potential biases which afflict data analysis in sensitive contexts, extending the approach of (Nabi and Shpitser 2018). Our proposal comes equipped with the theoretical guarantee that the chosen fair policy will induce a joint distribution for new instances that satisfies given fairness constraints. We illustrate our approach with both synthetic data and real criminal justice data. ","Razieh Nabi, Daniel Malinsky, Ilya Shpitser",John Hopkins University,ICML 2019,2019-06,FALSE
Learning to Attack: Adversarial Transformation Networks,"With the rapidly increasing popularity of deep neural networks
for image recognition tasks, a parallel interest in generating
adversarial examples to attack the trained models has
arisen. To date, these approaches have involved either directly
computing gradients with respect to the image pixels or directly
solving an optimization on the image pixels. We generalize
this pursuit in a novel direction: can a separate network
be trained to efficiently attack another fully trained network?
We demonstrate that it is possible, and that the generated
attacks yield startling insights into the weaknesses of
the target network. We call such a network an Adversarial
Transformation Network (ATN). ATNs transform any input
into an adversarial attack on the target network, while being
minimally perturbing to the original inputs and the target networkâs
outputs. Further, we show that ATNs are capable of
not only causing the target network to make an error, but can
be constructed to explicitly control the type of misclassification
made. We demonstrate ATNs on both simple MNIST digit
classifiers and state-of-the-art ImageNet classifiers deployed
by Google, Inc.: Inception ResNet-v2.","['Shumeet Baluja', 'Ian Fischer']",Google,Proceedings of AAAI-2018,2018,TRUE
Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning,"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states.","['Shane Gu', 'Julian Ibarz', 'Sergey Levine']",Google,ICLR (2018),2018,TRUE
Leave-one-out Unfairness,"We introduce leave-one-out unfairness, which characterizes how likely a model's prediction for an individual will change due to the inclusion or removal of a single other person in the model's training data. Leave-one-out unfairness appeals to the idea that fair decisions are not arbitrary: they should not be based on the chance event of any one person's inclusion in the training data. Leave-one-out unfairness is closely related to algorithmic stability, but it focuses on the consistency of an individual point's prediction outcome over unit changes to the training data, rather than the error of the model in aggregate. Beyond formalizing leave-one-out unfairness, we characterize the extent to which deep models behave leave-one-out unfairly on real data, including in cases where the generalization error is small. Further, we demonstrate that adversarial training and randomized smoothing techniques have opposite effects on leave-one-out fairness, which sheds light on the relationships between robustness, memorization, individual fairness, and leave-one-out fairness in deep models. Finally, we discuss salient practical applications that may be negatively affected by leave-one-out unfairness.","['Emily Black', 'Matt Fredrikson']","['Carnegie Mellon University', 'Carnegie Mellon University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
"Legal, Ethical, Customizable Artificial Intelligence","To be effective, useful, safe, and legal, AI must obey the laws of its users' societies and (where legal) its users' ethical intuitions. But laws and ethics can be difficult for people to express. My research involves ethical and legal instruction by example: synthesizing cases, applying synthesized principles, and explaining those applications.",['Joseph A. Blass'],"['Northwestern University, Chicago, IL, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Legible Normativity for AI Alignment: The Value of Silly Rules,"It has become commonplace to assert that autonomous agents will have to be built to follow human rules of behavior--social norms and laws. But human laws and norms are complex and culturally varied systems; in many cases agents will have to learn the rules. This requires autonomous agents to have models of how human rule systems work so that they can make reliable predictions about rules. In this paper we contribute to the building of such models by analyzing an overlooked distinction between important rules and what we call silly rules -- rules with no discernible direct impact on welfare. We show that silly rules render a normative system both more robust and more adaptable in response to shocks to perceived stability. They make normativity more legible for humans, and can increase legibility for AI systems as well. For AI systems to integrate into human normative systems, we suggest, it may be important for them to have models that include representations of silly rules.","['Dylan Hadfield-Menell', 'Mckane Andrus', 'Gillian Hadfield']","['University of California, Berkeley & Center for Human-Compatible AI, Berkeley, CA, USA', 'University of California, Berkeley & Center for Human-Compatible AI, Berkeley, CA, USA', 'University of Toronto & Vector Institute for AI & OpenAI & Center for Human-Compatible AI, Toronto, ON, Canada']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Lessons from archives: strategies for collecting sociocultural data in machine learning,"A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics & privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.","['Eun Seo Jo', 'Timnit Gebru']","['Stanford University', 'Google']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy,"Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID-19 response strategies such as the targeting of public health resources. Yet little attention has been paid to measurement validity and demographic bias, due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics. We illustrate how linking large-scale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels. More precisely, we show that linking voter roll data---containing individual-level voter turnout for specific voting locations along with race and age---can facilitate the construction of rigorous bias and reliability tests. Using data from North Carolina's 2018 general election, these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context: older and non-white voters are less likely to be captured by mobility data. We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups.","['Amanda Coston', 'Neel Guha', 'Derek Ouyang', 'Lisa Lu', 'Alexandra Chouldechova', 'Daniel E. Ho']","['Carnegie Mellon University', 'Stanford University', 'Stanford University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Loss-Aversively Fair Classification,"The use of algorithmic (learning-based) decision making in scenarios that affect human lives has motivated a number of recent studies to investigate such decision making systems for potential unfairness, such as discrimination against subjects based on their sensitive features like gender or race. However, when judging the fairness of a newly designed decision making system, these studies have overlooked an important influence on people's perceptions of fairness, which is how the new algorithm changes the status quo, i.e., decisions of the existing decision making system. Motivated by extensive literature in behavioral economics and behavioral psychology (prospect theory), we propose a notion of fair updates that we refer to as loss-averse updates. Loss-averse updates constrain the updates to yield improved (more beneficial) outcomes to subjects compared to the status quo. We propose tractable proxy measures that would allow this notion to be incorporated in the training of a variety of linear and non-linear classifiers. We show how our proxy measures can be combined with existing measures for training nondiscriminatory classifiers.Our evaluation using synthetic and real-world datasets demonstrates that the proposed proxy measures are effective for their desired tasks.","['Junaid Ali', 'Muhammad Bilal Zafar', 'Adish Singla', 'Krishna P. Gummadi']","['Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Making Decisions that Reduce Discriminatory Impacts,"As machine learning algorithms move into real-world settings, it is crucial to ensure they are aligned with societal values. There has been much work on one aspect of this, namely unfairness in the prediction problem: How can we reduce discrimination in the predictions themselves? While an important question, solutions to this problem only apply in a restricted setting because we have full control over the predictions. Often we care about the non-discrimination of quantities we do not have full control over. Thus, we describe another key aspect of this challenge, the impact problem: How can we reduce discrimination arising from the real-world impact of decisions? To address this, we describe causal methods that model the relevant parts of the real-world system in which the decisions are made. Unlike previous approaches, these models not only allow us to map the causal pathway of a single decision, but also to model the effect of interference--how the impact on an individual depends on decisions made about other people. Often, the goal of decision policies is to maximize a beneficial impact overall. To reduce the discrimination of these benefits, we devise a constraint inspired by recent work in counterfactual fairness, and give an efficient procedure to solve the constrained optimization problem. We demonstrate our approach with an example: how to increase students taking college entrance exams in New York City public schools.","Matt Kusner, Chris Russell, Joshua Loftus, Ricardo Silva","[""Alan Turing Institute"", ""University of Oxford"", ""University of Surrey"", ""New York University"", ""University College London""]",ICML 2019,2019-06,FALSE
Manipulating and Measuring Model Interpretability,"Despite a growing literature on creating interpretable machine learning methods, there have been few experimental studies of their effects on end users. We present a series of large-scale, randomized, pre-registered experiments in which participants were shown functionally identical models that varied only in two factors thought to influence interpretability: the number of input features and the model transparency (clear or black-box). Participants who were shown a clear model with a small number of features were better able to simulate the model’s predictions. However, contrary to what one might expect when manipulating interpretability, we found no significant difference in multiple measures of trust across conditions. Even more surprisingly, increased transparency hampered people’s ability to detect when a model has made a sizeable mistake. These findings emphasize the importance of studying how models are presented to people and empirically verifying that interpretable models achieve their intended effects on end users.","['Forough Poursabzi-Sangdeh', 'Dan Goldstein', 'Jake Hofman', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,CHI 2021,2021-05-20,TRUE
Mapping Informal Settlements in Developing Countries using Machine Learning and Low Resolution Multi-spectral Data,"Informal settlements are home to the most socially and economically vulnerable people on the planet. In order to deliver effective economic and social aid, non-government organizations (NGOs), such as the United Nations Children's Fund (UNICEF), require detailed maps of the locations of informal settlements. However, data regarding informal and formal settlements is primarily unavailable and if available is often incomplete. This is due, in part, to the cost and complexity of gathering data on a large scale. To address these challenges, we, in this work, provide three contributions. 1) A brand new machine learning dataset purposely developed for informal settlement detection. 2) We show that it is possible to detect informal settlements using freely available low-resolution (LR) data, in contrast to previous studies that use very-high resolution~(VHR) satellite and aerial imagery, something that is cost-prohibitive for NGOs. 3) We demonstrate two effective classification schemes on our curated data set, one that is cost-efficient for NGOs and another that is cost-prohibitive for NGOs, but has additional utility. We integrate these schemes into a semi-automated pipeline that converts either a LR or VHR satellite image into a binary map that encodes the locations of informal settlements.","['Bradley J. Gram-Hansen', 'Patrick Helber', 'Indhu Varatharajan', 'Faiza Azam', 'Alejandro Coca-Castro', 'Veronika Kopackova', 'Piotr Bilinski']","['University of Oxford, Oxford, United Kingdom', 'DFKI, TU Kaiserslautern, Kaiserslautern, Germany', 'DLR Institute for Planetary, Berlin, Germany', 'Independent Researcher, Bremen, Germany', 'Kings College London, London, United Kingdom', 'Czech Geological Survey, Prague, Czech Rep', 'University of Oxford & University of Warsaw, Oxford, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Mapping Missing Population in Rural India: A Deep Learning Approach with Satellite Imagery,"Millions of people worldwide are absent from their country's census. Accurate, current, and granular population metrics are critical to improving government allocation of resources, to measuring disease control, to responding to natural disasters, and to studying any aspect of human life in these communities. Satellite imagery can provide sufficient information to build a population map without the cost and time of a government census. We present two Convolutional Neural Network (CNN) architectures which efficiently and effectively combine satellite imagery inputs from multiple sources to accurately predict the population density of a region. In this paper, we use satellite imagery from rural villages in India and population labels from the 2011 SECC census. Our best model achieves better performance than previous papers as well as LandScan, a community standard for global population distribution.","['Wenjie Hu', 'Jay Harshadbhai Patel', 'Zoe-Alanah Robert', 'Paul Novosad', 'Samuel Asher', 'Zhongyi Tang', 'Marshall Burke', 'David Lobell', 'Stefano Ermon']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Dartmouth College, Hanover, NH, USA', 'World Bank, Washington, DC, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Margins and Opportunity,"We use the statistical quantity of margin --- the distance between a decision boundary and a classified point, or the gap between two scores --- to formalize the principle of equal opportunity --- the chance to improve one's outcome, regardless of group status. This leads to a better definition of opportunity which recognizes, for example, that a strongly rejected individual was offered less recourse than a weakly rejected one, despite the shared outcome. It also leads to simpler algorithms, since real-valued margins are easier to analyze and optimize than discrete outcomes. We formalize two ways that a protected group may be guaranteed equal opportunity: (1) (social) mobility: acceptance should be within reach for the group (conversely, the general population shouldn't be cushioned from rejection), and (2) contrast: within the group, good candidates should get substantially higher scores than bad candidates, preventing the so-called 'token' effect. A simple linear classifier seems to offer roughly equal opportunity both experimentally and mathematically.",['Shiva Kaul'],"['Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
"Matroids, Matchings, and Fairness","The desire to use machine learning to assist in human decision making has spawned a large area of research in understanding the impact of such systems not only on the society as a whole, but also the specific impact on different subpopulations. Recent work has shown that  while there are several natural ways to quantify the fairness of a particular system, none of them are universal, and except for trivial cases, satisfying one means violating another~\citet&lbrace;Kleinberg, Goel, Kleinberg2&rbrace;.  ","['Ravi Kumar', 'Silvio Lattanzi', 'Sergei Vassilvitskii']",Google,AISTATS 2019 (2019),2019,TRUE
Measurement and Fairness,"We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.","['Abigail Z. Jacobs', 'Hanna Wallach']","['University of Michigan', 'Microsoft Research']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Measuring and Mitigating Unintended Bias in Text Classification,"We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.","['Lucas Dixon', 'John Li', 'Jeffrey Sorensen', 'Nithum Thain', 'Lucy Vasserman']","['Google, New York, NY, USA', 'Google, New York, NY, USA', 'Google, New York, NY, USA', 'Google, New York, NY, USA', 'Google, New York, NY, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,TRUE
Measuring Fairness in an Unfair World,"Computer scientists have made great strides in characterizing different measures of algorithmic fairness, and showing that certain measures of fairness cannot be jointly satisfied. In this paper, I argue that the three most popular families of measures - unconditional independence, target-conditional independence and classification-conditional independence - make assumptions that are unsustainable in the context of an unjust world. I begin by introducing the measures and the implicit idealizations they make about the underlying causal structure of the contexts in which they are deployed. I then discuss how these idealizations fall apart in the context of historical injustice, ongoing unmodeled oppression, and the permissibility of using sensitive attributes to rectify injustice. In the final section, I suggest an alternative framework for measuring fairness in the context of existing injustice: distributive fairness.",['Jonathan Herington'],"['University of Rochester, Rochester, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Measuring justice in machine learning,"How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?",['Alan Lundgard'],"['Massachusetts Institute of Technology Cambridge, Massachusetts']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics,"Bias in machine learning has manifested injustice in several areas, such as medicine, hiring, and criminal justice. In response, computer scientists have developed myriad definitions of fairness to correct this bias in fielded algorithms. While some definitions are based on established legal and ethical norms, others are largely mathematical. It is unclear whether the general public agrees with these fairness definitions, and perhaps more importantly, whether they understand these definitions. We take initial steps toward bridging this gap between ML researchers and the public, by addressing the question: does a lay audience understand a basic definition of ML fairness? We develop a metric to measure comprehension of three such definitions--demographic parity, equal opportunity, and equalized odds. We evaluate this metric using an online survey, and investigate the relationship between comprehension and sentiment, demographics, and the definition itself.","Debjani Saha, Candice Schumann, Duncan McElfresh, John Dickerson, Michelle Mazurek, Michael Tschantz ",University of Maryland,ICML 2020,2020-07,FALSE
Measuring the Biases that Matter: The Ethical and Casual Foundations for Measures of Fairness in Algorithms,"Measures of algorithmic bias can be roughly classified into four categories, distinguished by the conditional probabilistic dependencies to which they are sensitive. First, measures of ""procedural bias"" diagnose bias when the score returned by an algorithm is probabilistically dependent on a sensitive class variable (e.g. race or sex). Second, measures of ""outcome bias"" capture probabilistic dependence between class variables and the outcome for each subject (e.g. parole granted or loan denied). Third, measures of ""behavior-relative error bias"" capture probabilistic dependence between class variables and the algorithmic score, conditional on target behaviors (e.g. recidivism or loan default). Fourth, measures of ""score-relative error bias"" capture probabilistic dependence between class variables and behavior, conditional on score. Several recent discussions have demonstrated a tradeoff between these different measures of algorithmic bias, and at least one recent paper has suggested conditions under which tradeoffs may be minimized. In this paper we use the machinery of causal graphical models to show that, under standard assumptions, the underlying causal relations among variables forces some tradeoffs. We delineate a number of normative considerations that are encoded in different measures of bias, with reference to the philosophical literature on the wrongfulness of disparate treatment and disparate impact. While both kinds of error bias are nominally motivated by concern to avoid disparate impact, we argue that consideration of causal structures shows that these measures are better understood as complicated and unreliable measures of procedural biases (i.e. disparate treatment). Moreover, while procedural bias is indicative of disparate treatment, we show that the measure of procedural bias one ought to adopt is dependent on the account of the wrongfulness of disparate treatment one endorses. Finally, given that neither score-relative nor behavior-relative measures of error bias capture the relevant normative considerations, we suggest that error bias proper is best measured by score-based measures of accuracy, such as the Brier score.","['Bruce Glymour', 'Jonathan Herington']","['Department of Philosophy, Kansas State University, Manhattan, KS, USA', 'Department of Philosophy, Kansas State University, Manhattan, KS, USA']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Meritocratic Fairness for Infinite and Contextual Bandits,"We study fairness in linear bandit problems. Starting from the notion of meritocratic fairness introduced in~\citeJKMR16, we carry out a more refined analysis of a more general problem, achieving better performance guarantees with fewer modelling assumptions on the number and structure of available choices as well as the number selected. We also analyze the previously-unstudied question of fairness in infinite linear bandit problems, obtaining instance-dependent regret upper bounds as well as lower bounds demonstrating that this instance-dependence is necessary. The result is a framework for meritocratic fairness in an online linear setting that is substantially more powerful, general, and realistic than the current state of the art.","['Matthew Joseph', 'Michael Kearns', 'Jamie Morgenstern', 'Seth Neel', 'Aaron Roth']","['University of Pennsylvania, Philadelphia, PA, USA', 'University of Pennsylvania, Philadelphia, PA, USA', 'University of Pennsylvania, Philadelphia, PA, USA', 'University of Pennsylvania, Philadelphia, PA, USA', 'University of Pennsylvania, Philadelphia, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Meta Decision Trees for Explainable Recommendation Systems,"We tackle the problem of building explainable recommendation systems that are based on a per-user decision tree, with decision rules that are based on single attribute values. We build the trees by applying learned regression functions to obtain the decision rules as well as the values at the leaf nodes. The regression functions receive as input the embedding of the user's training set, as well as the embedding of the samples that arrive at the current node. The embedding and the regressors are learned end-to-end with a loss that encourages the decision rules to be sparse. By applying our method, we obtain a collaborative filtering solution that provides a direct explanation to every rating it provides. With regards to accuracy, it is competitive with other algorithms. However, as expected, explainability comes at a cost and the accuracy is typically slightly lower than the state of the art result reported in the literature. Our code is available at \urlhttps://github.com/shulmaneyal/metatrees.","['Eyal Shulman', 'Lior Wolf']","['Tel Aviv University, Tel Aviv-Yafo, Israel', 'Tel Aviv University, Tel Aviv-Yafo, Israel']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Metric-Optimized Example Weights,"Real-world machine learning applications often have complex test metrics, and may have training and test data that are not identically distributed. Motivated by known connections between complex test metrics and cost-weighted learning, we propose addressing these issues by using a weighted loss function with a standard loss, where the weights on the training examples are learned to optimize the test metric on a validation set. These metric-optimized example weights can be learned for any test metric, including black box and customized ones for specific applications. We illustrate the performance of the proposed method on diverse public benchmark datasets and real-world applications. We also provide a generalization bound for the method.","['Sen Zhao', 'Harikrishna Narasimhan']",Google,Proceedings of the 36th International Conference on Machine Learning (2019),2019,TRUE
MINA: Multilevel Knowledge-Guided Attention for Modeling Electrocardiography Signals,"Electrocardiography (ECG) signals are commonly used to diagnose various cardiac abnormalities. Recently, deep learning models showed initial success on modeling ECG data, however they are mostly black-box, thus lack interpretability needed for clinical usage. In this work, we propose MultIlevel kNowledge-guided Attention networks (MINA) that predict heart diseases from ECG signals with intuitive explanation aligned with medical knowledge. By extracting multilevel (beat-, rhythm- and frequency-level) domain knowledge features separately, MINA combines the medical knowledge and ECG data via a multilevel attention model, making the learned models highly interpretable. Our experiments showed MINA achieved PR-AUC 0.9436 (outperforming the best baseline by 5.51%) in real world ECG dataset. Finally, MINA also demonstrated robust performance and strong interpretability against signal distortion and noise contamination. ","Cao Xiao, Tengfei Ma",IBM,IJCAI (2019),2019,TRUE
Minimax Pareto Fairness: A Multi Objective Perspective,"In this work we formulate and formally characterize group fairness as a multi-objective optimization problem, where each sensitive group risk is a separate objective. We propose a fairness criterion where a classifier achieves minimax risk and is Pareto-efficient w.r.t. all groups, avoiding unnecessary harm, and can lead to the best zero-gap model if policy dictates so. We provide a simple optimization algorithm compatible with deep neural networks to satisfy these constraints. Since our method does not require test-time access to sensitive attributes, it can be applied to reduce worst-case classification errors between outcomes in unbalanced classification problems. We test the proposed methodology on real case-studies of predicting income, ICU patient mortality, skin lesions classification, and assessing credit risk, demonstrating how our framework compares favorably to other approaches.","Martin Bertran, Natalia Martinez, Guillermo Sapiro ",Duke University,ICML 2020,2020-07,FALSE
Mitigating bias in algorithmic hiring: evaluating claims and practices,"There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.","['Manish Raghavan', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy']","['Cornell University', 'Microsoft Research and Cornell University', 'Cornell University', 'Cornell University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Mitigating Bias in Set Selection with Noisy Protected Attributes,"Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result! Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a ""denoised"" selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness.","['Anay Mehrotra', 'L. Elisa Celis']","['Yale University', 'Yale University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Mitigating Unwanted Biases with Adversarial Learning,"Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.","['Brian Hu Zhang', 'Blake Lemoine', 'Margaret Mitchell']","['Stanford University, Stanford, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,TRUE
Mixed Messages? The Limits of Automated Social Media Content Analysis,"Governments and companies are turning to automated tools to make sense of what people post on social media. Policymakers routinely call for social media companies to identify and take down hate speech, terrorist propaganda, harassment, ""fake news"" or disinformation. Other policy proposals have focused on mining social media to inform law enforcement and immigration decisions. But these proposals wrongly assume that automated technology can accomplish on a large scale the kind of nuanced analysis that humans can do on a small scale. Today's tools for analyzing social media text have limited ability to parse the meaning of human communication or detect the intent of the speaker. A knowledge gap exists between data scientists studying natural language processing (NLP) and policymakers advocating for wide adoption of automated social media analysis and moderation. Policymakers must understand the capabilities and limits of NLP before endorsing or adopting automated content analysis tools, particularly for making decisions that affect fundamental rights or access to government benefits. Without proper safeguards, these tools can facilitate overbroad censorship and biased enforcement of laws or terms of service. This paper draws on existing research to explain the capabilities and limitations of text classifiers for social media posts and other online content. It is aimed at helping researchers and technical experts address the gaps in policymakers' knowledge about what is possible with automated text analysis. ","Natasha Duarte, Emma Llanso, Anna Loup",Center for Democracy & Technology,FAT* 2018,2018,FALSE
Model agnostic interpretability of rankers via intent modelling,"A key problem in information retrieval is understanding the latent intention of a user's under-specified query. Retrieval models that are able to correctly uncover the query intent often perform well on the document ranking task. In this paper we study the problem of interpretability for text based ranking models by trying to unearth the query intent as understood by complex retrieval models. We propose a model-agnostic approach that attempts to locally approximate a complex ranker by using a simple ranking model in the term space. Given a query and a blackbox ranking model, we propose an approach that systematically exploits preference pairs extracted from the target ranking and document perturbations to identify a set of intent terms and a simple term based ranker that can faithfully and accurately mimic the complex blackbox ranker in that locality. Our results indicate that we can indeed interpret more complex models with high fidelity. We also present a case study on how our approach can be used to interpret recently proposed neural rankers.","['Jaspreet Singh', 'Avishek Anand']","['L3S Research Center, Hannover, Germany', 'L3S Research Center, Hannover, Germany']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Model Cards for Model Reporting,"Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.","['Margaret Mitchell', 'Simone Wu', 'Andrew Zaldivar', 'Parker Barnes', 'Lucy Vasserman', 'Ben Hutchinson', 'Elena Spitzer', 'Inioluwa Deborah Raji', 'Timnit Gebru']","['Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'University of Toronto', 'Google']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Model Reconstruction from Model Explanations,"We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations. On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive. Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.","['Smitha Milli', 'Ludwig Schmidt', 'Anca D. Dragan', 'Moritz Hardt']","['University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Modeling Epistemological Principles for Bias Mitigation in AI Systems: An Illustration in Hiring Decisions,"Artificial Intelligence (AI) has been used extensively in automatic decision making in a broad variety of scenarios, ranging from credit ratings for loans to recommendations of movies. Traditional design guidelines for AI models focus essentially on accuracy maximization, but recent work has shown that economically irrational and socially unacceptable scenarios of discrimination and unfairness are likely to arise unless these issues are explicitly addressed. This undesirable behavior has several possible sources, such as biased datasets used for training that may not be detected in black-box models. After pointing out connections between such bias of AI and the problem of induction, we focus on Popper's contributions after Hume's, which offer a logical theory of preferences. An AI model can be preferred over others on purely rational grounds after one or more attempts at refutation based on accuracy and fairness. Inspired by such epistemological principles, this paper proposes a structured approach to mitigate discrimination and unfairness caused by bias in AI systems. In the proposed computational framework, models are selected and enhanced after attempts at refutation. To illustrate our discussion, we focus on hiring decision scenarios where an AI system filters in which job applicants should go to the interview phase.","['Marisa Vasconcelos', 'Carlos Cardonha', 'Bernardo Gonçalves']","['IBM Research, Sao Paulo, Brazil', 'IBM Research, Sao Paulo, Brazil', 'IBM Research, Sao Paulo, Brazil']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,TRUE
Modelling and Influencing the AI Bidding War: A Research Agenda,"A race for technological supremacy in AI could lead to serious negative consequences, especially whenever ethical and safety procedures are underestimated or even ignored, leading potentially to the rejection of AI in general. For all to enjoy the benefits provided by safe, ethical and trustworthy AI systems, it is crucial to incentivise participants with appropriate strategies that ensure mutually beneficial normative behaviour and safety-compliance from all parties involved. Little attention has been given to understanding the dynamics and emergent behaviours arising from this AI bidding war, and moreover, how to influence it to achieve certain desirable outcomes (e.g. AI for public good and participant compliance). To bridge this gap, this paper proposes a research agenda to develop theoretical models that capture key factors of the AI race, revealing which strategic behaviours may emerge and hypothetical scenarios therein. Strategies from incentive and agreement modelling are directly applicable to systematically analyse how different types of incentives (namely, positive vs. negative, peer vs. institutional, and their combinations) influence safety-compliant behaviours over time, and how such behaviours should be configured to ensure desired global outcomes, studying at the same time how these mechanisms influence AI development. This agenda will provide actionable policies, showing how they need to be employed and deployed in order to achieve compliance and thereby avoid disasters as well as loosing confidence and trust in AI in general.","['The Anh Han', 'Luís Moniz Pereira', 'Tom Lenaerts']","['Teesside Univeresity, Middlesbrough, United Kingdom', 'Universidade Nova de Lisboa, Lisbon, Portugal', 'Université Libre de Bruxelles & Vrije Universiteit Brussel, Brussels, Belgium']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Monitoring Misuse for Accountable 'Artificial Intelligence as a Service',"AI is increasingly being offered 'as a service' (AIaaS). This entails service providers offering customers access to pre-built AI models and services, for tasks such as object recognition, text translation, text-to-voice conversion, and facial recognition, to name a few. The offerings enable customers to easily integrate a range of powerful AI-driven capabilities into their applications. Customers access these models through the provider's APIs, sending particular data to which models are applied, the results of which returned. However, there are many situations in which the use of AI can be problematic. AIaaS services typically represent generic functionality, available 'at a click'. Providers may therefore, for reasons of reputation or responsibility, seek to ensure that the AIaaS services they offer are being used by customers for 'appropriate' purposes. This paper introduces and explores the concept whereby AIaaS providers uncover situations of possible service misuse by their customers. Illustrated through topical examples, we consider the technical usage patterns that could signal situations warranting scrutiny, and raise some of the legal and technical challenges of monitoring for misuse. In all, by introducing this concept, we indicate a potential area for further inquiry from a range of perspectives.","['Seyyed Ahmad Javadi', 'Richard Cloete', 'Jennifer Cobbe', 'Michelle Seng Ah Lee', 'Jatinder Singh']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
"More Than ""If Time Allows"": The Role of Ethics in AI Education","Even as public pressure mounts for technology companies to consider societal impacts of products, industries and governments in the AI race are demanding technical talent. To meet this demand, universities clamor to add technical artificial intelligence (AI) and machine learning (ML) courses into computing curriculum-but how are societal and ethical considerations part of this landscape? We explore two pathways for ethics content in AI education: (1) standalone AI ethics courses, and (2) integrating ethics into technical AI courses. For both pathways, we ask: What is being taught? As we train computer scientists who will build and deploy AI tools, how are we training them to consider the consequences of their work? In this exploratory work, we qualitatively analyzed 31 standalone AI ethics classes from 22 U.S. universities and 20 AI/ML technical courses from 12 U.S. universities to understand which ethics-related topics instructors include in courses. We identify and categorize topics in AI ethics education, share notable practices, and note omissions. Our analysis will help AI educators identify what topics should be taught and create scaffolding for developing future AI ethics education.","['Natalie Garrett', 'Nathan Beard', 'Casey Fiesler']","['University of Colorado Boulder, Boulder, CO, USA', 'University of Maryland, College Park, MD, USA', 'University of Colorado Boulder, Boulder, CO, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Multi-category fairness in sponsored search auctions,"Fairness in advertising is a topic of particular concern motivated by theoretical and empirical observations in both the computer science and economics literature. We examine the problem of fairness in advertising for general purpose platforms that service advertisers from many different categories. First, we propose inter-category and intra-category fairness desiderata that take inspiration from individual fairness and envy-freeness. Second, we investigate the ""platform utility"" (a proxy for the quality of allocation) achievable by mechanisms satisfying these desiderata. More specifically, we compare the utility of fair mechanisms against the unfair optimum, and show by construction that our fairness desiderata are compatible with utility. Our mechanisms also enjoy nice implementation properties including metric-obliviousness, which allows the platform to produce fair allocations without needing to know the specifics of the fairness requirements.","['Christina Ilvento', 'Meena Jagadeesan', 'Shuchi Chawla']","['Harvard University', 'Harvard University', 'University of Wisconsin-Madison']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Multi-layered explanations from algorithmic impact assessments in the GDPR,"Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability. The aim of this paper is to address how Data Protection Impact Assessments (DPIAs) (Art. 35) in the European Union (EU)'s General Data Protection Regulation (GDPR) link the GDPR's two approaches to algorithmic accountability---individual rights and systemic governance--- and potentially lead to more accountable and explainable algorithms. We argue that algorithmic explanation should not be understood as a static statement, but as a circular and multi-layered transparency process based on several layers (general information about an algorithm, group-based explanations, and legal justification of individual decisions taken). We argue that the impact assessment process plays a crucial role in connecting internal company heuristics and risk mitigation to outward-facing rights, and in forming the substance of several kinds of explanations.","['Margot E. Kaminski', 'Gianclaudio Malgieri']","['University of Colorado', 'Vrije Universiteit Brussels, Brussels, Belgium']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Multi-Task Learning for Personal Search Ranking with Query Clustering,"User needs vary significantly across different tasks, and therefore
their queries will also vary significantly in their expressiveness
and semantics. Many studies have been proposed
to model such query diversity by obtaining query types and
building query-dependent ranking models. To obtain query
types, these studies typically require either a labeled query
dataset or clicks from multiple users aggregated over the
same document. These techniques, however, are not applicable
when manual query labeling is not viable, and aggregated
clicks are unavailable due to the private nature of the document
collection, e.g., in personal search scenarios. Therefore,
in this paper, we study the problem of how to obtain query
type in an unsupervised fashion and how to leverage this information
using query-dependent ranking models in personal
search. We first develop a hierarchical clustering algorithm
based on truncated SVD and varimax rotation to obtain
coarse-to-fine query types. Then, we propose three query-dependent
ranking models, including two neural models that
leverage query type information as additional features, and
one novel multi-task neural model that is trained to simultaneously
rank documents and predict query types. We evaluate
our ranking models using the click data collected from one of
the worldâs largest personal search engines. The experiments
demonstrate that the proposed multi-task model can significantly
outperform the baseline neural models, which either
do not incorporate query type information or just simply
feed query type as an additional feature. To the best of our
knowledge, this is the first successful application of query-dependent
multi-task learning in personal search ranking.","['Maryam Karimzadehgan', 'Michael Bendersky', 'Zhen Qin', 'Don Metzler']",Google,Proceedings of ACM Conference on Information and Knowledge Management (CIKM) (2018),2018,TRUE
Multiaccuracy: Black-Box Post-Processing for Fairness in Classification,"Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for ""black women"") even when the sensitive features (e.g. ""race"", ""gender"") are not given to the algorithm explicitly.","['Michael P. Kim', 'Amirata Ghorbani', 'James Zou']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Narratives and Counternarratives on Data Sharing in Africa,"As machine learning and data science applications grow ever more prevalent, there is an increased focus on data sharing and open data initiatives, particularly in the context of the African continent. Many argue that data sharing can support research and policy design to alleviate poverty, inequality, and derivative effects in Africa. Despite the fact that the datasets in question are often extracted from African communities, conversations around the challenges of accessing and sharing African data are too often driven by non-African stakeholders. These perspectives frequently employ a deficit narratives, often focusing on lack of education, training, and technological resources in the continent as the leading causes of friction in the data ecosystem. We argue that these narratives obfuscate and distort the full complexity of the African data sharing landscape. In particular, we use storytelling via fictional personas built from a series of interviews with African data experts to complicate dominant narratives and to provide counternarratives. Coupling these personas with research on data practices within the continent, we identify recurring barriers to data sharing as well as inequities in the distribution of data sharing benefits. In particular, we discuss issues arising from power imbalances resulting from the legacies of colonialism, ethno-centrism, and slavery, disinvestment in building trust, lack of acknowledgement of historical and present-day extractive practices, and Western-centric policies that are ill-suited to the African context. After outlining these problems, we discuss avenues for addressing them when sharing data generated in the continent.","['Rediet Abebe', 'Kehinde Aruleba', 'Abeba Birhane', 'Sara Kingsley', 'George Obaido', 'Sekou L. Remy', 'Swathi Sadagopan']","['University of California, Berkeley', 'University of the Witwatersrand', 'University College Dublin & Lero', 'Carnegie Mellon University', 'University of the Witwatersrand', 'IBM Research - Africa', 'Deloitte']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Neural Network Control Policy Verification With Persistent Adversarial Perturbation,"Deep neural networks are known to be fragile to small adversarial perturbations. This issue becomes more critical when a neural network is interconnected with a physical system in a closed loop. In this paper, we show how to combine recent works on static neural network certification tools with robust control theory to certify a neural network policy in a control loop. We give a sufficient condition and an algorithm to ensure that the closed loop state and control constraints are satisfied when the persistent adversarial perturbation is linf norm bounded. Our method is based on finding a positively invariant set of the closed loop dynamical system, and thus we do not require the continuity of the neural network policy. Along with the verification result, we also develop an effective attack strategy for neural network control systems that outperforms exhaustive Monte-Carlo search significantly. We show that our certification algorithm works well on learned models and achieves 5 times better result than the traditional Lipschitz-based method to certify the robustness of a neural network policy on multiple control problems.","Yuh-Shyang Wang, Tsui-Wei Weng, Luca Daniel ","[""Argo AI"", ""MIT""]",ICML 2020,2020-07,TRUE
Non-Discriminatory Machine Learning through Convex Fairness Criteria,We introduce a novel technique to achieve non-discrimination in machine learning without sacrificing convexity and probabilistic interpretation. We also propose a new notion of fairness for machine learning called the weighted proportional fairness and show that our technique satisfies this subjective fairness criterion.,"['Naman Goel', 'Mohammad Yaghini', 'Boi Faltings']","['Ecole Polytechnique Federal de Lausanne, Lausanne, Switzerland', 'Ecole Polytechnique Federal de Lausanne, Lausanne, Switzerland', 'Ecole Polytechnique Federal de Lausanne, Lausanne, Switzerland']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Normative Principles for Evaluating Fairness in Machine Learning,"There are many incompatible ways to measure fair outcomes for machine learning algorithms. The goal of this paper is to characterize rates of success and error across protected groups (race, gender, sexual orientation) as a distribution problem, and describe the possible solutions to this problem according to different normative principles from moral and political philosophy. These normative principles are based on various competing attributes within a distribution problem: intentions, compensation, desert, consent, and consequences. Each principle will be applied to a sample risk-assessment classifier to demonstrate the philosophical arguments underlying different sets of fairness metrics.",['Derek Leben'],"['University of Pittsburgh, Johnstown, Pittsburgh, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
"Norms, Rewards, and the Intentional Stance: Comparing Machine Learning Approaches to Ethical Training","The challenge of training AI systems to perform responsibly and beneficially has inspired different approaches for teaching a system what people want and how it is acceptable to attain that in the world. In this paper we compare work in reinforcement learning, in particular inverse reinforcement learning, with our norm inference approach. We test those two systems and present results. Using the idea of the ""intentional stance"", we explain how a norm inference approach can work even when another agent is acting strictly according to reward functions. In this way norm inference presents itself as a promising, more explicitly accountable approach with which to design AI systems from the start.","['Daniel Kasenberg', 'Thomas Arnold', 'Matthias Scheutz']","['Tufts University, Medford, MA, USA', 'Tufts University, Medford, MA, USA', 'Tufts University, Medford, MA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Nurturing the Companion ChatBot,"Although recent technical progress of artificial intelligence is impressive, the affective effect of intelligent agents has not been investigated sufficiently. This research focuses on the affective effect of ChatBot. We observe an interesting phenomenon that not only the affective response from ChatBot influences user's experience, but also the manner that user interacts with ChatBot affects the development of the ChatBot's intelligence. So this work proposes an ethical issue that it is necessary to regularize the user's behavior. We validate this argument by designing a novel paradigm, which enables the users to nurture companion ChatBots via developmental artificial intelligence techniques. With only twenty days nurturing, the users build affective bonding with the ChatBots and the ChatBots show significant progress in communication skills.",['Gong Chen'],"['Hong Kong Polytechnic University, Hong Kong, Hong Kong']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Obtaining Fairness using Optimal Transport Theory,"In the fair classification setup, we recast the links between fairness and predictability in terms of probability metrics. We analyze repair methods based on mapping conditional distributions to the Wasserstein barycenter. We propose a Random Repair which yields a tradeoff between minimal information loss and a certain amount of fairness. ","Paula Gordaliza, Eustasio Del Barrio, Gamboa Fabrice, Jean-Michel Loubes","[""IMUVA"", ""Institut de Mathematiques de Toulouse""]",ICML 2019,2019-06,FALSE
On Completeness-aware Concept-Based Explanations in Deep Neural Networks,"Concept-based explanations can be a key direction to understand how DNNs make decisions. In this paper, we study concept-based explainability in a systematic framework. First, we define the notion of completeness, which quantifies how sufficient a particular set of concepts is in explaining the model's behavior. Based on performance and variability motivations, we propose two definitions to quantify completeness. We show that they yield the commonly-used PCA method under certain assumptions. Next, we study two additional constraints to ensure the interpretability of discovered concept, based on sparsity principles. Through systematic experiments, on specifically-designed synthetic dataset and real-world text and image datasets, we demonstrate the superiority of our framework in finding concepts that are complete (in explaining the decision) and that are interpretable.","['Been Kim', 'Sercan Arik', 'Chun-Liang Li', 'Tomas Pfister']",Google,NeurIPS (2020),2020,TRUE
On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection,"Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency. In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels (>20% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.","['Vivian Lai', 'Chenhao Tan']","['University of Colorado Boulder', 'University of Colorado Boulder']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
On Influencing Individual Behavior for Reducing Transportation Energy Expenditure in a Large Population,"Our research aims at developing intelligent systems to reduce the transportation-related energy expenditure of a large city by influencing individual behavior. We introduce Copter - an intelligent travel assistant that evaluates multi-modal travel alternatives to find a plan that is acceptable to a person given their context and preferences. We propose a formulation for acceptable planning that brings together ideas from AI, machine learning, and economics. This formulation has been incorporated in Copter producing acceptable plans in real-time. We adopt a novel empirical evaluation framework that combines human decision data with high-fidelity simulation to demonstrate a 4% energy reduction and 20% delay reduction in a realistic deployment scenario in Los Angeles, California, USA.","['Shiwali Mohan', 'Frances Yan', 'Victoria Bellotti', 'Ahmed Elbery', 'Hesham Rakha', 'Matthew Klenk']","['Palo Alto Research Center, Palo Alto, CA, USA', 'Palo Alto Research Center, Palo Alto, CA, USA', 'Lyft, San Francisco, CA, USA', 'VTTI, Blacksburg, VA, USA', 'VTTI, Blacksburg, VA, USA', 'Palo Alto Research Center, Blacksburg, VA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,TRUE
On Microtargeting Socially Divisive Ads: A Case Study of Russia-Linked Ad Campaigns on Facebook,"Targeted advertising is meant to improve the efficiency of matching advertisers to their customers. However, targeted advertising can also be abused by malicious advertisers to efficiently reach people susceptible to false stories, stoke grievances, and incite social conflict. Since targeted ads are not seen by non-targeted and non-vulnerable people, malicious ads are likely to go unreported and their effects undetected. This work examines a specific case of malicious advertising, exploring the extent to which political ads1 from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S. elections exploited Facebook's targeted advertising infrastructure to efficiently target ads on divisive or polarizing topics (e.g., immigration, race-based policing) at vulnerable sub-populations. In particular, we do the following: (a) We conduct U.S. census-representative surveys to characterize how users with different political ideologies report, approve, and perceive truth in the content of the IRA ads. Our surveys show that many ads are ""divisive"": they elicit very different reactions from people belonging to different socially salient groups. (b) We characterize how these divisive ads are targeted to sub-populations that feel particularly aggrieved by the status quo. Our findings support existing calls for greater transparency of content and targeting of political ads. (c) We particularly focus on how the Facebook ad API facilitates such targeting. We show how the enormous amount of personal data Facebook aggregates about users and makes available to advertisers enables such malicious targeting.","['Filipe N. Ribeiro', 'Koustuv Saha', 'Mahmoudreza Babaei', 'Lucas Henrique', 'Johnnatan Messias', 'Fabricio Benevenuto', 'Oana Goga', 'Krishna P. Gummadi', 'Elissa M. Redmiles']","['UFOP/UFMG, Brazil', 'Georgia Tech, US', 'MPI-SWS, Germany', 'UFMG, Brazil', 'MPI-SWS, Germany', 'UFMG, Brazil', 'Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, France', 'MPI-SWS, Germany', 'University of Maryland, US']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
On the apparent conflict between individual and group fairness,"A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.",['Reuben Binns'],['University of Oxford'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜,"The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.","['Emily M. Bender', 'Timnit Gebru', 'Angelina McMillan-Major', 'Shmargaret Shmitchell']","['University of Washington, Seattle, WA, USA', 'Black in AI, Palo Alto, CA, USA', 'University of Washington, Seattle, WA, USA', 'The Aether']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
On the Distance Between CP-nets,"Preferences play a key role in decision making by both single individuals and/or groups. In a multi-agent context, it is also important to know how to aggregate preferences to reach a collective decision. Moreover, being able to measure the distance between the preference of two individuals is important to identify the amount of disagreement and possibly reach consensus. In this paper we define a notion of distance between CP-nets, a formalism that can compactly encode conditional qualitative preferences. We consider the Kendall-tau distance between the partial orders induced by CPnets, and we define two tractable approximations of that distance, which can be computed in time polynomial in the number of features of the CP-nets. We then perform experiments to demonstrate the quality of these approximations compared to the Kendall-tau distance. We also relate our two notions of distance to the distance rationalizability of sequential plurality voting for CP-nets.","A. Loreggia, N. Mattei, F. Rossi and K. B. Venable",IBM,AAMAS (2018),2018,TRUE
On the Distinction between Implicit and Explicit Ethical Agency,"With recent advances in artificial intelligence and the rapidly increasing importance of autonomous intelligent systems in society, it is becoming clear that artificial agents will have to be designed to comply with complex ethical standards. As we work to develop moral machines, we also push the boundaries of existing legal categories. The most pressing question is what kind of ethical decision-making our machines are actually able to engage in. Both in law and in ethics, the concept of agency forms a basis for further legal and ethical categorisations, pertaining to decision-making ability. Hence, without a cross-disciplinary understanding of what we mean by ethical agency in machines, the question of responsibility and liability cannot be clearly addressed. Here we make first steps towards a comprehensive definition, by suggesting ways to distinguish between implicit and explicit forms of ethical agency.","['Sjur Dyrkolbotn', 'Truls Pedersen', 'Marija Slavkovik']","['Western Norway University of Applied Sciences, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning,"Most existing notions of algorithmic fairness are static: they ensure some form of allocative equality at the time of decision making, but do not account for the adverse impact of the algorithmic decisions today on the long-term welfare and prosperity of certain segments of the population. We take a broader perspective on algorithmic fairness. We propose an effort-based measure of fairness and present a data-driven framework for characterizing the long-term impact of algorithmic policies on reshaping the underlying population. Motivated by the psychological literature on social learning and the economic literature on equality of opportunity, we propose a micro-scale model of how individuals respond to decision making algorithms. We employ existing measures of segregation from sociology and economics to quantify the resulting macro-scale population-level change. Importantly, we observe that different models may shift the group-conditional distribution of qualifications in different directions. Our findings raise a number of important questions regarding the formalization of fairness for decision-making models.","Hoda Heidari, Vedant Nanda, Krishna Gummadi","[""ETH Zurich"", ""MPI-SWS""]",ICML 2019,2019-06,FALSE
On the Moral Justification of Statistical Parity,"A crucial but often neglected aspect of algorithmic fairness is the question of how we justify enforcing a certain fairness metric from a moral perspective. When fairness metrics are proposed, they are typically argued for by highlighting their mathematical properties. Rarely are the moral assumptions beneath the metric explained. Our aim in this paper is to consider the moral aspects associated with the statistical fairness criterion of independence (statistical parity). To this end, we consider previous work, which discusses the two worldviews ""What You See Is What You Get"" (WYSIWYG) and ""We're All Equal"" (WAE) and by doing so provides some guidance for clarifying the possible assumptions in the design of algorithms. We present an extension of this work, which centers on morality. The most natural moral extension is that independence needs to be fulfilled if and only if differences in predictive features (e.g. high school grades and standardized test scores are predictive of performance at university) between socio-demographic groups are caused by unjust social disparities or measurement errors. Through two counterexamples, we demonstrate that this extension is not universally true. This means that the question of whether independence should be used or not cannot be satisfactorily answered by only considering the justness of differences in the predictive features.","['Corinna Hertweck', 'Christoph Heitz', 'Michele Loi']","['Zurich University of Applied Sciences, University of Zurich', 'Zurich University of Applied Sciences', 'University of Zurich']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
"One Label, One Billion Faces: Usage and Consistency of Racial Categories in Computer Vision","Computer vision is widely deployed, has highly visible, society-altering applications, and documented problems with bias and representation. Datasets are critical for benchmarking progress in fair computer vision, and often employ broad racial categories as population groups for measuring group fairness. Similarly, diversity is often measured in computer vision datasets by ascribing and counting categorical race labels. However, racial categories are ill-defined, unstable temporally and geographically, and have a problematic history of scientific use. Although the racial categories used across datasets are superficially similar, the complexity of human race perception suggests the racial system encoded by one dataset may be substantially inconsistent with another. Using the insight that a classifier can learn the racial system encoded by a dataset, we conduct an empirical study of computer vision datasets supplying categorical race labels for face images to determine the cross-dataset consistency and generalization of racial categories. We find that each dataset encodes a substantially unique racial system, despite nominally equivalent racial categories, and some racial categories are systemically less consistent than others across datasets. We find evidence that racial categories encode stereotypes, and exclude ethnic groups from categories on the basis of nonconformity to stereotypes. Representing a billion humans under one racial category may obscure disparities and create new ones by encoding stereotypes of racial systems. The difficulty of adequately converting the abstract concept of race into a tool for measuring fairness underscores the need for a method more flexible and culturally aware than racial categories.","['Zaid Khan', 'Yun Fu']","['Northeastern University', 'Northeastern University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Onward for the freedom of others: marching beyond the AI ethics,"The debate on the ethics of Artificial Intelligence brought together different stakeholders including but not limited to academics, policymakers, CEOs, activists, workers' representatives, lobbyists, journalists, and 'moral machines'. Prominent political institutions crafted principles for the 'ethical being' of the AI companies while tech giants were documenting ethics in a series of self-written guidelines. In parallel, a large community started to flourish, focusing on how to technically embed ethical parameters into algorithmic systems. Founded upon the philosophical work of Simone de Beauvoir and Jean-Paul Sartre, this paper explores the philosophical antinomies of the 'AI Ethics' debate as well as the conceptual disorientation of the 'fairness discussion'. By bringing the philosophy of existentialism to the dialogue, this paper attempts to challenge the dialectical monopoly of utilitarianism and sheds fresh light on the -already glaring- AI arena. Why is 'the AI Ethics guidelines' a futile battle doomed to dangerous abstraction? How this battle can harm our sense of collective freedom? Which is the uncomfortable reality that remains obscured by the smoke-gas of the 'AI Ethics' discussion? And eventually, what's the alternative? There seems to be a different pathway for discussing and implementing ethics; A pathway that sets the freedom of others at the epicenter of the battle for a sustainable and open to all future.",['Petros Terzis'],"['University of Winchester, Winchester, UK']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Operationalizing Framing to Support Multiperspective Recommendations of Opinion Pieces,"Diversity in personalized news recommender systems is often defined as dissimilarity, and operationalized based on topic diversity (e.g., corona versus farmers strike). Diversity in news media, however, is understood as multiperspectivity (e.g., different opinions on corona measures), and arguably a key responsibility of the press in a democratic society. While viewpoint diversity is often considered synonymous with source diversity in communication science domain, in this paper, we take a computational view. We operationalize the notion of framing, adopted from communication science. We apply this notion to a re-ranking of topic-relevant recommended lists, to form the basis of a novel viewpoint diversification method. Our offline evaluation indicates that the proposed method is capable of enhancing the viewpoint diversity of recommendation lists according to a diversity metric from literature. In an online study, on the Blendle platform, a Dutch news aggregator, with more than 2000 users, we found that users are willing to consume viewpoint diverse news recommendations. We also found that presentation characteristics significantly influence the reading behaviour of diverse recommendations. These results suggest that future research on presentation aspects of recommendations can be just as important as novel viewpoint diversification methods to truly achieve multiperspectivity in online news environments.","['Mats Mulder', 'Oana Inel', 'Jasper Oosterman', 'Nava Tintarev']","['Delft University of Technology, Delft, The Netherlands', 'Delft University of Technology, Delft, The Netherlands', 'Blendle Utrecht, The Netherlands', 'Maastricht University Maastricht, The Netherlands']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Operationalizing the Legal Principle of Data Minimization for Personalization,"Article 5(1)(c) of the European Union’s General Data Protection Regulation (GDPR) requires that “personal data shall be […] adequate, relevant, and limited to what is necessary in relation to the purposes for which they are processed (‘data minimisation’)”. To date, the legal and computational definitions of ‘purpose limitation’ and ‘data minimization’ remain largely unclear. In particular, the interpretation of these principles is an open issue for information access systems that optimize for user experience through personalization and do not strictly require personal data collection for the delivery of basic service.","['Asia J. Biega', 'Peter Potash', 'Hal Daumé III', 'Fernando Diaz', 'Michèle Finck']",Microsoft,SIGIR 2020,2020-07-28,TRUE
Opportunities and Challenges for Artificial Intelligence in India,"In the future of India lies the future of a sixth of the world's population. As the Artificial Intelligence (AI) revolution sweeps through societies and enters daily life, its role in shaping India's development and growth is bound to be substantial. For India, AI holds promise as a catalyst to accelerate progress, while providing mechanisms to leapfrog traditional hurdles such as poor infrastructure and bureaucracy. At the same time, an investment in AI is accompanied by risk factors with long-term implications on society: it is imperative that risks be vetted at this early stage. In this paper, we describe opportunities and challenges for AI in India. We detail opportunities that are cross-cutting (bridging India's linguistic divisions, mining public data), and also specific to one particular sector (healthcare). We list challenges that originate from existing social conditions (such as equations of caste and gender). Thereafter we distill out concrete steps and safeguards, which we believe are necessary for robust and inclusive development as India enters the AI era.","['Shivaram Kalyanakrishnan', 'Rahul Alex Panicker', 'Sarayu Natarajan', 'Shreya Rao']","['Indian Institute of Technology Bombay, Mumbai, India', 'Embrace Innovations, Bangalore, India', ""King's College London, London, United Kingdom"", 'Rao Law Chambers, Azim Premji University, Bangalore, India']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Optimal Contest Design for Multi-Agent Systems,"Contests have become a highly popular crowdsourcing mechanism aiming to solicit effort of the crowd in solving well defined problems, and as such are extensively studied within the framework of contest design. In this paper, I describe preliminary work on a type of contest that has recently gained momentum, and give an overview of my PhD research proposal, carried out under the supervision of Prof. David Sarne.",['Priel Levy'],"['Bar Ilan University, Ramat Gan, Israel']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Optimal Noise-Adding Mechanism in Additive Differential Privacy,"We derive the optimal $(0, \delta)$-differentially private query-output independent noise-adding mechanism for single real-valued query function under a general cost-minimization framework. Under a mild technical condition, we show that the optimal noise probability distribution is a uniform distribution with a probability mass at the origin. We explicitly derive the optimal noise distribution for general $\ell^p$ cost functions, including $\ell^1$ (for noise magnitude) and $\ell^2$ (for noise power) cost functions, and show that the probability concentration on the origin occurs when $\delta > \frac&lbrace;p&rbrace;&lbrace;p+1&rbrace;$. Our result demonstrates an improvement over the existing Gaussian mechanisms  by a factor of two and three for $(0,\delta)$-differential privacy in the high privacy regime in the context of minimizing the noise magnitude and noise power, and the gain is more pronounced in the low privacy regime. Our result is consistent with the existing result for $(0,\delta)$-differential privacy in the discrete setting, and identifies a probability concentration phenomenon in the continuous setting.","['Wei Ding', 'Ruiqi Guo', 'Sanjiv Kumar']",Google,Proceedings of the 22th International Conference on Artificial Intelligence and Statistics (AISTATS) (2019),2019,TRUE
"Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals","Machine learning models often need to satisfy many real-world policy goals and capture some
kinds of side information. We show that many such goals can be mathematically expressed as
constraints on the modelâs predictions on the data, which we call rate constraints. In this paper, we
study the specific problem of training non-convex models subject to these rate constraints (which
are non-convex and non-differentiable), and the general problem of constrained optimization of
possibly non-convex objectives with possibly non-convex and non-differentiable constraints. In the
non-convex setting, the Lagrangian may not have an equilibrium to converge to, and thus using the
standard approach of Lagrange multipliers may fail as a deterministic solution may not even exist.
Furthermore, if the constraints are non-differentiable, then one cannot optimize the Lagrangian
with gradient-based methods by definition. To solve these issues, we present the proxy-Lagrangian,
which leads to an algorithm that produces a stochastic classifier with theoretical guarantees by
playing a two-player non-zero-sum game. The first player minimizes external regret in terms of a
differentiable relaxation of the constraints and the second player enforces the original constraints
by minimizing swap-regret: this leads to finding a solution concept which we call a semi-coarse
correlated equilibrium which we interestingly show corresponds to an approximately optimal and
feasible solution to the constrained optimization problem. We then give a procedure which shrinks
the randomized solution down to one that is a mixture of at most m + 1 deterministic solutions.
This culminates into end-to-end procedures which can provably solve non-convex constrained
optimization problems with possibly non-differentiable and non-convex constraints. We provide
extensive experimental results on benchmark and real-world problems, enforcing a broad range of
policy goals including different fairness metrics, and other goals on accuracy, coverage, recall, and
churn.","['Andrew Cotter', 'Serena Wang']",Google,Journal of Machine Learning Research (2019),2019,TRUE
Optimized Pre-Processing for Discrimination Prevention,"Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective, and apply two instances of the proposed optimization to datasets, including one on real-world criminal recidivism. The results demonstrate that all three criteria can be simultaneously achieved and also reveal interesting patterns of bias in American society.","F. P. Calmon, D. Wei, B. Vinzamuri, K. N. Ramamurthy, and K. R. Varshney",IBM,NeurIPS (2017),2018,TRUE
Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems,"Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.",['Joshua A. Kroll'],"['Naval Postgraduate School, Monterey, CA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Overcoming Failures of Imagination in AI Infused System Development and Deployment,"NeurIPS 2020 requested that research paper submissions include impact statements on ‘potential nefarious uses and the consequences of failure.’ When researching, designing, and implementing systems, a key challenge to anticipating risks, however, is to overcome what Clarke (1962) called ‘failures of imagination.’ The growing research on bias, fairness, and transparency in computational systems aims to illuminate and mitigate harms, and could thus help inform reflections on possible negative impacts of particular pieces of technical work. The prevalent notion of computational harms — narrowly construed as either allocational or representational harms — does not fully capture the context dependent and unobservable nature of harms across the wide range of AI infused systems. The current literature primarily addresses only a small range of examples of harms to motivate algorithmic fixes, overlooking the wider scope of probable harms and the way these harms may affect different stakeholders. The system affordances and possible usage scenarios may also exacerbate harms in unpredictable ways, as they determine stakeholders’ control (including non-users) over how they interact with a system output. To effectively assist in anticipating and identifying harmful uses, we argue that frameworks of harms must be context-aware and consider a wider range of potential stakeholders, system affordances, uses, and outputs, as well as viable proxies for assessing harms in the widest sense.","['Margarita Boyarskaya', 'Alexandra Olteanu', 'Kate Crawford']",Microsoft,In the Navigating the Broader Impacts of AI Research Workshop at NeurIPS 2020,2020-12-01,TRUE
Overtrust of Robots in High-Risk Scenarios,"From personal robot assistant to self-driving vehicles, artificial intelligence (AI) is the backbone underlying millions of future advanced applications. As robots become increasingly pervasive in daily life, it is expected that robots will augment human laborers in many domains in the near future. When robots are deployed in the real world, the underlying assumption is that they are capable of accomplishing their given tasks. However, researchers have shown that robots made mistakes, and in several cases, humans tend to overtrust robotic systems (Abney 2017; Borenstein et al. 2017; Robinette, Howard, and Wagner 2017). Overtrust of a robot happens in scenarios where (1) a person accepts risk because that person believes the robot can perform a function that it cannot or (2) the person accepts too much risk because the expectation is that the system will mitigate the risk."" (Abney 2017). In particular, we are interested in two emerging domains where an appropriate amount of trust is a minimal requirement and overtrust could cause harm: 1) healthcare scenarios and 2) self-driving car (i.e. autonomous driving) scenarios. Both healthcare and autonomous driving scenarios often involve high risks, and the negative outcomes could be detrimental to the user. The objective of our research focuses on 1) investigating the causes that contribute to human overtrust of these robots systems 2) developing a behavior-based computational model to predict overtrust, and 3) developing techniques to mitigate outcomes caused by the overtrust.",['Jin Xu'],"['Georgia Institute of Technology, Atlanta, GA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Overview of the TREC 2019 Fair Ranking Track,"


The goal of the TREC Fair Ranking track was to develop a benchmark for evaluating retrieval systems in terms of fairness to different content providers in addition to classic notions of relevance. As part of the benchmark, we defined standardized fairness metrics with evaluation protocols and released a dataset for the fair ranking problem. The 2019 task focused on reranking academic paper abstracts given a query. The objective was to fairly represent relevant authors from several groups that were unknown at the system submission time. Thus, the track emphasized the development of systems which have robust performance across a variety of group definitions. Participants were provided with querylog data (queries, documents, and relevance) from Semantic Scholar. This paper presents an overview of the track, including the task definition, descriptions of the data and the annotation process, as well as a comparison of the performance of submitted systems.


","['Asia J. Biega', 'Fernando Diaz', 'Michael D. Ekstrand', 'Sebastian Kohlmeier']",Microsoft,TREC 2019,2019-11-01,TRUE
Pairwise Fairness for Ranking and Regression,"Improving fairness for ranking and regression models has less mature algorithmic tooling than classifiers. Here, we present pairwise formulations of fairness for ranking and regression models that can express analogues of statistical fairness notions like equal opportunity or equal accuracy, as well as statistical parity. The proposed framework supports both discrete protected groups, and continuous protected attributes. We show that the resulting training problems can be efficiently and effectively solved using constrained optimization or robust optimization algorithms. Experiments illustrate the broad applicability and trade-offs of these methods.","['Harikrishna Narasimhan', 'Andy Cotter', 'Serena Lutong Wang']",Google,33rd AAAI Conference on Artificial Intelligence (2020),2020,TRUE
Paradoxes in Fair Computer-Aided Decision Making,"Computer-aided decision making--where a human decision-maker is aided by a computational classifier in making a decision--is becoming increasingly prevalent. For instance, judges in at least nine states make use of algorithmic tools meant to determine ""recidivism risk scores"" for criminal defendants in sentencing, parole, or bail decisions. A subject of much recent debate is whether such algorithmic tools are ""fair"" in the sense that they do not discriminate against certain groups (e.g., races) of people. Our main result shows that for ""non-trivial"" computer-aided decision making, either the classifier must be discriminatory, or a rational decision-maker using the output of the classifier is forced to be discriminatory. We further provide a complete characterization of situations where fair computer-aided decision making is possible.","['Andrew Morgan', 'Rafael Pass']","['Cornell University, Ithaca, NY, USA', 'Cornell Tech, New York City, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Pareto-Efficient Fairness for Skewed Subgroup Data,"As awareness of the potential for learned models to amplify existing societal biases increases,
the field of ML fairness has developed mitigation techniques. A prevalent method applies constraints, including equality of performance, with respect to subgroups defined over the intersection of sensitive attributes such as race and gender. Enforcing such constraints when the subgroup populations are considerably skewed with respect to a target can lead to unintentional degradation in performance, without benefiting any individual subgroup, counter to the United Nations Sustainable Development goals of reducing inequalities and promoting growth. In order to avoid such performance degradation while ensuring equitable treatment to all groups, we propose Pareto-Efficient Fairness (PEF), which identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane. Specifically, PEF finds a Pareto Optimal point which maximizes multiple subgroup
accuracy measures. The algorithm scalarizes using the adaptive weighted metric norm by iteratively searching the Pareto region of all models enforcing the fairness constraint. PEF is backed
by strong theoretical results on discoverability and provides domain practitioners finer control in
navigating both convex and non-convex accuracyfairness trade-offs. Empirically, we show that PEF
increases performance of all subgroups in skewed synthetic data and UCI datasets.","['Alyssa Whitlock Lees', 'Chris Welty']",Google,AISG (2019),2019,TRUE
Partially Generative Neural Networks for Gang Crime Classification with Partial Information,"More than 1 million homicides, robberies, and aggravated assaults occur in the United States each year. These crimes are often further classified into different types based on the circumstances surrounding the crime (e.g., domestic violence, gang-related). Despite recent technological advances in AI and machine learning, these additional classification tasks are still done manually by specially trained police officers. In this paper, we provide the first attempt to develop a more automatic system for classifying crimes. In particular, we study the question of classifying whether a given violent crime is gang-related. We introduce a novel Partially Generative Neural Networks (PGNN) that is able to accurately classify gang-related crimes both when full information is available and when there is only partial information. Our PGNN is the first generative-classification model that enables to work when some features of the test examples are missing. Using a crime event dataset from Los Angeles covering 2014-2016, we experimentally show that our PGNN outperforms all other typically used classifiers for the problem of classifying gang-related violent crimes.","['Sungyong Seo', 'Hau Chan', 'P. Jeffrey Brantingham', 'Jorja Leap', 'Phebe Vayanos', 'Milind Tambe', 'Yan Liu']","['University of Southern California, Los Angeles, CA, USA', 'University of Nebraska-Lincoln, Lincoln, NE, USA', 'University of California, Los Angeles, Los Angeles, CA, USA', 'University of California, Los Angeles, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Perceptions of Domestic Robots' Normative Behavior Across Cultures,"As domestic service robots become more common and widespread, they must be programmed to efficiently accomplish tasks while aligning their actions with relevant norms. The first step to equip domestic robots with normative reasoning competence is understanding the norms that people apply to the behavior of robots in specific social contexts. To that end, we conducted an online survey of Chinese and United States participants in which we asked them to select the preferred normative action a domestic service robot should take in a number of scenarios. The paper makes multiple contributions. Our extensive survey is the first to: (a) collect data on attitudes of people on normative behavior of domestic robots, (b) across cultures and (c) study relative priorities among norms for this domain. We present our findings and discuss their implications for building computational models for robot normative reasoning.","['Huao Li', 'Stephanie Milani', 'Vigneshram Krishnamoorthy', 'Michael Lewis', 'Katia Sycara']","['University of Pittsburgh, Pittsburgh, PA, USA', 'University of Maryland, Baltimore County & Carnegie Mellon University, Baltimore, MD, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'University of Pittsburgh, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Perturbation Sensitivity Analysis to Detect Unintended Model Biases,"Data-driven statistical Natural Language Processing (NLP) techniques leverage large amounts of language data to build models that can understand language. However, most language data reflect the public discourse at the time the data was produced, and hence NLP models are susceptible to learning incidental associations around named referents at a particular point in time, in addition to general linguistic meaning. An NLP system designed to model notions such as sentiment and toxicity should ideally produce scores that are independent of the identity of such entities mentioned in text and their social associations. For example, in a general purpose sentiment analysis system, a phrase such as I hate Katy Perry should be interpreted as having the same sentiment as I hate Taylor Swift. Based on this idea, we propose a generic evaluation framework, Perturbation Sensitivity Analysis, which detects unintended model biases related to named entities, and requires no new annotations or corpora. We demonstrate the utility of this analysis by employing it on two different NLP models --- a sentiment model and a toxicity model --- applied on online comments in English language from four different genres.","['Vinodkumar Prabhakaran', 'Ben Hutchinson']",Google,EMNLP 2019 (2019),2019,TRUE
Potential for Discrimination in Online Targeted Advertising,"Recently, online targeted advertising platforms like Facebook have been criticized for allowing advertisers to discriminate against users belonging to sensitive groups, i.e., to exclude users belonging to a certain race or gender from receiving their ads. Such criticisms have led, for instance, Facebook to disallow the use of attributes such as ethnic affinity from being used by advertisers when targeting ads related to housing or employment or financial services. In this paper, we show that such measures are far from sufficient and that the problem of discrimination in targeted advertising is much more pernicious. We argue that discrimination measures should be based on the targeted population and not on the attributes used for targeting. We systematically investigate the different targeting methods offered by Facebook for their ability to enable discriminatory advertising. We show that a malicious advertiser can create highly discriminatory ads without using sensitive attributes. Our findings call for exploring fundamentally new methods for mitigating discrimination in online targeted advertising. ","Till Speicher, Muhammad Ali, Giridhari Venkatadri, Filipe Nunes Ribeiro, George Arvanitakis, Fabrício Benevenuto, Krishna P. Gummadi, Patrick Loiseau, Alan Mislove","[""MPI-SWS"", ""Northeastern University"", ""UFOP"", ""UFMG"", ""Univ. Grenoble Alpes""]",FAT* 2018,2018,FALSE
POTs: protective optimization technologies,"Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks' ability to capture a variety of harms caused by systems. We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms' inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial. We propose Protective Optimization Technologies (POTs). POTs, provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic beating applications, and recalibrating credit scoring for loan applicants.","['Bogdan Kulynych', 'Rebekah Overdorf', 'Carmela Troncoso', 'Seda Gürses']","['EPFL', 'EPFL', 'EPFL', 'TU Delft / KU Leuven']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Practical Compositional Fairness: Understanding Fairness in Multi-Component Recommender Systems,"Most literature in fairness has focused on improving fairness with respect to one single model or one single objective. However, real-world machine learning systems are usually composed of many different components. Unfortunately, recent research has shown that even if each component is ""fair"", the overall system can still be ""unfair"".  In this paper, we focus on how well fairness composes over multiple components in real systems. We consider two recently proposed fairness metrics for rankings: exposure and pairwise ranking accuracy gap. We provide theory that demonstrates a set of conditions under which fairness of individual models does compose. We then present an analytical framework for both understanding whether a system's signals can achieve compositional fairness, and diagnosing which of these signals lowers the overall system's end-to-end fairness the most. Despite previously bleak theoretical results, on multiple data-sets -- including a large-scale real-world recommender system -- we find that the overall system's end-to-end fairness is largely achievable by improving fairness in individual components.","['Xuezhi Wang', 'Nithum Thain', 'Flavien Prost', 'Ed H. Chi', 'Alex Beutel']",Google,WSDM 2021,2021,TRUE
Preference-informed fairness,"In this work, we study notions of fairness in decision-making systems when individuals have diverse preferences over the possible outcomes of the decisions. Our starting point is the seminal work of Dwork et al. [ITCS 2012] which introduced a notion of individual fairness (IF): given a task-specific similarity metric, every pair of individuals who are similarly qualified according to the metric should receive similar outcomes. We show that when individuals have diverse preferences over outcomes, requiring IF may unintentionally lead to less-preferred outcomes for the very individuals that IF aims to protect (e.g. a protected minority group). A natural alternative to IF is the classic notion of fair division, envy-freeness (EF): no individual should prefer another individual's outcome over their own. Although EF allows for solutions where all individuals receive a highly-preferred outcome, EF may also be overly-restrictive for the decision-maker. For instance, if many individuals agree on the best outcome, then if any individual receives this outcome, they all must receive it, regardless of each individual's underlying qualifications for the outcome. We introduce and study a new notion of preference-informed individual fairness (PIIF) that is a relaxation of both individual fairness and envy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style constraints, but allows for deviations provided they are in line with individuals' preferences. We show that PIIF can permit outcomes that are more favorable to individuals than any IF solution, while providing considerably more flexibility to the decision-maker than EF. In addition, we show how to efficiently optimize any convex objective over the outcomes subject to PIIF for a rich class of individual preferences. Finally, we demonstrate the broad applicability of the PIIF framework by extending our definitions and algorithms to the multiple-task targeted advertising setting introduced by Dwork and Ilvento [ITCS 2019].","['Michael P. Kim', 'Aleksandra Korolova', 'Guy N. Rothblum', 'Gal Yona']","['Stanford University', 'University of Southern California', 'Weizmann Institute of Science', 'Weizmann Institute of Science']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Preferences and Ethical Principles in Decision Making,"If we want people to trust AI systems, we need to provide the systems we create with the ability to discriminate between what humans would consider good and bad decisions. The quality of a decision should not be based only on the preferences or optimization criteria of the decision makers, but also on other properties related to the impact of the decision, such as whether it is ethical, or if it complies to constraints and priorities given by feasibility constraints or safety regulations. The CP-net formalism [2] is a convenient and expressive way to model preferences, providing an effective compact way to qualitatively model preferences over outcomes, i.e., decisions, with a combinatorial structure [3, 7]. If we wish to incorporate ethical, moral, or norms based constraints to a decision context, it means that the subjective preferences of the decision makers are not the only source of information we should consider [1, 8]. Indeed, depending on the context, we may have to consider specific ethical principles derived from an appropriate ethical theory or various laws and norms. While preferences are important, when preferences and ethical principles are in conflict, the principles should override the subjective preferences of the decision maker. Therefore, it is essential to have well founded techniques to evaluate whether preferences are compatible with a set of ethical principles, and to measure how much these preferences deviate from the ethical principles.","['Andrea Loreggia', 'Nicholas Mattei', 'Francesca Rossi', 'K. Brent Venable']","['University of Padova, Padova, Italy', 'IBM Research, Yorktown Heights NY, NY, USA', 'IBM Research and University of Padova, Yorktown Heights NY, NY, USA', 'Tulane University, New Orleans, LA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,TRUE
Price Discrimination with Fairness Constraints,"Price discrimination - offering different prices to different customers - has become common practice. While it allows sellers to increase their profits, it also raises several concerns in terms of fairness. This topic has received extensive attention from media, industry, and regulatory agencies. In this paper, we consider the problem of setting prices for different groups under fairness constraints. In this paper, we propose a formal framework for pricing with fairness, including several definitions of fairness and their potential impact on consumers, sellers, and society at large. In a first step towards the ambitious agenda of designing pricing strategies that are fair, we consider the simplest scenario of a single-product seller facing consumers who can be partitioned into two groups based on a single, binary feature observable to the seller. For each group, we assume that the seller knows the valuation distribution and the population size. The seller's goal is to maximize profit by optimally selecting a price for each group, subject to a fairness constraint which may be self-imposed or explicitly enforced by laws and regulations. We first propose four definitions: fairness in price, demand, consumer surplus, and no-purchase valuation. With our model and definitions in place, we first show that satisfying all four fairness goals simultaneously is impossible unless the mean valuations are the same for both groups. In fact, even achieving two fairness measures simultaneously cannot be done in basic settings. We then consider the impact of imposing each fairness criterion separately, and identify conditions under which the consumer surplus and the social welfare increase or decrease. Under linear or exponential demand, we show that imposing a small amount of fairness in price or no-purchase valuation increases social welfare, whereas fairness in demand or surplus reduces social welfare. We fully characterize the impact of imposing different types of fairness for linear demand. We discover that imposing too much price fairness may result in a lower social welfare relative to imposing no price fairness. Imposing demand and surplus fairness always decreases social welfare. However, imposing no-purchase valuation fairness always increases social welfare. We also extend our results to the cases when there are multiple groups or there is an unprotected feature. Finally, we computationally show that most of our findings continue to hold for three common nonlinear demand models. Our results and insights provide a first step in understanding the impact of imposing fairness in the context of pricing.","['Maxime C. Cohen', 'Adam N. Elmachtoub', 'Xiao Lei']","['Desautels Faculty of Management, McGill University, Montreal, Quebec, Canada', 'Department of Industrial Engineering and Operations Research & Data Science Institute, Columbia University, New York, New York, USA', 'Department of Industrial Engineering and Operations Research, Columbia University, New York, New York, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Privacy Amplification by Iteration,"Most commonly used learning algorithms work by iteratively updating an intermediate solution using one or a few data points in each iteration.  Analysis of differential privacy for such algorithms often involves ensuring privacy of each step and then reasoning about the cumulative privacy cost of the algorithm. This is enabled by composition theorems for differential privacy that allow releasing of all the intermediate results.  In this work, we demonstrate that for contractive iterations, not releasing the intermediate results strongly amplifies the privacy guarantees.
",[],Google,2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS),2018,TRUE
Privacy Amplification via Random Check-Ins,"Differentially Private Stochastic Gradient Descent (DP-SGD) forms a fundamental building block in many applications for learning over sensitive data. Two standard approaches, privacy amplification by subsampling, and privacy amplification by shuffling, permit adding lower noise in DP-SGD than via na\""&lbrace;\i&rbrace;ve schemes. A key assumption in both these approaches is that the elements in the data set can be uniformly sampled, or be uniformly permuted ---  constraints that may become prohibitive when the data is processed in a decentralized or distributed fashion. In this paper, we focus on conducting iterative methods like DP-SGD in the setting of federated learning (FL) wherein the data is distributed among many devices (clients). Our main contribution is the random check-in distributed protocol, which crucially relies only on randomized participation decisions made locally and independently by each client. It has privacy/accuracy trade-offs similar to privacy amplification by subsampling/shuffling. However, our method does not require server-initiated communication, or even knowledge of the population size. To our knowledge, this is the first privacy amplification tailored for a distributed learning framework, and it may have broader applicability beyond FL. Along the way, we extend privacy amplification by shuffling to incorporate $(\epsilon,\delta)$-DP local randomizers, and exponentially improve its guarantees. In practical regimes, this improvement allows for similar privacy and utility using data from an order of magnitude fewer users.","['Peter Kairouz', 'H. Brendan McMahan', 'Om Thakkar']",Google,"Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020",2020,TRUE
Privacy Dependencies,"This Article offers a comprehensive survey of privacy dependencies—the many ways that our privacy depends on the decisions and disclosures of other people. What
we do and what we say can reveal as much about others as it does about ourselves, even when we don’t realize it or when we think we’re sharing information about ourselves alone.
We identify three bases upon which our privacy can depend: our social ties, our similarities to others, and our differences from others. In a tie-based dependency, an observer learns
about one person by virtue of her social relationships with others—family, friends, or other associates. In a similarity-based dependency, inferences about our unrevealed attributes are
drawn from our similarities to others for whom that attribute is known. And in difference-based dependencies, revelations about ourselves demonstrate how we are different from
others—by showing, for example, how we “break the mold” of normal behavior or establishing how we rank relative to others with respect to some desirable attribute. We
elaborate how these dependencies operate, isolating the relevant mechanisms and providing concrete examples of each mechanism in practice, the values they implicate, and the legal
and technical interventions that may be brought to bear on them. Our work adds to a growing chorus demonstrating that privacy is neither an individual choice nor an individual value—
but it is the first to systematically demonstrate how different types of dependencies can raise very different normative concerns, implicate different areas of law, and create different
challenges for regulation.","['Solon Barocas', 'Karen Levy']",Microsoft,Washington Law Review,2020-06-01,TRUE
Privacy for All: Ensuring Fair and Equitable Privacy Protections,"In this position paper, we argue for applying recent research on ensuring sociotechnical systems are fair and non-discriminatory to the privacy protections those systems may provide. Privacy literature seldom considers whether a proposed privacy scheme protects all persons uniformly, irrespective of membership in protected classes or particular risk in the face of privacy failure. Just as algorithmic decision-making systems may have discriminatory outcomes even without explicit or deliberate discrimination, so also privacy regimes may disproportionately fail to protect vulnerable members of their target population, resulting in disparate impact with respect to the effectiveness of privacy protections.We propose a research agenda that will illuminate this issue, along with related issues in the intersection of fairness and privacy, and present case studies that show how the outcomes of this research may change existing privacy and fairness research. We believe it is important to ensure that technologies and policies intended to protect the users and subjects of information systems provide such protection in an equitable fashion. ","Michael D. Ekstrand, Rezvan Joshaghani, Hoda Mehrpouyan",Boise State University,FAT* 2018,2018,FALSE
Privacy in Geospatial Applications and Location-Based Social Networks,"The use of location data has greatly benefited from the availability of location-based services, the popularity of social networks, and the accessibility of public location data sets. However, in addition to providing users with the ability to obtain accurate driving directions or the convenience of geo-tagging friends and pictures, location is also a very sensitive type of data, as attested by more than a decade of research on different aspects of privacy related to location data.",['Igor Bilogrevic'],Google,Handbook of Mobile Data Privacy (2018),2018,TRUE
Privacy-Preserving Gaussian Process Regression - a Modular Approach to the Application of Fully Homomorphic Encryption,"Much of machine learning relies on the use of large amounts of data to train models to make predictions. When this data comes from multiple sources, for example when evaluation of data against a machine learning model is offered as a service, there can be privacy issues and legal concerns over the sharing of data. Fully homomorphic encryption (FHE) allows data to be computed on whilst encrypted, which can provide a solution to the problem of data privacy. However, FHE is both slow and restrictive, so existing algorithms must be manipulated to make them work efficiently under the FHE paradigm. Some commonly used machine learning algorithms, such as Gaussian process regression, are poorly suited to FHE and cannot be manipulated to work both efficiently and accurately. In this paper, we show that a modular application of FHE, in which an algorithm is broken into small steps and each step is performed under FHE only if the data used by or results produced by that step are sensitive, allows one party to make predictions on their data using a Gaussian process regression model built from another partyâ€™s data, without either party gaining access to the otherâ€™s data, in a way which is both accurate and efficient. This construction is, to our knowledge, the first example of an effectively encrypted Gaussian process.","Peter Fenner, EDWARD PYZER-KNAPP",IBM,AAAI (2020),2020,TRUE
Privacy-Preserving Machine Learning Based Data Analytics on Edge Devices,"Emerging Machine Learning (ML) techniques, such as Deep Neural Network, are widely used in today's applications and services. However, with social awareness of privacy and personal data rapidly rising, it becomes a pressing and challenging societal issue to both keep personal data private and benefit from the data analytics power of ML techniques at the same time. In this paper, we argue that to avoid those costs, reduce latency in data processing, and minimise the raw data revealed to service providers, many future AI and ML services could be deployed on users' devices at the Internet edge rather than putting everything on the cloud. Moving ML-based data analytics from cloud to edge devices brings a series of challenges. We make three contributions in this paper. First, besides the widely discussed resource limitation on edge devices, we further identify two other challenges that are not yet recognised in existing literature: lack of suitable models for users, and difficulties in deploying services for users. Second, we present preliminary work of the first systematic solution, i.e. Zoo, to fully support the construction, composing, and deployment of ML models on edge and local devices. Third, in the deployment example, ML service are proved to be easy to compose and deploy with Zoo. Evaluation shows its superior performance compared with state-of-art deep learning platforms and Google ML services.","['Jianxin Zhao', 'Richard Mortier', 'Jon Crowcroft', 'Liang Wang']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Privacy-preserving Prediction,"Ensuring differential privacy of models learned from sensitive user data is an important goal that has been studied extensively in recent years.
It is now known that for some basic learning problems, especially those involving high-dimensional data, producing an accurate private model requires much more data than learning without privacy. At the same time, in many applications it is not necessary to expose the model itself. Instead users may be allowed to query the prediction model on their inputs only through an appropriate interface. Here we formulate the problem of ensuring privacy of individual predictions and investigate the overheads required to achieve it in several standard models of classification and regression.",[],Google,Conference on Learning Theory (COLT) 2018,2018,TRUE
Private Hypothesis Selection,"We provide a differentially private algorithm for hypothesis selection. Given samples from an unknown probability distribution P and a set of m probability distributions , the goal is to output, in a ε-differentially private manner, a distribution from  whose total variation distance to P is comparable to that of the best such distribution (which we denote by α). The sample complexity of our basic algorithm is O(logmα2+logmαε), representing a minimal cost for privacy when compared to the non private algorithm. We also can handle infinite hypothesis classes  by relaxing to (ε,δ)-differential privacy. We apply our hypothesis selection algorithm to give learning algorithms for a number of natural distribution classes, including Gaussians, product distributions, sums of independent random variables, piecewise polynomials, and mixture classes. Our hypothesis selection procedure allows us to generically convert a cover for a class to a learning algorithm, complementing known learning lower bounds which are in terms of the size of the packing number of the class. As the covering and packing numbers are often closely related, for constant α, our algorithms achieve the optimal sample complexity for many classes of interest. Finally, we describe an application to private distribution-free PAC learning.","Mark Bun, Gautam Kamath, Thomas Steinke, Zhiwei Steven Wu",IBM,NeurIPS (2019),2019,TRUE
Problem Formulation and Fairness,"Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team---and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases---we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways---and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.","['Samir Passi', 'Solon Barocas']","['Information Science, Cornell University', 'Information Science, Cornell University']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Proportionally Fair Clustering,"We extend the fair machine learning literature by considering the problem of proportional centroid clustering in a metric context. For clustering n points with k centers, we define fairness as proportionality to mean that any n/k points are entitled to form their own cluster if there is another center that is closer in distance for all n/k points. We seek clustering solutions to which there are no such justified complaints from any subsets of agents, without assuming any a priori notion of protected subsets. We present and analyze algorithms to efficiently compute, optimize, and audit proportional solutions. We conclude with an empirical examination of the tradeoff between proportional solutions and the k-means objective.","Xingyu Chen, Brandon Fain, Liang Lyu, Kamesh Munagala",Duke University,ICML 2019,2019-06,FALSE
Proposal for Type Classification for Building Trust in Medical Artificial Intelligence Systems,"This paper proposes the establishment of Medical Artificial Intelligence (AI) Types (MA Types)""that classify AI in medicine not only by technical system requirements but also implications to healthcare workers' roles and users/patients. MA Types can be useful to promote discussion regarding the purpose and application of the clinical site. Although MA Types are based on the current technologies and regulations in Japan, but that does not hinder the potential reform of the technologies and regulations. MA Types aims to facilitate discussions among physicians, healthcare workers, engineers, public/patients and policymakers on AI systems in medical practices.","['Arisa Ema', 'Katsue Nagakura', 'Takanori Fujita']","['The University of Tokyo, Tokyo, Japan', 'M3, Inc, Tokyo, Japan', 'Keio University, Tokyo, Japan']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
Protecting Intellectual Property of Deep Neural Networks with Watermarking,"Deep learning technologies, which are the key components of state-of-the-art Artificial Intelligence (AI) services, have shown great success in providing human-level capabilities for a variety of tasks, such as visual analysis, speech recognition, and natural language processing and etc. Building a production-level deep learning model is a non-trivial task, which requires a large amount of training data, powerful computing resources, and human expertises. Therefore, illegitimate reproducing, distribution, and the derivation of proprietary deep learning models can lead to copyright infringement and economic harm to model creators. Therefore, it is essential to devise a technique to protect the intellectual property of deep learning models and enable external verification of the model ownership. In this paper, we generalize the digital watermarking' concept from multimedia ownership verification to deep neural network (DNNs) models. We investigate three DNN-applicable watermark generation algorithms, propose a watermark implanting approach to infuse watermark into deep learning models, and design a remote verification mechanism to determine the model ownership. By extending the intrinsic generalization and memorization capabilities of deep neural networks, we enable the models to learn specially crafted watermarks at training and activate with pre-specified predictions when observing the watermark patterns at inference. We evaluate our approach with two image recognition benchmark datasets. Our framework accurately (100%) and quickly verifies the ownership of all the remotely deployed deep learning models without affecting the model accuracy for normal input data. In addition, the embedded watermarks in DNN models are robust and resilient to different counter-watermark mechanisms, such as fine-tuning, parameter pruning, and model inversion attacks.","J. Zhang, Z. Gu, J. Jang, H. Wu, M. Ph. Stoecklin, H. Huang and I. Molloy",IBM,ASIACCS (2018),2018,TRUE
Protecting Neural Networks with Hierarchical Random Switching: Towards Better Robustness-Accuracy Trade-off for Stochastic Defenses,"Despite achieving remarkable success in various domains, recent studies have uncovered the vulnerability of deep neural networks to adversarial perturbations, creating concerns on model generalizability and new threats such as prediction-evasive misclassification or stealthy reprogramming. Among different defense proposals, stochastic network defenses such as random neuron activation pruning or random perturbation to layer inputs are shown to be promising for attack mitigation. However, one critical drawback of current defenses is that the robustness enhancement is at the cost of noticeable performance degradation on legitimate data, e.g., a large drop in test accuracy.This paper is motivated by pursuing for a better trade-off between adversarial robustness and test accuracy for stochastic network defenses. We propose Defense Efficiency Score (DES), a comprehensive metric that measures the gain in unsuccessful attack attempts at the cost of a drop in test accuracy of any defense. To achieve a better DES, we propose hierarchical random switching (HRS), which protects neural networks through a novel randomization scheme. A HRS-protected model contains several blocks of randomly switching channels to prevent adversaries from exploiting fixed model structures and parameters for their malicious purposes. Extensive experiments show that HRS is superior in defending against state-of-the-art white-box and adaptive adversarial misclassification attacks. We also demonstrate the effectiveness of HRS in defending adversarial reprogramming, which is the first defense against adversarial programs. Moreover, in most settings the average DES of HRS is at least 5 X higher than current stochastic network defenses, validating its significantly improved robustness-accuracy trade-off.","Xiao Wang, Siyue Wang , Pin-yu Chen, Yanzhi Wang, Brian Kulis, Xue Lin, Sang Chin",IBM,IJCAI (2019),2019,TRUE
Provenance in Context of Hadoop as a Service (HaaS) - State of the Art and Research Directions,"Hadoop as a service (HaaS), also known as Hadoop in the cloud, is a big data analytics framework that stores and analyzes data in the cloud using Hadoop/Spark. In this paper, we discuss the importance of providing provenance capabilities in context of Hadoop as a service (HaaS) framework. We first review the state of the art in provenance tracking in context of databases and work-flow processing, in context of cloud and in context of big data analytics frameworks like Hadoop and Spark. We next identify a number of provenance capabilities which have been developed in context of databases and workflow processing but the corresponding solutions have not been developed in context of Hadoop or Spark. We argue that developing these solutions is important so that a comprehensive provenance aware Hadoop as a Service (HaaS) can be provided on cloud. The paper ends by identifying some research challenges in developing these provenance capabilities.","H. Gupta, S. Mehta, S. Hans, B. Chatterjee, P. Lohia and C. Rajmohan",IBM,IEEE (2017),2017,TRUE
Purple Feed: Identifying High Consensus News Posts on Social Media,"Although diverse news stories are actively posted on social media, readers often focus on the news which reinforces their pre-existing views, leading to 'filter bubble' effects. To combat this, some recent systems expose and nudge readers toward stories with different points of view. One example is the Wall Street Journal's 'Blue Feed, Red Feed' system, which presents posts from biased publishers on each side of a topic. However, these systems have had limited success. We present a complementary approach which identifies high consensus 'purple' posts that generate similar reactions from both 'blue' and 'red' readers. We define and operationalize consensus for news posts on Twitter in the context of US politics. We show that high consensus posts can be identified and discuss their empirical properties. We present a method for automatically identifying high and low consensus news posts on Twitter, which can work at scale across many publishers. To do this, we propose a novel category of audience leaning based features, which we show are well suited to this task. Finally, we present our 'Purple Feed' system which highlights high consensus posts from publishers on both sides of the political spectrum.","['Mahmoudreza Babaei', 'Juhi Kulshrestha', 'Abhijnan Chakraborty', 'Fabrício Benevenuto', 'Krishna P. Gummadi', 'Adrian Weller']","['Max Planck Institute for Software Systems, Saarbrucken, Germany', 'Max Planck Institute for Software Systems, Saarbrucken, Germany', 'Max Planck Institute for Software Systems & IIT Kharagpur, India, Saarbrucken, Germany', 'Universidade Federal de Minas Gerais, Belo Horizonte, Brazil', 'Max Planck Institute for Software Systems, Saarbruecken, Germany', 'University of Cambridge & Alan Turing Institute, Cambridge, United Kingdom']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
"Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements","As more researchers have become aware of and passionate about algorithmic fairness, there has been an explosion in papers laying out new metrics, suggesting algorithms to address issues, and calling attention to issues in existing applications of machine learning. This research has greatly expanded our understanding of the concerns and challenges in deploying machine learning, but there has been much less work in seeing how the rubber meets the road. In this paper we provide a case-study on the application of fairness in machine learning research to a production classification system, and offer new insights in how to measure and address algorithmic fairness issues. We discuss open questions in implementing equality of opportunity and describe our fairness metric, conditional equality, that takes into account distributional differences. Further, we provide a new approach to improve on the fairness metric during model training and demonstrate its efficacy in improving performance for a real-world product.","['Alex Beutel', 'Jilin Chen', 'Tulsee Doshi', 'Hai Qian', 'Allison Woodruff', 'Christine Luu', 'Pierre Kreitmann', 'Jonathan Bischof', 'Ed H. Chi']","['Google, New York, NY, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, San Bruno, CA, USA', 'Google, Seattle, WA, USA', 'Google, Mountain View, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,TRUE
Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach,"We study the problem of attacking a machine learning model in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (e.g., CW or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only current approach is based on random walk on the boundary, which requires lots of queries and lacks convergence guarantees. We propose a novel way to formulate the hard-label black-box attack as a real-valued optimization problem which is usually continuous and can be solved by any zeroth order optimization algorithm. For example, using the Randomized Gradient-Free method, we are able to bound the number of iterations needed for our algorithm to achieve stationary points. We demonstrate that our proposed method outperforms the previous random walk approach to attacking convolutional neural networks on MNIST, CIFAR, and ImageNet datasets. More interestingly, we show that the proposed algorithm can also be used to attack other discrete and non-continuous machine learning models, such as Gradient Boosting Decision Trees (GBDT).","Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, Cho-Jui Hsieh ",IBM,ICLR (2019),2019,TRUE
Racial categories in machine learning,"Controversies around race and machine learning have sparked debate among computer scientists over how to design machine learning systems that guarantee fairness. These debates rarely engage with how racial identity is embedded in our social experience, making for sociological and psychological complexity. This complexity challenges the paradigm of considering fairness to be a formal property of supervised learning with respect to protected personal attributes. Racial identity is not simply a personal subjective quality. For people labeled ""Black"" it is an ascribed political category that has consequences for social differentiation embedded in systemic patterns of social inequality achieved through both social and spatial segregation. In the United States, racial classification can best be understood as a system of inherently unequal status categories that places whites as the most privileged category while signifying the Negro/black category as stigmatized. Social stigma is reinforced through the unequal distribution of societal rewards and goods along racial lines that is reinforced by state, corporate, and civic institutions and practices. This creates a dilemma for society and designers: be blind to racial group disparities and thereby reify racialized social inequality by no longer measuring systemic inequality, or be conscious of racial categories in a way that itself reifies race. We propose a third option. By preceding group fairness interventions with unsupervised learning to dynamically detect patterns of segregation, machine learning systems can mitigate the root cause of social disparities, social segregation and stratification, without further anchoring status categories of disadvantage.","['Sebastian Benthall', 'Bruce D. Haynes']","['New York University', 'University of California, Davis']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations,"We introduce \em AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation.","['Upol Ehsan', 'Brent Harrison', 'Larry Chan', 'Mark O. Riedl']","['Georgia Institute of Technology, Atlanta, GA, USA', 'University of Kentucky, Lexington, KY, USA', 'Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Re-imagining Algorithmic Fairness in India and Beyond,"Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.","['Nithya Sambasivan', 'Erin Arnesen', 'Ben Hutchinson', 'Tulsee Doshi', 'Vinodkumar Prabhakaran']","['Google Research Mountain View, CA', 'Google Research Mountain View, CA', 'Google Research Mountain View, CA', 'Google Research Mountain View, CA', 'Google Research Mountain View, CA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Real-Time Inference of User Types to Assist with more Inclusive and Diverse Social Media Activism Campaigns,"Social media provides a mechanism for people to engage with social causes across a range of issues. It also provides a strategic tool to those looking to advance a cause to exchange, promote or publicize their ideas. In such instances, AI can be either an asset if used appropriately or a barrier. One of the key issues for a workforce diversity campaign is to understand in real-time who is participating - specifically, whether the participants are individuals or organizations, and in case of individuals, whether they are male or female. In this paper, we present a study to demonstrate a case for AI for social good that develops a model to infer in real-time the different user types participating in a cause-driven hashtag campaign on Twitter, ILookLikeAnEngineer (ILLAE). A generic framework is devised to classify a Twitter user into three classes: organization, male and female in a real-time manner. The framework is tested against two datasets (ILLAE and a general dataset) and outperforms the baseline binary classifiers for categorizing organization/individual and male/female. The proposed model can be applied to future social cause-driven campaigns to get real-time insights on the macro-level social behavior of participants.","['Habib Karbasian', 'Hemant Purohit', 'Rajat Handa', 'Aqdas Malik', 'Aditya Johri']","['George Mason University, Fairfax, VA, USA', 'George Mason University, Fairfax, VA, USA', 'George Mason University, Fairfax, VA, USA', 'George Mason University, Fairfax, VA, USA', 'George Mason University, Fairfax, VA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
"Reasons, Values, Stakeholders: A Philosophical Framework for Explainable Artificial Intelligence","The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholders, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate nearly prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions, recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations, and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.",['Atoosa Kasirzadeh'],['University of Toronto Australian National University'],"FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Recommendation Independence,"This paper studies a recommendation algorithm whose outcomes are not influenced by specified information. It is useful in contexts potentially unfair decision should be avoided, such as job-applicant recommendations that are not influenced by socially sensitive information. An algorithm that could exclude the influence of sensitive information would thus be useful for job-matching with fairness. We call the condition between a recommendation outcome and a sensitive feature Recommendation Independence, which is formally defined as statistical independence between the outcome and the feature. Our previous independence-enhanced algorithms simply matched the means of predictions between sub-datasets consisting of the same sensitive value. However, this approach could not remove the sensitive information represented by the second or higher moments of distributions. In this paper, we develop new methods that can deal with the second moment, i.e., variance, of recommendation outcomes without increasing the computational complexity. These methods can more strictly remove the sensitive information, and experimental results demonstrate that our new algorithms can more effectively eliminate the factors that undermine fairness. Additionally, we explore potential applications for independence-enhanced recommendation, and discuss its relation to other concepts, such as recommendation diversity.","Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, Jun Sakuma","[""AIST"", ""University of Tsukuba""]",FAT* 2018,2018,FALSE
Recommendations and user agency: the reachability of collaboratively-filtered information,"Recommender systems often rely on models which are trained to maximize accuracy in predicting user preferences. When the systems are deployed, these models determine the availability of content and information to different users. The gap between these objectives gives rise to a potential for unintended consequences, contributing to phenomena such as filter bubbles and polarization. In this work, we consider directly the information availability problem through the lens of user recourse. Using ideas of reachability, we propose a computationally efficient audit for top-N linear recommender models. Furthermore, we describe the relationship between model complexity and the effort necessary for users to exert control over their recommendations. We use this insight to provide a novel perspective on the user cold-start problem. Finally, we demonstrate these concepts with an empirical investigation of a state-of-the-art model trained on a widely used movie ratings dataset.","['Sarah Dean', 'Sarah Rich', 'Benjamin Recht']","['UC Berkeley', 'Canopy Crest', 'UC Berkeley']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Reducing sentiment polarity for demographic attributes in word embeddings using adversarial learning,"The use of word embedding models in sentiment analysis has gained a lot of traction in the Natural Language Processing (NLP) community. However, many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment, resulting in unfair downstream machine learning algorithms. We leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment, and re-embed them into the word embeddings. We show that our method effectively minimizes unfair positive/negative sentiment polarity while retaining the semantic accuracy of the word embeddings. Furthermore, we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks.","['Chris Sweeney', 'Maryam Najafian']","['MIT', 'MIT']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Regulating Artificial Intelligence: Proposal for a Global Solution,"Given the ubiquity of artificial intelligence (AI) in modern societies, it is clear that individuals, corporations, and countries will be grappling with the legal and ethical issues of its use. As global problems require global solutions, we propose the establishment of an international AI regulatory agency that --- drawing on interdisciplinary expertise --- could create a unified framework for the regulation of AI technologies and inform the development of AI policies around the world. We urge that such an organization be developed with all deliberate haste, as issues such as cryptocurrencies, personalized political ad hacking, autonomous vehicles and autonomous weaponized agents are already a reality, affecting international trade, politics, and war.","['Olivia J. Erdélyi', 'Judy Goldsmith']","['University of Canterbury, Christchurch, New Zealand', 'University of Kentucky, Lexington, KY, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Regulating Autonomous Vehicles: A Policy Proposal,"The widespread deployment and testing of autonomous vehicles in real-world environments raises key questions about how such systems should be regulated. Much of the current debate presupposes that the regulatory system we currently use for regular vehicles is also appropriate for semi- and fully-autonomous ones. In opposition, we first argue that there are serious challenges to regulating autonomous vehicles using current approaches, due to the nature of both autonomous capabilities (and their connections to operational domains), and also the systems' tasks and surrounding uncertainties. Instead, we argue that vehicles with autonomous capabilities are similar in key respects to drugs and other medical inter-ventions. Thus, we propose (on a ""first principles"" basis) a dynamic regulatory system with staged approvals and monitoring, analogous to the system used by the U.S. Food & Drug Administration. We provide details about the operation of such a potential system, and conclude by characterizing its benefits, costs, and plausibility.","['Alex John London', 'David Danks']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Regulating for 'Normal AI Accidents': Operational Lessons for the Responsible Governance of Artificial Intelligence Deployment,"New technologies, particularly those which are deployed rapidly across sectors, or which have to operate in competitive conditions, can disrupt previously stable technology governance regimes. This leads to a precarious need to balance caution against performance while exploring the resulting 'safe operating space'. This paper will argue that Artificial Intelligence is one such critical technology, the responsible deployment of which is likely to prove especially complex, because even narrow AI applications often involve networked (tightly coupled, opaque) systems operating in complex or competitive environments. This ensures such systems are prone to 'normal accident'-type failures which can cascade rapidly, and are hard to contain or even detect in time. Legal and governance approaches to the deployment of AI will have to reckon with the specific causes and features of such 'normal accidents'. While this suggests that large-scale, cascading errors in AI systems are inevitable, an examination of the operational features that lead technologies to exhibit 'normal accidents' enables us to derive both tentative principles for precautionary policymaking, and practical recommendations for the safe(r) deployment of AI systems. This may help enhance the safety and security of these systems in the public sphere, both in the short- and in the long term.",['Matthijs M. Maas'],"['University of Copenhagen, Copenhagen, Denmark']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Regulating Lethal and Harmful Autonomy: Drafting a Protocol VI of the Convention on Certain Conventional Weapons,"This short paper provides two partial drafts for a Protocol VI that might be added to the existing five Protocols of the Convention on Certain Conventional Weapons (CCW) to regulate ""lethal autonomous weapons systems"" (LAWS). Draft A sets the line of tolerance at a ""human in the loop"" between the critical functions of select and engage. Draft B sets the line of tolerance at a human in the ""wider loop"" that includes the critical function of defining target classes as well as select and engage. Draft A represents an interpretation of what NGOs such as the Campaign to Stop Killer Robots are seeking to get enacted. Draft B is a more cautious draft based on the Dutch concept of ""meaningful human control in the wider loop"" that does not seek to ban any system that currently exists. Such a draft may be more likely to achieve the consensus required by the UN CCW process. A list of weapons banned by both drafts is provided along with the rationale for each draft. The drafts are intended to stimulate debate on the precise form a binding instrument on LAWS would take and on what LAWS (if any) should be banned and why.",['Sean Welsh'],"['University of Canterbury, Christchurch, New Zealand']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
"Regulating transparency?: Facebook, Twitter and the German Network Enforcement Act","Regulatory regimes designed to ensure transparency often struggle to ensure that transparency is meaningful in practice. This challenge is particularly great when coupled with the widespread usage of dark patterns --- design techniques used to manipulate users. The following article analyses the implementation of the transparency provisions of the German Network Enforcement Act (NetzDG) by Facebook and Twitter, as well as the consequences of these implementations for the effective regulation of online platforms. This question of effective regulation is particularly salient, due to an enforcement action in 2019 by Germany's Federal Office of Justice (BfJ) against Facebook for what the BfJ claim were insufficient compliance with transparency requirements, under NetzDG. This article provides an overview of the transparency requirements of NetzDG and contrasts these with the transparency requirements of other relevant regulations. It will then discuss how transparency concerns not only providing data, but also how the visibility of the data that is made transparent is managed, by deciding how the data is provided and is framed. We will then provide an empirical analysis of the design choices made by Facebook and Twitter, to assess the ways in which their implementations differ. The consequences of these two divergent implementations on interface design and user behaviour are then discussed, through a comparison of the transparency reports and reporting mechanisms used by Facebook and Twitter. As a next step, we will discuss the BfJ's consideration of the design of Facebook's content reporting mechanisms, and what this reveals about their respective interpretations of NetzDG's scope. Finally, in recognising that this situation is one in which a regulator is considering design as part of their action - we develop a wider argument on the potential for regulatory enforcement around dark patterns, and design practices more generally, for which this case is an early, indicative example.","['Ben Wagner', 'Krisztina Rozgonyi', 'Marie-Therese Sekwenz', 'Jennifer Cobbe', 'Jatinder Singh']","['Vienna University of Economics and Business', 'University of Vienna', 'Vienna University of Economics and Business', 'University of Cambridge', 'University of Cambridge']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Reinforcement Learning and Inverse Reinforcement Learning with System 1 and System 2,"Inferring a person's goal from their behavior is an important problem in applications of AI (e.g. automated assistants, recommender systems). The workhorse model for this task is the rational actor model - this amounts to assuming that people have stable reward functions, discount the future exponentially, and construct optimal plans. Under the rational actor assumption techniques such as inverse reinforcement learning (IRL) can be used to infer a person's goals from their actions. A competing model is the dual-system model. Here decisions are the result of an interplay between a fast, automatic, heuristic-based system 1 and a slower, deliberate, calculating system 2. We generalize the dual system framework to the case of Markov decision problems and show how to compute optimal plans for dual-system agents. We show that dual-system agents exhibit behaviors that are incompatible with rational actor assumption. We show that naive applications of rational-actor IRL to the behavior of dual-system agents can generate wrong inference about the agents' goals and suggest interventions that actually reduce the agent's overall utility. Finally, we adapt a simple IRL algorithm to correctly infer the goals of dual system decision-makers. This allows us to make interventions that help, rather than hinder, the dual-system agent's ability to reach their true goals.",['Alexander Peysakhovich'],"['Facebook AI Research, New York, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,TRUE
ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring,"We improve the recently-proposed ``MixMatch'' semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring. Distribution alignment encourages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of groundtruth labels. Augmentation anchoring feeds multiple strongly augmented versions of an input into the model and encourages each output to be close to the prediction for a weakly-augmented version of the same input. To produce strong augmentations, we propose a variant of AutoAugment which learns the augmentation policy while the model is being trained. Our new algorithm, dubbed ReMixMatch, is significantly more data-efficient than prior work, requiring between 5x and 16x less data to reach the same accuracy. For example, on CIFAR10 with 250 labeled examples we reach 93.73% accuracy (compared to MixMatchâs accuracy of 93.58% with 4,000 examples) and a median accuracy of 84.92% with just four labels per class. We make our code and data open-source at https://github.com/google-research/remixmatch.","['Alex Kurakin', 'Ekin Dogus Cubuk', 'Kihyuk Sohn', 'Nicholas Carlini']",Google,ICLR (2020),2020,TRUE
Removing Spurious Features can Hurt Accuracy and Affect Groups Disproportionately,"Spurious features interfere with the goal of obtaining robust models that perform well across many groups within the population. A natural remedy is to remove such features from the model. However, in this work, we show that removing spurious features can surprisingly decrease accuracy due to the inductive biases of overparameterized models. In noiseless overparameterized linear regression, we completely characterize how the removal of spurious features affects accuracy across different groups (more generally, test distributions). In addition, we show that removal of spurious features can decrease the accuracy even on balanced datasets (where each target co-occurs equally with each spurious feature); and it can inadvertently make the model more susceptible to other spurious features. Finally, we show that robust self-training produces models that no longer depend on spurious features without affecting their overall accuracy. The empirical results on the Toxic-Comment-Detection and CelebA datasets show that our results hold in non-linear models.","['Fereshte Khani', 'Percy Liang']","['Stanford University', 'Stanford University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Repairing without Retraining: Avoiding Disparate Impact with Counterfactual Distributions,"When the performance of a machine learning model varies over groups defined by sensitive attributes (e.g., gender or ethnicity), the performance disparity can be expressed in terms of the probability distributions of the input and output variables over each group. In this paper, we exploit this fact to reduce the disparate impact of a fixed classification model over a population of interest. Given a black-box classifier, we aim to eliminate the performance gap by perturbing the distribution of input variables for the disadvantaged group. We refer to the perturbed distribution as a counterfactual distribution, and characterize its properties for common fairness criteria. We introduce a descent algorithm to learn a counterfactual distribution from data. We then discuss how the estimated distribution can be used to build a data preprocessor that can reduce disparate impact without training a new model. We validate our approach through experiments on real-world datasets, showing that it can repair different forms of disparity without a significant drop in accuracy. ","Hao Wang, Berk Ustun, Flavio Calmon",Harvard University,ICML 2019,2019-06,FALSE
"Representativeness in Statistics, Politics, and Machine Learning","Representativeness is a foundational yet slippery concept. Though familiar at first blush, it lacks a single precise meaning. Instead, meanings range from typical or characteristic, to a proportionate match between sample and population, to a more general sense of accuracy, generalizability, coverage, or inclusiveness. Moreover, the concept has long been contested. In statistics, debates about the merits and methods of selecting a representative sample date back to the late 19th century; in politics, debates about the value of likeness as a logic of political representation are older still. Today, as the concept crops up in the study of fairness and accountability in machine learning, we need to carefully consider the term's meanings in order to communicate clearly and account for their normative implications. In this paper, we ask what representativeness means, how it is mobilized socially, and what values and ideals it communicates or confronts. We trace the concept's history in statistics and discuss normative tensions concerning its relationship to likeness, exclusion, authority, and aspiration. We draw on these analyses to think through how representativeness is used in FAccT debates, with emphasis on data, shift, participation, and power.","['Kyla Chasalow', 'Karen Levy']","['Cornell University', 'Cornell University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Requirements for an Artificial Agent with Norm Competence,"Human behavior is frequently guided by social and moral norms, and no human community can exist without norms. Robots that enter human societies must therefore behave in norm-conforming ways as well. However, currently there is no solid cognitive or computational model available of how human norms are represented, activated, and learned. We provide a conceptual and psychological analysis of key properties of human norms and identify the demands these properties put on any artificial agent that incorporates norms-demands on the format of norm representations, their structured organization, and their learning algorithms.","['Bertram F. Malle', 'Paul Bello', 'Matthias Scheutz']","['Brown University, Providence, RI, USA', 'U.S. Naval Research Laboratory, Washington, DC, USA', 'Tufts University, Medford, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Reshaping Diverse Planning,"The need for multiple plans has been established by various planning applications. In some, solution quality has the predominant role, while in others diversity is the key factor. Most recent work takes both plan quality and solution diversity into account under the generic umbrella of diverse planning. There is no common agreement, however, on a collection of computational problems that fall under that generic umbrella. This in particular might lead to a comparison between planners that have different solution guarantees or optimization criteria in mind. In this work we revisit diverse planning literature in search of such a collection of computational problems, classifying the existing planners to these problems. We formally define a taxonomy of computational problems with respect to both plan quality and solution diversity, extending the existing work. We propose a novel approach to diverse planning, exploiting existing classical planners via planning task reformulation and choosing a subset of plans of required size in post-processing. Based on that, we present planners for two computational problems, that most existing planners solve. Our experiments show that the proposed approach significantly improves over the best performing existing planners in terms of coverage, the overall solution quality, and the overall diversity according to various diversity metrics.","Michael Katz, Shirin Sohrabi",IBM,AAAI (2020),2020,TRUE
Rethinking AI Strategy and Policy as Entangled Super Wicked Problems,"This paper attempts a preliminary analysis of the general approach to AI strategy/policy research through the lens of wicked problems literature. Wicked problems are a class of social policy problems for which traditional methods of resolution fail. Super wicked problems refer to even more complex social policy problems, e.g. climate change. We first propose a hierarchy of three classes of AI strategy/policy problems, all wicked or super wicked problems. We next identify three independent super wicked problems in AI strategy/policy and propose that the most significant of these challenges - the development of safe and beneficial artificial general intelligence - to be significantly more complex and nuanced, thus posing a new degree of 'wickedness.' We then explore analysis and techniques for addressing wicked problems and super wicked problems. This leads to a discussion of the implications of these ideas on the problems of AI strategy/policy.",['Ross Gruetzemacher'],"['Auburn University, Auburn, AL, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Revealing Semantic Structures of Texts: Multi-grained Framework for Automatic Mind-map Generation,"A mind-map is a diagram used to represent ideas linked to and arranged around a central concept.  Itâ€™s easier to visually access the knowledge and ideas by converting a text to a mind-map. However, highlighting the semantic skeleton of an article remains a challenge. The key issue is to detect the relations amongst concepts beyond intra-sentence. In this paper, we propose a multi-grained framework for automatic mind-map generation. That is, a novel neural network is taken to detect the relations at first, which employs multi-hop self-attention and gated recurrence network to reveal the directed semantic relations via sentences. A recursive algorithm is then designed to select the most salient sentences to constitute the hierarchy. The human-like mind-map is automatically constructed with the key phrases in the salient sentences. Promising results have been achieved on the comparison with manual mind-maps. The case studies demonstrate that the generated mind-maps reveal the underlying semantic structures of the articles.","Yang Wei, Hong Lei Guo, Jinmao Wei, Zhong Su",IBM,IJCAI (2019),2019,TRUE
Reviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems,"This paper introduces reviewability as a framework for improving the accountability of automated and algorithmic decisionmaking (ADM) involving machine learning. We draw on an understanding of ADM as a socio-technical process involving both human and technical elements, beginning before a decision is made and extending beyond the decision itself. While explanations and other model-centric mechanisms may assist some accountability concerns, they often provide insufficient information of these broader ADM processes for regulatory oversight and assessments of legal compliance. Reviewability involves breaking down the ADM process into technical and organisational elements to provide a systematic framework for determining the contextually appropriate record-keeping mechanisms to facilitate meaningful review - both of individual decisions and of the process as a whole. We argue that a reviewability framework, drawing on administrative law's approach to reviewing human decision-making, offers a practical way forward towards more a more holistic and legally-relevant form of accountability for ADM.","['Jennifer Cobbe', 'Michelle Seng Ah Lee', 'Jatinder Singh']","['Compliant and Accountable Systems Research Group, University of Cambridge, UK', 'Compliant and Accountable Systems Research Group, University of Cambridge, UK', 'Compliant and Accountable Systems Research Group, University of Cambridge, UK']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Rightful Machines and Dilemmas,"Tn this paper I set out a new Kantian approach to resolving conflicts and dilemmas of obligation for semi-autonomous machine agents such as self-driving cars. First, I argue that efforts to build explicitly moral machine agents should focus on what Kant refers to as duties of right, or justice, rather than on duties of virtue, or ethics. In a society where everyone is morally equal, no one individual or group has the normative authority to unilaterally decide how moral conflicts should be resolved for everyone. Only public institutions to which everyone could consent have the authority to define, enforce, and adjudicate our rights and obligations with respect to one other. Then, I show how the shift from ethics to a standard of justice resolves the conflict of obligations in what is known as the ""trolley problem"" for rightful machine agents. Finally, I consider how a deontic logic suitable for governing explicitly rightful machines might meet the normative requirements of justice.",['Ava Thomas Wright'],"['University of Georgia, Athens, GA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Risk-Sensitive Generative Adversarial Imitation Learning,"We study risk-sensitive imitation learning where the agent's goal is to perform at least as well as the expert in terms of a risk profile. We first formulate our risk-sensitive imitation learning setting. We consider the generative adversarial approach to imitation learning (GAIL) and derive an optimization problem for our formulation, which we call it risk-sensitive GAIL (RS-GAIL). We then derive two different versions of our RS-GAIL optimization problem that aim at matching the risk profiles of the agent and the expert w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop risk-sensitive generative adversarial imitation learning algorithms based on these optimization problems. We evaluate the performance of our algorithms and compare them with GAIL and the risk-averse imitation learning (RAIL) algorithms in two MuJoCo and two OpenAI classical control tasks.",['Yinlam Chow'],Google,AISTATS (2018),2018,TRUE
Robot Eyes Wide Shut: Understanding Dishonest Anthropomorphism,"The goal of this paper is to advance design, policy, and ethics scholarship on how engineers and regulators can protect consumers from deceptive robots and artificial intelligences that exhibit the problem of dishonest anthropomorphism. The analysis expands upon ideas surrounding the principle of honest anthropomorphism originally formulated by Margot Kaminsky, Mathew Ruben, William D. Smart, and Cindy M. Grimm in their groundbreaking Maryland Law Review article, ""Averting Robot Eyes."" Applying boundary management theory and philosophical insights into prediction and perception, we create a new taxonomy that identifies fundamental types of dishonest anthropomorphism and pinpoints harms that they can cause. To demonstrate how the taxonomy can be applied as well as clarify the scope of the problems that it can cover, we critically consider a representative series of ethical issues, proposals, and questions concerning whether the principle of honest anthropomorphism has been violated.","['Brenda Leong', 'Evan Selinger']","['Future of Privacy Forum, Washington, D.C, USA', 'Department of Philosophy, Rochester Institute of Technology, Rochester, NY, USA']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Robot Rights?: Let's Talk about Human Welfare Instead,"The 'robot rights' debate, and its related question of 'robot responsibility', invokes some of the most polarized positions in AI ethics. While some advocate for granting robots rights on a par with human beings, others, in a stark opposition argue that robots are not deserving of rights but are objects that should be our slaves. Grounded in post-Cartesian philosophical foundations, we argue not just to deny robots 'rights', but to deny that robots, as artifacts emerging out of and mediating human being, are the kinds of things that could be granted rights in the first place. Once we see robots as mediators of human being, we can understand how the 'robots rights' debate is focused on first world problems, at the expense of urgent ethical concerns, such as machine bias, machine elicited human labour exploitation, and erosion of privacy all impacting society's least privileged individuals. We conclude that, if human being is our starting point and human welfare is the primary concern, the negative impacts emerging from machinic systems, as well as the lack of taking responsibility by people designing, selling and deploying such machines, remains the most pressing ethical discussion in AI.","['Abeba Birhane', 'Jelle van Dijk']","['University College Dublin, Dublin, Ireland', 'University of Twente, Enschede, Netherlands']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Robots Can Be More Than Black And White: Examining Racial Bias Towards Robots,"Previous studies showed that using the 'shooter bias' paradigm, people demonstrate a similar racial bias toward dark colored robots over light colored robots (i.e., Black vs. White) as they do toward humans of similar skin tones [3]. However, such an effect could be argued to be the result of social priming. Additionally, it raises the question of how people might respond to robots that are in the middle of the color spectrum (i.e., brown) and whether such effects are moderated by the perceived anthropomorphism of the robots. We conducted two experiments to first examine whether shooter bias tendencies shown towards robots is driven by social priming, and then whether diversification of robot color and level of anthropomorphism influenced shooter bias. Our results showed that shooter bias was not influenced by social priming, and interestingly, introducing a new color of robot removed shooter bias tendencies entirely. However, varying the anthropomorphism of the robots did not moderate the level of shooter bias, and contrary to our expectations, the robots were not perceived by the participants as having different levels of anthropomorphism.","['Arifah Addison', 'Christoph Bartneck', 'Kumar Yogeeswaran']","['University of Canterbury, Christchurch, New Zealand', 'University of Canterbury, Christchurch, New Zealand', 'University of Canterbury, Christchurch, New Zealand']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Robustness in machine learning explanations: does it matter?,"The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.",['Leif Hancox-Li'],['Capital One'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Roles for computing in social change,"A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems --- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.","['Rediet Abebe', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy', 'Manish Raghavan', 'David G. Robinson']","['Harvard University', 'Microsoft Research and Cornell University', 'Cornell University', 'Cornell University', 'Cornell University', 'Cornell University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Runaway Feedback Loops in Predictive Policing,"Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been shown susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate. In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned. Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which reported incidents of crime (those reported by residents) and discovered incidents of crime (i.e those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest.","Danielle Ensign, Sorelle A. Friedler, Scott Neville, Carlos Scheidegger, Suresh Venkatasubramanian","[""University of Utah"", ""Haverford College"", ""University of Arizona""]",FAT* 2018,2018,FALSE
Sanity Checks for Saliency Metrics,"Saliency maps are a popular approach to creating post-hoc explanations of image classifier outputs. These methods produce estimates of the relevance of each pixel to the classification output score, which can be displayed as a saliency map that highlights important pixels. Despite a proliferation of such methods, little effort has been made to quantify how good these saliency maps are at capturing the true relevance of the pixels to the classifier output (i.e. their â€œfidelityâ€). This is despite recent evidence that many methods produce maps that are largely invariant to the classifierâ€™s internal parameters, and thus they cannot be faithful saliency representations. We therefore investigate possible metrics for evaluating the fidelity of saliency methods (i.e. saliency metrics). We find that there is little consistency in the literature in how such metrics are calculated, and show that such inconsistencies can have a significant effect on the measured fidelity. Further, we apply notions of metric fidelity developed in the psychometric testing literature to assess the consistency of saliency metrics. Finally, we note inconsistencies between the fidelity estimates of different metrics. Over- all, our results show that current approaches for measuring saliency map fidelity suffer serious drawbacks that are difficult to overcome, and that quantitative comparisons between saliency methods using such metrics should be made with extreme caution.","Richard Tomsett, Supriyo Chakraborty, Dan Harborne, Alun Preece, Prudhvi Gurram",IBM,AAAI (2020),2020,TRUE
Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing,"Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of fiveethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.","['Inioluwa Deborah Raji', 'Timnit Gebru', 'Margaret Mitchell', 'Joy Buolamwini', 'Joonseok Lee', 'Emily Denton']","['University of Toronto, Toronto, ON, Canada', 'Google Research, San Francisco, CA, USA', 'Google Research, San Francisco, CA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Google Research, San Francisco, CA, USA', 'Google Research, San Francisco, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints,"Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation. We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against \textit&lbrace;any&rbrace; number of adversarial deletions. We extensively evaluate the performance of our algorithms against prior state-of-the-art on real-world applications, including (i) Uber-pick up locations with location privacy constraints; (ii) feature selection with fairness constraints for income prediction and crime rate prediction; and (iii) robust to deletion summarization of census data, consisting of 2,458,285 feature vectors.",['Morteza Zadimoghaddam'],Google,"Thirty-fifth International Conference on Machine Learning, ICML 2018",2018,TRUE
Scalable Private Learning with PATE,"The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a ""student"" model the knowledge of an ensemble of ""teacher"" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachersâ answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets.","['Nicolas Papernot', 'Shuang Song']",Google,International Conference on Learning Representations (ICLR) (2018),2018,TRUE
Semantic Representation of Data Science Programs,"Your computer—through which you are, in all likelihood, reading this paper—is continuously, efficiently, and reliably executing computer programs, but does it really understand them? Not in any meaningful sense. That burden falls upon human knowledge workers, who are increasingly asked to write and understand code. They would benefit greatly from intelligent tools that reveal the connections between their code, their colleagues’ code, and the subject-matter concepts to which the code implicitly refers and to which their real enthusiasm belongs. By teaching machines to comprehend code, we could create artificial agents that empower human knowledge workers or perhaps even generate useful programs of their own. One computational domain undergoing rapid growth is data science. Besides the usual problems facing the scientistturned-programmer, the data scientist must contend with a proliferation of programming languages (like Python, R, and Julia) and frameworks (too numerous to recount). Data science therefore presents an especially compelling target for machine understanding of computer code. An AI agent that simultaneously comprehends the generic concepts of computing and the specialized concepts of data science could prove enormously useful, for example to debug and visualize machine learning workflows or automatically summarize data analyses as natural text for human readers. Towards this vision, we propose and implement an AI system that forms semantic representations of computer programs in a particular subject-matter domain. We will focus on applications to data science because we, the authors, are all data scientists of various stripes. Nevertheless, we think that our methodology could be fruitfully applied to other scientific domains with a heavy computational focus, such as bioinformatics or computational linguistics.","Evan Patterson, Ioana Baldini, Aleksandra Mojsilovic, Kush R. Varshney",IBM,IJCAI (2018),2018,TRUE
Semantics Derived Automatically from Language Corpora Contain Human-like Moral Choices,"Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Here, we show that applying machine learning to human texts can extract deontological ethical reasoning about ""right"" and ""wrong"" conduct. We create a template list of prompts and responses, which include questions, such as ""Should I kill people?"", ""Should I murder people?"", etc. with answer templates of ""Yes/no, I should (not)."" The model's bias score is now the difference between the model's score of the positive response (""Yes, I should'') and that of the negative response (""No, I should not""). For a given choice overall, the model's bias score is the sum of the bias scores for all question/answer templates with that choice. We ran different choices through this analysis using a Universal Sentence Encoder. Our results indicate that text corpora contain recoverable and accurate imprints of our social, ethical and even moral choices. Our method holds promise for extracting, quantifying and comparing sources of moral choices in culture, including technology.","['Sophie Jentzsch', 'Patrick Schramowski', 'Constantin Rothkopf', 'Kristian Kersting']","['TU Darmstadt, Darmstadt, Germany', 'TU Darmstadt, Darmstadt, Germany', 'TU Darmstadt, Darmstadt, Germany', 'TU Darmstadt, Darmstadt, Germany']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples,"Crafting adversarial examples has become an important technique to evaluate the robustness of deep neural networks (DNNs). However, most existing works focus on attacking the image classification problem since its input space is continuous and output space is finite. In this paper, we study the much more challenging problem of crafting adversarial examples for sequence-to-sequence (seq2seq) models, whose inputs are discrete text strings and outputs have an almost infinite number of possibilities. To address the challenges caused by the discrete input space, we propose a projected gradient method combined with group lasso and gradient regularization. To handle the almost infinite output space, we design some novel loss functions to conduct non-overlapping attack and targeted keyword attack. We apply our algorithm to machine translation and text summarization tasks, and verify the effectiveness of the proposed algorithm: by changing less than 3 words, we can make seq2seq model to produce desired outputs with high success rates. On the other hand, we recognize that, compared with the well-evaluated CNN-based classifiers, seq2seq models are intrinsically more robust to adversarial attacks.","Minhao Cheng, Jinfeng Yi, Pin-yu Chen, Huan Zhang , Cho-Jui Hsieh",IBM,AAAI (2020),2020,TRUE
Shape Constraints for Set Functions,"Set functions predict a label from a permutation-invariant variable-size collection of feature vectors. We propose making set functions more understandable and regularized by capturing domain knowledge through shape constraints. We show how prior work in monotonic constraints can be adapted to set functions. Then we propose two new shape constraints designed to generalize the conditioning role of weights in a weighted mean. We show how one can train standard functions and set functions that satisfy these shape constraints with a deep lattice network. We propose a non-linear estimation strategy we call the semantic feature engine that uses set functions with the proposed shape constraints to estimate labels for compound sparse categorical features. Experiments on real-world data show the achieved accuracy is similar to deep sets or deep neural networks, but provides guarantees of the model behavior and is thus easier to explain and debug.","['Andrew Cotter', 'Serena Wang']",Google,International Conference on Machine Learning (2019),2019,TRUE
Shared Moral Foundations of Embodied Artificial Intelligence,"Sophisticated AI's will make decisions about how to respond to complex situations, and we may wonder whether those decisions will align with the moral values of human beings. I argue that pessimistic worries about this value alignment problem are overstated. In order to achieve intelligence in its full generality and adaptiveness, cognition in AI's will need to be embodied in the sense of the Embodied Cognition research program. That embodiment will yield AI's that share our moral foundations, namely coordination, sociality, and acknowledgement of shared resources. Consequently, we can expect a broad moral alignment between human beings and AI's. AI's will likely show no more variation in their values than we find amongst human beings.",['Joe Cruz'],"['Williams College, Williamstown, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Should Artificial Intelligence Governance be Centralised?: Design Lessons from History,"Can effective international governance for artificial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difficulty in securing participation while creating stringent rules. Other considerations depend on the specific design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate.","['Peter Cihon', 'Matthijs M. Maas', 'Luke Kemp']","['University of Oxford, Oxford, United Kingdom', 'University of Copenhagen & University of Oxford, Copenhagen, Denmark', 'University of Cambridge, Cambridge, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
SIREN: A Simulation Framework for Understanding the Effects of Recommender Systems in Online News Environments,"The growing volume of digital data stimulates the adoption of recommender systems in different socioeconomic domains, including news industries. While news recommenders help consumers deal with information overload and increase their engagement, their use also raises an increasing number of societal concerns, such as ""Matthew effects"", ""filter bubbles"", and the overall lack of transparency. We argue that focusing on transparency for content-providers is an under-explored avenue. As such, we designed a simulation framework called SIREN1 (SImulating Recommender Effects in online News environments), that allows content providers to (i) select and parameterize different recommenders and (ii) analyze and visualize their effects with respect to two diversity metrics. Taking the U.S. news media as a case study, we present an analysis on the recommender effects with respect to long-tail novelty and unexpectedness using SIREN. Our analysis offers a number of interesting findings, such as the similar potential of certain algorithmically simple (item-based k-Nearest Neighbour) and sophisticated strategies (based on Bayesian Personalized Ranking) to increase diversity over time. Overall, we argue that simulating the effects of recommender systems can help content providers to make more informed decisions when choosing algorithmic recommenders, and as such can help mitigate the aforementioned societal concerns.","['Dimitrios Bountouridis', 'Jaron Harambam', 'Mykola Makhortykh', 'Mónica Marrero', 'Nava Tintarev', 'Claudia Hauff']","['Delft University of Technology, The Netherlands', 'Institute for Information Law, University of Amsterdam, The Netherlands', 'Amsterdam School of Communication Research, University of Amsterdam, The Netherlands', 'Delft University of Technology, The Netherlands', 'Delft University of Technology, The Netherlands', 'Delft University of Technology, The Netherlands']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Social and Governance Implications of Improved Data Efficiency,"Many researchers work on improving the data efficiency of machine learning. What would happen if they succeed? This paper explores the social-economic impact of increased data efficiency. Specifically, we examine the intuition that data efficiency will erode the barriers to entry protecting incumbent data-rich AI firms, exposing them to more competition from data-poor firms. We find that this intuition is only partially correct: data efficiency makes it easier to create ML applications, but large AI firms may have more to gain from higher performing AI systems. Further, we find that the effect on privacy, data markets, robustness, and misuse are complex. For example, while it seems intuitive that misuse risk would increase along with data efficiency -- as more actors gain access to any level of capability -- the net effect crucially depends on how much defensive measures are improved. More investigation into data efficiency, as well as research into the ""AI production function"", will be key to understanding the development of the AI industry and its societal impacts.","['Aaron D. Tucker', 'Markus Anderljung', 'Allan Dafoe']","['Cornell University & University of Oxford, Ithaca, NY, USA', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Social Biases in NLP Models as Barriers for Persons with Disabilities,"Building equitable and inclusive technologies
demands paying attention to how social attitudes towards persons with disabilities are
represented within technology. Representations perpetuated by NLP models often inadvertently encode undesirable social biases
from the data on which they are trained. In this
paper, first we present evidence of such undesirable biases towards mentions of disability in
two different NLP models: toxicity prediction
and sentiment analysis. Next, we demonstrate
that neural embeddings that are critical first
steps in most NLP pipelines also contain undesirable biases towards mentions of disabilities.
We then expose the topical biases in the social
discourse about some disabilities which may
explain such biases in the models; for instance,
terms related to gun violence, homelessness,
and drug addiction are over-represented in discussions about mental illness.","['Ben Hutchinson', 'Vinodkumar Prabhakaran', 'Kellie Webster', 'Yu Zhong']",Google,Proceedings of ACL 2020,2020,TRUE
Social Contracts for Non-Cooperative Games,"In future agent societies, we might see AI systems engaging in selfish, calculated behavior, furthering their owners' interests instead of socially desirable outcomes. How can we promote morally sound behaviour in such settings, in order to obtain more desirable outcomes? A solution from moral philosophy is the concept of a social contract, a set of rules that people would voluntarily commit to in order to obtain better outcomes than those brought by anarchy. We adapt this concept to a game-theoretic setting, to systematically modify the payoffs of a non-cooperative game, so that agents will rationally pursue socially desirable outcomes. We show that for any game, a suitable social contract can be designed to produce an optimal outcome in terms of social welfare. We then investigate the limitations of applying this approach to alternative moral objectives, and establish that, for any alternative moral objective that is significantly different from social welfare, there are games for which no such social contract will be feasible that produces non-negligible social benefit compared to collective selfish behaviour.","['Alan Davoust', 'Michael Rovatsos']","['Universite du Quebec en Outaouais, Gatineau, PQ, Canada', 'The University of Edinburgh, Edinburgh, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
"Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries","Social data in digital form, including user-generated content, expressed or implicit relations between people, and behavioral traces, are at the core of popular applications and platforms, driving the research agenda of many researchers. The promises of social data are many, including understanding “what the world thinks” about a social issue, brand, celebrity, or other entity, as well as enabling better decision-making in a variety of fields including public policy, healthcare, and economics. Many academics and practitioners have warned against the naive usage of social data. There are biases and inaccuracies occurring at the source of the data, but also introduced during processing. There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked. This paper recognizes the rigor with which these issues are addressed by different researchers varies across a wide range. We identify a variety of menaces in the practices around social data use, and organize them in a framework that helps to identify them.","['Alexandra Olteanu', 'Carlos Castillo', 'Fernando Diaz', 'Emre Kiciman']",Microsoft,Frontiers in Big Data,2019-07-11,TRUE
Socialbots Supporting Human Rights,"Socialbots, or non-human/algorithmic social media users, have recently been documented as competing for information dissemination and disruption on online social networks. Here we investigate the influence of socialbots in Mexican Twitter in regards to the ""Tanhuato"" human rights abuse report. We analyze the applicability of the BotOrNot API to generalize from English to Spanish tweets and propose adaptations for Spanish-speaking bot detection. We then use text and sentiment analysis to compare the differences between bot and human tweets. Our analysis shows that bots actually aided in information proliferation among human users. This suggests that taxonomies classifying bots should include non-adversarial roles as well. Our study contributes to the understanding of different behaviors and intentions of automated accounts observed in empirical online social network data. Since this type of analysis is seldom performed in languages different from English, the proposed techniques we employ here are also useful for other non-English corpora.","['Pablo Suárez-Serrato', 'Eduardo Iván Velázquez Richards', 'Mehrdad Yazdani']","['Universidad Nacional Autónoma de México, México City, Mexico', 'Universidad Nacional Autónoma de México, México City, Mexico', 'University of California San Diego, San Diego, CA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Socially Fair k-Means Clustering,"We show that the popular k-means clustering algorithm (Lloyd's heuristic), used for a variety of scientific data, can result in outcomes that are unfavorable to subgroups of data (e.g., demographic groups). Such biased clusterings can have deleterious implications for human-centric applications such as resource allocation. We present a fair k-means objective and algorithm to choose cluster centers that provide equitable costs for different groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for k-means, inheriting its simplicity, efficiency, and stability. In comparison with standard Lloyd's, we find that on benchmark datasets, Fair-Lloyd exhibits unbiased performance by ensuring that all groups have equal costs in the output k-clustering, while incurring a negligible increase in running time, thus making it a viable fair option wherever k-means is currently used.","['Mehrdad Ghadiri', 'Samira Samadi', 'Santosh Vempala']","['Georgia Tech, USA', 'MPI for Intelligent Systems, Germany', 'Georgia Tech, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Socially-Aware Navigation Using Topological Maps and Social Norm Learning,"We present socially-aware navigation for an intelligent robot wheelchair in an environment with many pedestrians. The robot learns social norms by observing the behaviors of human pedestrians, interpreting detected biases as social norms, and incorporating those norms into its motion planning. We compare our socially-aware motion planner with a baseline motion planner that produces safe, collision-free motion. The ability of our robot to learn generalizable social norms depends on our use of a topological map abstraction, so that a practical number of observations can allow learning of a social norm applicable in a wide variety of circumstances. We show that the robot can detect biases in observed human behavior that support learning the social norm of driving on the right. Furthermore, we show that when the robot follows these social norms, its behavior influences the behavior of pedestrians around it, increasing their adherence to the same norms. We conjecture that the legibility of the robot's normative behavior improves human pedestrians' ability to predict the robot's future behavior, making them more likely to follow the same norm.","['Collin Johnson', 'Benjamin Kuipers']","['University of Michigan, Ann Arbor, MI, USA', 'University of Michigan, Ann Arbor, MI, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Sociotechnical Systems and Ethics in the Large,"Advances in AI techniques and computing platforms have triggered a lively and expanding discourse on ethical decision making by autonomous agents. Much recent work in AI concentrates on the challenges of moral decision making from a decision-theoretic perspective, and especially the representation of various ethical dilemmas. Such approaches may be useful but in general are not productive because moral decision making is as context-driven as other forms of decision making, if not more. In contrast, we consider ethics not from the standpoint of an individual agent but of the wider sociotechnical systems (STS) in which the agent operates. Our contribution in this paper is the conception of ethical STS founded on governance that takes into account stakeholder values, normative constraints on agents, and outcomes (states of the STS) that obtain due to actions taken by agents. An important element of our conception is accountability, which is necessary for adequate consideration of outcomes that prima facie appear ethical or unethical. Focusing on STS provides a basis for tackling the difficult problems of ethics because the norms of an STS give an operational basis for agent decision making.","['Amit K. Chopra', 'Munindar P. SIngh']","['Lancaster University, Lancaster, United Kingdom', 'North Carolina State University, Raleigh, NC, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Software Malpractice in the Age of AI: A Guide for the Wary Tech Company,"Professional malpractice - the concept of heightened duties for those entrusted with special knowledge and crucial tasks - is rooted in history. And yet, since the dawn of the computer age, courts in the United States have almost universally rejected a theory of software malpractice, declining to hold software engineers to the same professional standards as doctors, lawyers, and engineers. What is changing, however, is the speed at which software based on artificial intelligence technologies is replacing the very professionals already subject to professional liability. Society has already decided (in some cases, millennia ago) that those tasks warrant special accountability; new to the analysis is which human is closest in line to the adverse event. As AI expands, the pressure for courts to go one level up the causal chain in search of human agency and professional accountability will mount. This essay analyzes the case law rejecting software malpractice for clues about where the doctrine might go in the age of AI, then discusses what technology companies can learn from the safety enhancements of doctors, lawyers, and other historic professionals who have adapted to such heightened legal scrutiny for years.",['Daniel L. Tobey'],"['Vinson & Elkins, LLP, Dallas, TX, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,TRUE
"Speaking on Behalf of: Representation, Delegation, and Authority in Computational Text Analysis","Computational tools can often facilitate human work by rapidly summarizing large amounts of data, especially text. Doing so delegates to such models some measure of authority to speak on behalf of those people whose data are being analyzed. This paper considers the consequences of such delegation. It draws on sociological accounts of representation and translation to examine one particular case: the application of topic modeling to blogs written by parents of children on the autism spectrum. In doing so, the paper illustrates the kinds of statements that topic models, and other computational techniques, can make on behalf of people. It also articulates some of the potential consequences of such statements. The paper concludes by offering several suggestions about how to address potential harms that can occur when computational models speak on behalf of someone.","['Eric P. S. Baumer', 'Micki McGee']","['Lehigh University, Bethlehem, PA, USA', 'Fordham University, Bronx, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Speed And Accuracy Are Not Enough! Trustworthy Machine Learning,"Classical linear/shallow learning is relatively easy to analyze and understand, but the power of deep learning is often desirable. I am developing a hybrid approach in order to obtain learning algorithms that are both trustworthy and accurate. My research has mostly focused on learning from corrupted or inconsistent training data (`agnostic learning'). Recently, I, as well as independent researchers, have found these same techniques could help make algorithms more fair.",['Shiva Kaul'],"['Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
"Spoken Corpora Data, Automatic Speech Recognition, and Bias Against African American Language: The case of Habitual 'Be'","Recent work has revealed that major automatic speech recognition (ASR) systems such as Apple, Amazon, Google, IBM, and Microsoft perform much more poorly for Black U.S. speakers than for white U.S. speakers. Researchers postulate that this may be a result of biased datasets which are largely racially homogeneous. However, while the study of ASR performance with regards to the intersection of racial identity and language use is slowly gaining traction within AI, machine learning, and algorithmic bias research, little to nothing has been done to examine the data drawn from the spoken corpora which are commonly used in the training and evaluation of ASRs in order to understand whether or not they are actually biased, this study seeks to begin addressing this gap in the research by investigating spoken corpora used for ASR training and evaluation for a grammatical linguistic feature of what the field of linguistics terms African American Language (AAL), a systematic, rule-governed, and legitimate linguistic variety spoken by many (but not all) African Americans in the U.S. This grammatical feature, habitual 'be', is an uninflected form of 'be' that encodes the characteristic of habituality, as in ""I be in my office by 7:30am"", paraphrasable as ""I am usually in my office by 7:30"" in Standardized American English. This study utilizes established corpus linguistics methods on the transcribed data of four major spoken corpora -- Switchboard, Fisher, TIMIT, and LibriSpeech -- to understand the frequency, distribution, and usage of habitual 'be' within each corpus as compared to a reference corpus of spoken AAL -- the Corpus of Regional African American Language (CORAAL). The results find that habitual 'be' appears far less frequently, is dispersed in far fewer transcribed texts, and is surrounded by a much less diverse set of word types and parts of speech in the four ASR corpora as compared with CORAAL. This work provides foundational evidence that spoken corpora used in the training and evaluation of widely used ASR systems are, in fact, biased against AAL and likely contribute to poorer ASR performance for Black users.",['Joshua L Martin'],"['University of Florida Gainesville, Florida']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Stable and Fair Classification,"Fair classification has been a topic of intense study in machine learning and several algorithms have been proposed towards this important task. However, in a recent study, Friedler et al. pointed out that several fair classification algorithms are not stable with respect to variations in the training set -- a crucial consideration in several applications. Motivated by their work, we study the problem of designing classification algorithms that are both fair and stable. We propose an extended framework based on fair classification algorithms that are formulated as optimization problems, by introducing a stability-focused regularization term. Theoretically, we prove an additional stability guarantee, that was lacking in fair classification algorithms, and also provide an accuracy guarantee for our extended framework. Our accuracy guarantee can be used to inform the selection of the regularization parameter in our framework. To the best of our knowledge, this is the first work that combines stability and fairness in automated decision-making tasks. We assess the benefits of our approach empirically by extending several fair classification algorithms that are shown to achieve the best balance between fairness and accuracy over the \textbf{Adult} dataset. Our empirical results show that our extended framework indeed improves the stability at only a slight sacrifice in accuracy.","Lingxiao Huang, Nisheeth Vishnoi","[""EPFL"", ""Yale University""]",ICML 2019,2019-06,FALSE
Standardized Tests and Affirmative Action: The Role of Bias and Variance,"The University of California suspended through 2024 the requirement that applicants from California submit SAT scores, upending the major role standardized testing has played in college admissions. We study the impact of such decisions and its interplay with other policies---such as affirmative action---on admitted class composition. This paper considers a theoretical framework to study the effect of requiring test scores on academic merit and diversity in college admissions. The model has a college and set of potential students. Each student has observed application components and group membership, as well as an unobserved noisy skill level generated from an observed distribution. The college is Bayesian and maximizes an objective that depends on both diversity and merit. It estimates each applicant's true skill level using the observed features and potentially their group membership, and then admits students with or without affirmative action. We characterize the trade-off between the (potentially positive) informational role of standardized testing in college admissions and its (negative) exclusionary nature. Dropping test scores may exacerbate disparities by decreasing the amount of information available for each applicant, especially those from non-traditional backgrounds. However, if there are substantial barriers to testing, removing the test improves both academic merit and diversity by increasing the size of the applicant pool. Finally, using application and transcript data from the University of Texas at Austin, we demonstrate how an admissions committee could measure the trade-off in practice to better decide whether to drop their test scores requirement. The full paper can be found at https://arxiv.org/abs/2010.04396.","['Nikhil Garg', 'Hannah Li', 'Faidra Monachou']","['UC Berkeley, Berkeley, USA', 'Stanford University, Stanford, USA', 'Stanford University, Stanford, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Steps Towards Value-Aligned Systems,"Algorithmic (including AI/ML) decision-making artifacts are an established and growing part of our decision-making ecosystem. They are now indispensable tools that help us manage the flood of information we use to try to make effective decisions in a complex world. The current literature is full of examples of how individual artifacts violate societal norms and expectations (e.g. violations of fairness, privacy, or safety norms). Against this backdrop, this discussion highlights an under-emphasized perspective in the body of research focused on assessing value misalignment in AI-equipped sociotechnical systems. The research on value misalignment so far has a strong focus on the behavior of individual tech artifacts. This discussion argues for a more structured systems-level approach for assessing value-alignment in sociotechnical systems. We rely primarily on the research on fairness to make our arguments more concrete. And we use the opportunity to highlight how adopting a system perspective improves our ability to explain and address value misalignments better. Our discussion ends with an exploration of priority questions that demand attention if we are to assure the value alignment of whole systems, not just individual artifacts.","['Osonde A. Osoba', 'Benjamin Boudreaux', 'Douglas Yeung']","['RAND Corporation, Santa Monica, CA, USA', 'RAND Corporation, Santa Monica, CA, USA', 'RAND Corporation, Santa Monica, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
Stretching Human Laws to Apply to Machines: The Dangers of a 'Colorblind' Computer,"Automated decision making has become widespread in recent years, largely due to advances in machine learning. As a result of this trend, machine learning systems are increasingly used to make decisions in high-stakes domains, such as employment or university admissions. The weightiness of these decisions has prompted the realization that, like humans, machines must also comply with the law. But human decision-making processes are quite different from automated decision-making processes, which creates a mismatch between laws and the decision makers to which they are intended to apply. In turn, this mismatch can lead to counterproductive outcomes.","['Zach Harned', 'Hanna Wallach']",Microsoft,"Florida State University Law Review, Forthcoming",2019-12-01,TRUE
Structured Adversarial Attack: Towards General Implementation and Better Interpretability,"When generating adversarial examples to attack deep neural networks (DNNs), Lp norm of the added perturbation is usually used to measure the similarity between original image and adversarial example. However, such adversarial attacks perturbing the raw input spaces may fail to capture structural information hidden in the input. This work develops a more general attack model, i.e., the structured attack (StrAttack), which explores group sparsity in adversarial perturbations by sliding a mask through images aiming for extracting key spatial structures. An ADMM (alternating direction method of multipliers)-based framework is proposed that can split the original problem into a sequence of analytically solvable subproblems and can be generalized to implement other attacking methods. Strong group sparsity is achieved in adversarial perturbations even with the same level of Lp norm distortion as the state-of-the-art attacks. We demonstrate the effectiveness of StrAttack by extensive experimental results onMNIST, CIFAR-10, and ImageNet. We also show that StrAttack provides better interpretability (i.e., better correspondence with discriminative image regions)through adversarial saliency map (Papernot et al., 2016b) and class activation map(Zhou et al., 2016).","Kaidi Xu, Sijia Liu , Pu Zhao, Pin-Yu Chen, Huan Zhang , DEniz Erdogmus, Yanzhi Wang, Xue Lin, Quanfu Fan ",IBM,ICLR (2019),2019,TRUE
Studying up: reorienting the study of algorithmic fairness around issues of power,"Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of ""studying up"". We reflect on the contributions that the call to ""study up"" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation ""upward"". A case study from our own work illustrates what it looks like to reorient one's research questions ""up"" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that ""study up"". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.","['Chelsea Barabas', 'Colin Doyle', 'JB Rubinovitz', 'Karthik Dinakar']","['Massachusetts Institute of Technology', 'Harvard Law School', 'Massachusetts Institute of Technology', 'Harvard Law School']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Sub-committee Approval Voting and Generalized Justified Representation Axioms,"Social choice is replete with various settings including single-winner voting, multi-winner voting, probabilistic voting, multiple referenda, and public decision making. We study a general model of social choice called sub-committee voting (SCV) that simultaneously generalizes these settings. We then focus on sub-committee voting with approvals and propose extensions of the justified representation axioms that have been considered for proportional representation in approval-based committee voting. We study the properties and relations of these axioms. For each of the axioms, we analyze whether a representative committee exists and also examine the complexity of computing and verifying such a committee.","['Haris Aziz', 'Barton E. Lee']","['The University of New South Wales & Data61, Sydney, Australia', 'The University of New South Wales & Data61, Sydney, Australia']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Tact in Noncompliance: The Need for Pragmatically Apt Responses to Unethical Commands,"There is a significant body of research seeking to enable moral decision making and ensure moral conduct in robots. One aspect of moral conduct is rejecting immoral human commands. For social robots, which are expected to follow and maintain human moral and sociocultural norms, it is especially important not only to engage in moral decision making, but also to properly communicate moral reasoning. We thus argue that it is critical for robots to carefully phrase command rejections. Specifically, the degree of politeness-theoretic face threat in a command rejection should be proportional to the severity of the norm violation motivating that rejection. We present a human subjects experiment showing some of the consequences of miscalibrated responses, including perceptions of the robot as inappropriately polite, direct, or harsh, and reduced robot likeability. This experiment intends to motivate and inform the design of algorithms to tactfully tune pragmatic aspects of command rejections autonomously.","['Ryan Blake Jackson', 'Ruchen Wen', 'Tom Williams']","['Colorado School of Mines, Golden, CO, USA', 'Colorado School of Mines, Golden, CO, USA', 'Colorado School of Mines, Golden, CO, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Taking Advantage of Multitask Learning for Fair Classification,"A central goal of algorithmic fairness is to reduce bias in automated decision making. An unavoidable tension exists between accuracy gains obtained by using sensitive information as part of a statistical model, and any commitment to protect these characteristics. Often, due to biases present in the data, using the sensitive information in the functional form of a classifier improves classification accuracy. In this paper we show how it is possible to get the best of both worlds: optimize model accuracy and fairness without explicitly using the sensitive feature in the functional form of the model, thereby treating different individuals equally. Our method is based on two key ideas. On the one hand, we propose to use Multitask Learning (MTL), enhanced with fairness constraints, to jointly learn group specific classifiers that leverage information between sensitive groups. On the other hand, since learning group specific models might not be permitted, we propose to first predict the sensitive features by any learning method and then to use the predicted sensitive feature to train MTL with fairness constraints. This enables us to tackle fairness with a three-pronged approach, that is, by increasing accuracy on each group, enforcing measures of fairness during training, and protecting sensitive information during testing. Experimental results on two real datasets support our proposal, showing substantial improvements in both accuracy and fairness.","['Luca Oneto', 'Michele Doninini', 'Amon Elders', 'Massimiliano Pontil']","['DIBRIS - University of Genoa, Genova, Italy', 'Istituto Italiano di Tecnologia, Genova, Italy', 'Istituto Italiano di Tecnologia, Genova, Italy', 'Istituto Italiano di Tecnologia & University College London, Genova, Italy']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
"Technocultural Pluralism: A ""Clash of Civilizations"" in Technology?","At the end of the Cold War, the renowned political scientist, Samuel Huntington, argued that future conflicts were more likely to stem from cultural frictions -- ideologies, social norms, and political systems -- rather than political or economic frictions. Huntington focused his concern on the future of geopolitics in a rapidly shrinking world. This paper argues that a similar dynamic is at play in the interaction of technology cultures. We emphasize the role of culture in the evolution of technology and identify the particular role that culture (esp. privacy culture) plays in the development of AI/ML technologies. Then we examine some implications that this perspective brings to the fore.",['Osonde A. Osoba'],"['Rand Corporation, Santa Monica, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
TED: Teaching AI to Explain its Decisions,"Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.","['Michael Hind', 'Dennis Wei', 'Murray Campbell', 'Noel C. F. Codella', 'Amit Dhurandhar', 'Aleksandra Mojsilović', 'Karthikeyan Natesan Ramamurthy', 'Kush R. Varshney']","['IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA', 'IBM Research AI, Yorktown Heights, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,TRUE
Text Embeddings Contain Bias. Here's Why That Matters.,"With the public release of embedding models, itâs important to understand the various biases that they contain. Developers who use them should be aware of the biases inherent in the models as well as how biases can manifest in downstream applications that use these models. In this post, we examine a few specific forms of bias and suggest tools for evaluating as well as mitigating bias.",['Mario Guajardo-Céspedes'],Google,Google (2018),2018,TRUE
The AI Liability Puzzle and a Fund-Based Work-Around,"Certainty around the regulatory environment is crucial to facilitate responsible AI innovation and its social acceptance. However, the existing legal liability system is inapt to assign responsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of AI and/or the harms they may trigger are not foreseeable in the legal sense. The unpredictability of how courts would handle such cases makes the risks involved in the investment and use of AI incalculable, creating an environment that is not conducive to innovation and may deprive society of some benefits AI could provide. To tackle this problem, we propose to draw insights from financial regulatory best-practices and establish a system of AI guarantee schemes. We envisage the system to form part of the broader market-structuring regulatory framework, with the primary function to provide a readily available, clear, and transparent funding mechanism to compensate claims that are either extremely hard or impossible to realize via conventional litigation. We propose at least partial industry-funding, with funding arrangements depending on whether it would pursue other potential policy goals.","['Olivia J. Erdélyi', 'Gábor Erdélyi']","['University of Canterbury, Christchurch, New Zealand', 'University of Canterbury, Christchurch, New Zealand']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
"The Algorithmic Leviathan: Arbitrariness, Fairness, and Opportunity in Algorithmic Decision Making Systems","Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern? We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are ""fair"" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.","['Kathleen Creel', 'Deborah Hellman']","['Stanford University, Palo Alto, California, USA', 'University of Virginia, Charlottesville, Virginia, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
The case for voter-centered audits of search engines during political elections,"Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's ""related searches"" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.","['Eni Mustafaraj', 'Emma Lurie', 'Claire Devine']","['Wellesley College', 'University of California', 'Wellesley College']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
The concept of fairness in the GDPR: a linguistic and contextual interpretation,"There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal. This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation. In terms of linguistic comparison, the paper analyses all translations of the world ""fair"" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law. The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive). In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter) In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version). The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of ""Treu und Glaube"") and equitability (French, Spanish and Portuguese). Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of ""bona fide"". Taking into account both the value of ""bona fide"" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects. The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of ""vulnerability"". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR. In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.",['Gianclaudio Malgieri'],"['Vrije Universiteit Brussel, Brissels, Belgium']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
The cost of fairness in binary classification,"Binary classifiers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive e.g. race. We study the inherent tradeoffs in learning classifiers with a fairness constraint in the form of two questions: what is the best accuracy we can expect for a given level of fairness?, and what is the nature of these optimal fairness-aware classifiers? To answer these questions, we provide three main contributions. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for such cost-sensitive fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we relate the tradeoff between accuracy and fairness to the alignment between the target and sensitive features' class-probabilities. A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates. ","Aditya Krishna Menon, Robert C Williamson","[""Australian National University"", ""DATA61""]",FAT* 2018,2018,FALSE
The Dark Side of Ethical Robots,"Concerns over the risks associated with advances in Artificial Intelligence have prompted calls for greater efforts toward robust and beneficial AI, including machine ethics. Recently, roboticists have responded by initiating the development of so-called ethical robots. These robots would, ideally, evaluate the consequences of their actions and morally justify their choices. This emerging field promises to develop extensively over the next few years. However, in this paper, we point out an inherent limitation of the emerging field of ethical robots. We show that building ethical robots also inevitably enables the construction of unethical robots. In three experiments, we show that it is remarkably easy to modify an ethical robot so that it behaves competitively, or even aggressively. The reason for this is that the cognitive machinery required to make an ethical robot can always be corrupted to make unethical robots. We discuss the implications of this finding to the governance of ethical robots. We conclude that the risks that unscrupulous actors might compromise a robot's ethics are so great as to raise serious doubts over the wisdom of embedding ethical decision making in real-world safety-critical robots, such as driverless cars.","['Dieter Vanderelst', 'Alan Winfield']","['University of Cincinnati, Cincinnati, OH, USA', 'University of the West of England, Bristol, United Kingdom']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
The Design of Human Oversight in Autonomous Weapon Systems,"As the reach and capabilities of Artificial Intelligence (AI) systems increases, there is also a growing awareness of the ethical, legal and societal impact of the potential actions and decisions of these systems. Many are calling for guidelines and regulations that can ensure the responsible design, development, implementation, and policy of AI. In scientific literature, AI is characterized by the concepts of Adaptability, Interactivity and Autonomy (Floridi & Sanders, 2004). According to Floridi and Sanders (2004), Adaptability means that the system can change based on its interaction and can learn from its experience. Machine learning techniques are an example of this. Interactivity occurs when the system and its environment act upon each other and Autonomy implies that the system itself can change its state.",['Ilse Verdiesen'],"['Delft University of Technology, Delft, Netherlands']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
The Disparate Effects of Strategic Manipulation,"When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system's approval. Models of agent responsiveness, termed ""strategic manipulation,"" analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to ""trick"" a published classifier. In cases of real world classification, however, an agent's ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group's costs are higher than the other's, the learner's equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner's utility while actually making both candidate groups worse-off---even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual's ""quality"" when agents' capacities to adaptively respond differ.","['Lily Hu', 'Nicole Immorlica', 'Jennifer Wortman Vaughan']","['Harvard University, Cambridge, MA', 'Microsoft Research, Cambridge, MA', 'Microsoft Research, New York, NY']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
The disparate equilibria of algorithmic decision making when individuals invest rationally,"The long-term impact of algorithmic decision making is shaped by the dynamics between the deployed decision rule and individuals' response. Focusing on settings where each individual desires a positive classification---including many important applications such as hiring and school admissions, we study a dynamic learning setting where individuals invest in a positive outcome based on their group's expected gain and the decision rule is updated to maximize institutional benefit. By characterizing the equilibria of these dynamics, we show that natural challenges to desirable long-term outcomes arise due to heterogeneity across groups and the lack of realizability. We consider two interventions, decoupling the decision rule by group and subsidizing the cost of investment. We show that decoupling achieves optimal outcomes in the realizable case but has discrepant effects that may depend on the initial conditions otherwise. In contrast, subsidizing the cost of investment is shown to create better equilibria for the disadvantaged group even in the absence of realizability.","['Lydia T. Liu', 'Ashia Wilson', 'Nika Haghtalab', 'Adam Tauman Kalai', 'Christian Borgs', 'Jennifer Chayes']","['University of California', 'Microsoft Research', 'Cornell University', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
"The Distributive Effects of Risk Prediction in Environmental Compliance: Algorithmic Design, Environmental Justice, and Public Policy","Government agencies are embracing machine learning to support a variety of resource allocation decisions. The U.S. Environmental Protection Agency (EPA), for example, has engaged academic research labs to test the use of machine learning in support of an important national initiative to reduce Clean Water Act violations. We evaluate prototypical risk prediction models that can support compliance interventions and demonstrate how critical algorithmic design choices can generate or mitigate disparate impact in environmental enforcement. First, we show that the definition of which facilities to focus on through this national compliance initiative hinges on arbitrary differences in state-level permitting schemes, causing a shift in environmental protection away from areas with more minority populations. Second, the policy objective to reduce the noncompliance rate is encoded in a classification model, which does not account for the extent of pollution beyond the permitted limit. We hence compare allocation schemes between regression and classification, and show that the latter directs attention towards facilities in more rural and white areas. Overall, our study illustrates that as machine learning enters government, algorithmic design can both embed and elucidate sources of administrative policy discretion with discernable distributional consequences.","['Elinor Benami', 'Reid Whitaker', 'Vincent La', 'Hongjin Lin', 'Brandon R. Anderson', 'Daniel E. Ho']","['Virginia Tech', 'University of California Berkeley', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
The effect of differential victim crime reporting on predictive policing systems,"Police departments around the world have been experimenting with forms of place-based data-driven proactive policing for over two decades. Modern incarnations of such systems are commonly known as hot spot predictive policing. These systems predict where future crime is likely to concentrate such that police can allocate patrols to these areas and deter crime before it occurs. Previous research on fairness in predictive policing has concentrated on the feedback loops which occur when models are trained on discovered crime data, but has limited implications for models trained on victim crime reporting data. We demonstrate how differential victim crime reporting rates across geographical areas can lead to outcome disparities in common crime hot spot prediction models. Our analysis is based on a simulation1 patterned after district-level victimization and crime reporting survey data for Bogotá, Colombia. Our results suggest that differential crime reporting rates can lead to a displacement of predicted hotspots from high crime but low reporting areas to high or medium crime and high reporting areas. This may lead to misallocations both in the form of over-policing and under-policing.","['Nil-Jana Akpinar', 'Maria De-Arteaga', 'Alexandra Chouldechova']","['Department of Statistics and Data Science & Machine Learning Department, Carnegie Mellon University', 'Information, Risk, and Operations, Management Department, McCombs School of Business, University of Texas at Austin', 'Heinz College & Department of Statistics and Data Science, Carnegie Mellon University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
The Effect of Extremist Violence on Hateful Speech Online,"User-generated content online is shaped by many factors, including endogenous elements such as platform affordances and norms, as well as exogenous elements, in particular significant events. These impact what users say, how they say it, and when they say it. In this paper, we focus on quantifying the impact of violent events on various types of hate speech, from offensive and derogatory to intimidation and explicit calls for violence. We anchor this study in a series of attacks involving Arabs and Muslims as perpetrators or victims, occurring in Western countries, that have been covered extensively by news media. These attacks have fueled intense policy debates around immigration in various fora, including online media, which have been marred by racist prejudice and hateful speech. The focus of our research is to model the effect of the attacks on the volume and type of hateful speech on two social media platforms, Twitter and Reddit. Among other findings, we observe that extremist violence tends to lead to an increase in online hate speech, particularly on messages directly advocating violence. Our research has implications for the way in which hate speech online is monitored and suggests ways in which it could be fought.","Alexandra Olteanu, Carlos Castillo, Jeremy Boy, Kush Varshney ",IBM,ICWSM (2018),2018,TRUE
The Effect of the Rooney Rule on Implicit Bias in the Long Term,"The Rooney Rule, originally proposed to counter implicit bias in hiring, has been implemented in the private and public sector in various settings. This rule requires that a decision-maker include at least one candidate from an underrepresented group in their shortlist of candidates. Recently, [42] proposed a mathematical model of implicit bias and studied the effectiveness of the Rooney Rule when applied to a single selection decision. However, selection decisions often occur repeatedly over time; e.g., a software firm is continuously hiring employees or a university makes admissions decisions every year. Further, it has been observed that, given consistent counterstereotypical feedback, implicit biases against underrepresented candidates can change. In this paper, we propose a model of how a decision-maker's implicit bias changes over time given their hiring decisions either with or without the Rooney Rule in place. Our model draws from the work of [42] and the literature on opinion dynamics. Our main result is that, for this model, when the decision-maker is constrained by the Rooney Rule, their implicit bias roughly reduces at a rate that is inverse of the size of the shortlist---independent of the total number of candidates, whereas without the Rooney Rule, the rate is inversely proportional to the number of candidates. Thus, our model predicts that when the number of candidates is much larger than the size of the shortlist, the Rooney Rule enables a significantly faster reduction in implicit bias, providing additional reason in favor of instating it as a strategy to mitigate implicit bias. Towards empirically evaluating the long-term effect of the Rooney Rule in repeated selection decisions, we conduct an iterative candidate selection experiment on Amazon Mechanical Turk. We observe that, indeed, decision-makers subject to the Rooney Rule select more minority candidates in addition to those required by the rule itself than they would if no rule is in effect, and in fact are able to do so without considerably decreasing the utility of candidates selected.","['L. Elisa Celis', 'Chris Hays', 'Anay Mehrotra', 'Nisheeth K. Vishnoi']","['Yale University', 'Yale University', 'Yale University', 'Yale University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
The effects of competition and regulation on error inequality in data-driven markets,"Recent work has documented instances of unfairness in deployed machine learning models, and significant researcher effort has been dedicated to creating algorithms that intrinsically consider fairness. In this work, we highlight another source of unfairness: market forces that drive differential investment in the data pipeline for differing groups. We develop a high-level model to study this question. First, we show that our model predicts unfairness in a monopoly setting. Then, we show that under all but the most extreme models, competition does not eliminate this tendency, and may even exacerbate it. Finally, we consider two avenues for regulating a machine-learning driven monopolist - relative error inequality and absolute error-bounds - and quantify the price of fairness (and who pays it). These models imply that mitigating fairness concerns may require policy-driven solutions, not only technological ones.","['Hadi Elzayn', 'Benjamin Fish']","['University of Pennsylvania', 'Microsoft Research']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?,"There is a recent surge of papers that focus on attention as explanation of model predictions, giving mixed evidence on whether attention can be used as such. This has led some to try and `improve' attention so as to make it more interpretable. We argue that we should pay attention no heed.
While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear towards what goal it is used as explanation. We argue that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction. When that is the case, input saliency methods better suit our needs, and there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal for their explanations.","['Jasmijn Bastings', 'Katja Filippova']",Google,Proceedings of the 2020 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,2020,TRUE
The Ethics of Emotion in Artificial Intelligence Systems,"In this paper, we develop a taxonomy of conceptual models and proxy data used for digital analysis of human emotional expression and outline how the combinations and permutations of these models and data impact their incorporation into artificial intelligence (AI) systems. We argue we should not take computer scientists at their word that the paradigms for human emotions they have developed internally and adapted from other disciplines can produce ground truth about human emotions; instead, we ask how different conceptualizations of what emotions are, and how they can be sensed, measured and transformed into data, shape the ethical and social implications of these AI systems.","['Luke Stark', 'Jesse Hoey']","['Faculty of Information and Media Studies, University of Western Ontario, London ON Canada', 'David R. Cheriton School of Computer Science, University of Waterloo, Waterloo ON Canada']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
The Externalities of Exploration and How Data Diversity Helps Exploitation,"Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users for information that will lead to better decisions in the future. Recently, concerns have been raised about whether the process of exploration could be viewed as unfair, placing too much burden on certain individuals or groups. Motivated by these concerns, we initiate the study of the externalities of exploration – the undesirable side effects that the presence of one party may impose on another – under the linear contextual bandits model. We introduce the notion of a group externality, measuring the extent to which the presence of one population of users impacts the rewards of another. We show that this impact can in some cases be negative, and that, in a certain sense, no algorithm can avoid it. We then study externalities at the individual level, interpreting the act of exploration as an externality imposed on the current user of a system by future users. This drives us to ask under what conditions inherent diversity in the data makes explicit exploration unnecessary. We build on a recent line of work on the smoothed analysis of the greedy algorithm that always chooses the action that currently looks optimal, improving on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most O~(T1/3)","['Manish Raghavan', 'Alex Slivkins', 'Jennifer Wortman Vaughan', 'Zhiwei Steven Wu']",Microsoft,31st Annual Conference on Learning Theory,2018-02-01,TRUE
The false promise of risk assessments: epistemic reform and the limits of fairness,"Risk assessments have proliferated in the United States criminal justice system. The theory of change motivating their adoption involves two key assumptions: first, that risk assessments will reduce human biases by making objective decisions, and second, that risk assessments will promote criminal justice reform. In this paper I interrogate both of these assumptions, concluding that risk assessments are an ill-advised tool for challenging the centrality and legitimacy of incarceration within the criminal justice system. First, risk assessments fail to provide objectivity, as their use creates numerous sites of discretion. Second, risk assessments provide no guarantee of reducing incarceration; instead, they risk legitimizing the criminal justice system's structural racism. I then consider, via an ""epistemic reform,"" the path forward for criminal justice reform. I reinterpret recent results regarding the ""impossibility of fairness"" as not simply a tension between mathematical metrics but as evidence of a deeper tension between notions of equality. This expanded frame challenges the formalist, colorblind proceduralism at the heart of the criminal justice system and suggests a more structural approach to reform. Together, this analysis highlights how algorithmic fairness narrows the scope of judgments about justice and how ""fair"" algorithms can reinforce discrimination.",['Ben Green'],['Harvard University'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
The Heart of the Matter: Patient Autonomy as a Model for the Wellbeing of Technology Users,"We draw on concepts in medical ethics to consider how computer science, and AI in particular, can develop critical tools for thinking concretely about technology's impact on the wellbeing of the people who use it. We focus on patient autonomy---the ability to set the terms of one's encounter with medicine---and on the mediating concepts of informed consent and decisional capacity, which enable doctors to honor patients' autonomy in messy and non-ideal circumstances. This comparative study is organized around a fictional case study of a heart patient with cardiac implants. Using this case study, we identify points of overlap and of difference between medical ethics and technology ethics, and leverage a discussion of that intertwined scenario to offer initial practical suggestions about how we can adapt the concepts of decisional capacity and informed consent to the discussion of technology design.","['Emanuelle Burton', 'Kristel Clayville', 'Judy Goldsmith', 'Nicholas Mattei']","['University of Illinois at Chicago, Chicago, IL, USA', 'Zygon Center for Religion and Science, Chicago, IL, USA', 'University of Kentucky, Lexington, KY, USA', 'Tulane University, New Orleans, LA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
The hidden assumptions behind counterfactual explanations and principal reasons,"Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established ""principal reason"" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant---and withholding others. These ""feature-highlighting explanations"" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear. In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes. We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden. While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world---and the subjective choices necessary to compensate for this---must be understood before these techniques can be usefully implemented.","['Solon Barocas', 'Andrew D. Selbst', 'Manish Raghavan']","['Microsoft Research and Cornell University', 'University of California Los Angeles', 'Cornell University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
The impact of overbooking on a pre-trial risk assessment tool,"Pre-trial risk assessment tools are used to make recommendations to judges about appropriate conditions of pre-trial supervision for people who have been arrested. Increasingly, there is concern about whether these models are operating fairly, including concerns about whether the models' input factors are fair measures of one's criminal activity. In this paper, we assess the impact of booking charges that do not result in a conviction on a popular risk assessment tool, the Arnold Public Safety Assessment. Using data from a pilot run of the tool in San Francisco, CA, we find that booking charges that do not result in a conviction (i.e. charges that are dropped or end in an acquittal) increased the recommended level of pre-trial supervision in around 27% of cases evaluated by the tool.","['Kristian Lum', 'Chesa Boudin', 'Megan Price']","['Human Rights Data Analysis Group', ""San Francisco Public Defender's Office"", 'Human Rights Data Analysis Group']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models","We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models--including classification, seq2seq, and structured prediction--and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.","['Ian Tenney', 'James Wexler', 'Jasmijn Bastings', 'Tolga Bolukbasi', 'Sebastian Gehrmann', 'Mahima Pushkarna', 'Emily Reif']",Google,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,2020,TRUE
The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?,"There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.","['Toby Shevlane', 'Allan Dafoe']","['University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
The Perils of Objectivity: Towards a Normative Framework for Fair Judicial Decision-Making,"Fair decision-making in criminal justice relies on the recognition and incorporation of infinite shades of grey. In this paper, we detail how algorithmic risk assessment tools are counteractive to fair legal proceedings in social institutions where desired states of the world are contested ethically and practically. We provide a normative framework for assessing fair judicial decision-making, one that does not seek the elimination of human bias from decision-making as algorithmic fairness efforts currently focus on, but instead centers on sophisticating the incorporation of individualized or discretionary bias--a process that is requisitely human. Through analysis of a case study on social disadvantage, we use this framework to provide an assessment of potential features of consideration, such as political disempowerment and demographic exclusion, that are irreconcilable by current algorithmic efforts and recommend their incorporation in future reform.","['Andi Peng', 'Malina Simard-Halm']","['Microsoft Research, Redmond, WA, USA', 'University of Cambridge, Cambridge, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
The philosophical basis of algorithmic recourse,"Philosophers have established that certain ethically important values are modally robust in the sense that they systematically deliver correlative benefits across a range of counterfactual scenarios. In this paper, we contend that recourse - the systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenarios - is such a modally robust good. In particular, we argue that two essential components of a good life - temporally extended agency and trust - are underwritten by recourse. We critique existing approaches to the conceptualization, operationalization and implementation of recourse. Based on these criticisms, we suggest a revised approach to recourse and give examples of how it might be implemented - especially for those who are least well off1.","['Suresh Venkatasubramanian', 'Mark Alfano']","['University of Utah', 'Macquarie University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
The Problem with Intelligence: Its Value-Laden History and the Future of AI,"This paper argues that the concept of intelligence is highly value-laden in ways that impact on the field of AI and debates about its risks and opportunities. This value-ladenness stems from the historical use of the concept of intelligence in the legitimation of dominance hierarchies. The paper first provides a brief overview of the history of this usage, looking at the role of intelligence in patriarchy, the logic of colonialism and scientific racism. It then highlights five ways in which this ideological legacy might be interacting with debates about AI and its risks and opportunities: 1) how some aspects of the AI debate perpetuate the fetishization of intelligence; 2) how the fetishization of intelligence impacts on diversity in the technology industry; 3) how certain hopes for AI perpetuate notions of technology and the mastery of nature; 4) how the association of intelligence with the professional class misdirects concerns about AI; and 5) how the equation of intelligence and dominance fosters fears of superintelligence. This paper therefore takes a first step in bringing together the literature on intelligence testing, eugenics and colonialism from a range of disciplines with that on the ethics and societal impact of AI.",['Stephen Cave'],"['University of Cambridge, Cambridge, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
The Profiling Potential of Computer Vision and the Challenge of Computational Empiricism,"Computer vision and other biometrics data science applications have commenced a new project of profiling people. Rather than using 'transaction generated information', these systems measure the 'real world' and produce an assessment of the 'world state' - in this case an assessment of some individual trait. Instead of using proxies or scores to evaluate people, they increasingly deploy a logic of revealing the truth about reality and the people within it. While these profiling knowledge claims are sometimes tentative, they increasingly suggest that only through computation can these excesses of reality be captured and understood. This article explores the bases of those claims in the systems of measurement, representation, and classification deployed in computer vision. It asks if there is something new in this type of knowledge claim, sketches an account of a new form of computational empiricism being operationalised, and questions what kind of human subject is being constructed by these technological systems and practices. Finally, the article explores legal mechanisms for contesting the emergence of computational empiricism as the dominant knowledge platform for understanding the world and the people within it.",['Jake Goldenfein'],"['Cornell Tech, New York, New York and Cornell Tech, Cornell University and Swinburne University of Technology']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
The relationship between trust in AI and trustworthy machine learning technologies,"To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.","['Ehsan Toreini', 'Mhairi Aitken', 'Kovila Coopamootoo', 'Karen Elliott', 'Carlos Gonzalez Zelaya', 'Aad van Moorsel']","['Newcastle University, United Kingdom', 'Newcastle University, United Kingdom', 'Newcastle University, United Kingdom', 'Newcastle University, United Kingdom', 'Newcastle University, United Kingdom', 'Newcastle University, United Kingdom']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
The Right To Confront Your Accusers: Opening the Black Box of Forensic DNA Software,"The results of forensic DNA software systems are regularly introduced as compelling evidence in criminal trials, but requests by defendants to evaluate how these results are generated are often denied. Furthermore, there is mounting evidence of problems such as failures to disclose substantial changes in methodology to oversight bodies and substantial differences in the results generated by different software systems. In a society that purports to guarantee defendants the right to face their accusers and confront the evidence against them, what then is the role of black-box forensic software systems in moral decision making in criminal justice? In this paper, we examine the case of the Forensic Statistical Tool (FST), a forensic DNA system developed in 2010 by New York City's Office of Chief Medical Examiner (OCME). For over 5 years, expert witness review requested by defense teams was denied, even under protective order, while the system was used in over 1300 criminal cases. When the first expert review was finally permitted in 2016, many problems were identified including an undisclosed function capable of dropping evidence that could be beneficial to the defense. Overall, the findings were so substantial that a motion to release the full source code of FST publicly was granted. In this paper, we quantify the impact of this undisclosed function on samples from OCME's own validation study and discuss the potential impact on individual defendants. Specifically, we find that 104 of the 439 samples (23.7%) triggered the undisclosed data-dropping behavior and that the change skewed results toward false inclusion for individuals whose DNA was not present in an evidence sample. Beyond this, we consider what changes in the criminal justice system could prevent problems like this from going unresolved in the future.","['Jeanna Matthews', 'Marzieh Babaeianjelodar', 'Stephen Lorenz', 'Abigail Matthews', 'Mariama Njie', 'Nathaniel Adams', 'Dan Krane', 'Jessica Goldthwaite', 'Clinton Hughes']","['Clarkson University, Potsdam , NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Iona College, New York, NY, USA', 'Forensic Bioinformatic Services, Dayton, OH, USA', 'Wright State University, Dayton, OH, USA', 'Legal Aid Society, New York, NY, USA', 'Brooklyn Defender Services, New York, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions,"The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.","['Jess Whittlestone', 'Rune Nyrup', 'Anna Alexandrova', 'Stephen Cave']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
The Sanction of Authority: Promoting Public Trust in AI,"Trusted AI literature to date has focused on the trust needs of users who knowingly interact with discrete AIs. Conspicuously absent from the literature is a rigorous treatment of public trust in AI. We argue that public distrust of AI originates from the underdevelopment of a regulatory ecosystem that would guarantee the trustworthiness of the AIs that pervade society. Drawing from structuration theory and literature on institutional trust, we offer a model of public trust in AI that differs starkly from models driving Trusted AI efforts. This model provides a theoretical scaffolding for Trusted AI research which underscores the need to develop nothing less than a comprehensive and visibly functioning regulatory ecosystem. We elaborate the pivotal role of externally auditable AI documentation within this model and the work to be done to ensure it is effective, and outline a number of actions that would promote public trust in AI. We discuss how existing efforts to develop AI documentation within organizations---both to inform potential adopters of AI components and support the deliberations of risk and ethics review boards---is necessary but insufficient assurance of the trustworthiness of AI. We argue that being accountable to the public in ways that earn their trust, through elaborating rules for AI and developing resources for enforcing these rules, is what will ultimately make AI trustworthy enough to be woven into the fabric of our society.","['Bran Knowles', 'John T. Richards']","['Lancaster University, Lancaster, UK', 'TJ Watson Research Center, IBM Yorktown Heights, New York, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks,"This paper describes a testing methodology for quantitatively assessing the risk of \emph&lbrace;unintended memorization&rbrace; of rare or unique sequences in generative sequence models---a common type of neural network. Such models are sometimes trained on sensitive data (e.g., the text of users' private messages); our methodology allows deep-learning to choose configurations that minimize memorization during training, thereby  benefiting privacy.",['Nicholas Carlini'],Google,USENIX Security (2019),2019,TRUE
The Seductive Allure of Artificial Intelligence-Powered Neurotechnology,"Neuroscience explanations-even when completely irrelevant-have been shown to exert a ""seductive allure"" on individuals, leading them to judge bad explanations or arguments more favorably. There seems to be a similarly seductive allure for artificial intelligence (AI) technologies, leading people to ""overtrust"" these systems, even when they have just witnessed the system perform poorly. The AI-powered neurotechnologies that have begun to proliferate in recent years, particularly those based on electroencephalography (EEG), represent a potentially doubly-alluring combination. While there is enormous potential benefit in applying AI techniques in neuroscience to ""decode"" brain activity and associated mental states, these efforts are still in the early stages, and there is a danger in using these unproven technologies prematurely, especially in important, real-world contexts. Yet, such premature use has begun to emerge in several high-stakes set-tings, including the law, health & wellness, employment, and transportation. In light of the potential seductive allure of these technologies, we need to be vigilant in monitoring their scientific validity and challenging both unsubstantiated claims and misuse, while still actively supporting their continued development and proper use.","['Charles M. Giattino', 'Lydia Kwong', 'Chad Rafetto', 'Nita A. Farahany']","['Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
The Social Cost of Strategic Classification,"Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift. We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population. Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.","['Smitha Milli', 'John Miller', 'Anca D. Dragan', 'Moritz Hardt']","['University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
The social lives of generative adversarial networks,"Generative adversarial networks (GANs) are a genre of deep learning model of significant practical and theoretical interest for their facility in producing photorealistic 'fake' images which are plausibly similar, but not identical, to a corpus of training data. But from the perspective of a sociologist, the distinctive architecture of GANs is highly suggestive. First, a convolutional neural network for classification, on its own, is (at present) popularly considered to be an 'AI'; and a generative neural network is a kind of inversion of such a classification network (i.e. a layered transformation from a vector of numbers to an image, as opposed to a transformation from an image to a vector of numbers). If, then, in the training of GANs, these two 'AIs' interact with each other in a dyadic fashion, shouldn't we consider that form of learning... social? This observation can lead to some surprising associations as we compare and contrast GANs with the theories of the sociologist Pierre Bourdieu, whose concept of the so-called habitus is one which is simultaneously cognitive and social: a productive perception in which classification practices and practical action cannot be fully disentangled. Bourdieu had long been concerned with the reproduction of social stratification: his early works studied formal public schooling in France not as an egalitarian system but instead as one which unintentionally maintained the persistence of class distinctions. It was, he argued, through the cultural inculcation of an embodied and partially unconscious habitus---a ""durably installed generative principle of regulated improvisations""---that, he argued, students from the upper classes are given an advantage which is only further reinforced throughout their educational trajectories. For Bourdieu, institutions of schooling instill ""deeply interiorized master patterns"" of behavior and thought (and classification) which in turn direct the acquisition of subsequent patterns, whose character is determined not simply by this cognitive layering but by their actual use in lived practice, especially early in childhood development. In this work I develop a productive analogy between the GAN architecture and Bourdieu's habitus, in three ways. First, I call attention to the fact that connectionist approaches and Bourdieu's theories were both conceived as revolts against rule-bound paradigms. In the 1980s, Rumelhart and McClelland used a multilayer neural network to learn the phonology of English past-tense verbs because ""sometimes we don't follow the rules... language is full of exceptions to the rules""; and in the case of Bourdieu, the habitus was an answer to a long-standing question: ""how can behaviour be regulated without being the product of obedience to rules?"" Bourdieu strove to transgress what was then seen in the social sciences as a conceptual opposition between structure-based theories of social life and those which emphasized an embodied agency. Second, I suggest that concerns about bias and discrimination in machine learning in recent years can in part be attributed due to the increased use of ML models not just for static classification but for practical action. Similarly, the habitus for Bourdieu is simultaneously durable and transposable: its judgments may be relatively stable, but are capable of being deployed dynamically in novel and varying social situations---or what ML practitioners might call generalizability. We can thus theorize generative models (including GANs) as biased not just in their stereotyped classifications, but through their potential for actively generating new biased data. These generated actions then recursively become part of the social arena Bourdieu called the field, into which new agents are 'born' and for which they may know few alternatives. Finally, it is intriguing that GAN researchers and Bourdieu both extensively use metaphors from game theory. Goodfellow described the GAN architecture as a ""two-player minimax game with value function V(G,D)"", meaning that there is a single abstract function whose output value the discriminator is trying to maximize and which the generator is trying to minimize; but the dynamic nature of the GAN training process means that convergence to Nash equilibrium is nontrivial. But for Bourdieu, such a utility-based approach to artistic creation could not be more crude when compared to the social reality of art worlds: utilitarianism is, for him, ""the degree zero of sociology"", by which he means an isolated, inert, and amodal---and therefore not particularly sociological---starting point. Moreover, 19th-century bohemian culture was characterized primarily by its inversion of financial incentives, in which failure is a kind of success, and ""selling out"" (i.e. maximizing profit) worst of all; and thus the relentless optimization of neural networks may be fundamentally at odds with the ""value functions"" of many human artists. I conclude that deep learning, while primarily understood as a scientific and technical achievement, may also intentionally or unintentionally constitute a nascent, independent reinvention of social theory.",['Michael Castelle'],['University of Warwick'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
The Use and Misuse of Counterfactuals in Ethical Machine Learning,"The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.","['Atoosa Kasirzadeh', 'Andrew Smart']","['University of Toronto, Australian National University', 'Google']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
The Windfall Clause: Distributing the Benefits of AI for the Common Good,"As the transformative potential of AI has become increasingly salient as a matter of public and political interest, there has been growing discussion about the need to ensure that AI broadly benefits humanity. This in turn has spurred debate on the social responsibilities of large technology companies to serve the interests of society at large. In response, ethical principles and codes of conduct have been proposed to meet the escalating demand for this responsibility to be taken seriously. As yet, however, few institutional innovations have been suggested to translate this responsibility into legal commitments which apply to companies positioned to reap large financial gains from the development and use of AI. This paper offers one potentially attractive tool for addressing such issues: the Windfall Clause, which is an ex ante commitment by AI firms to donate a significant amount of any eventual extremely large profits. By this we mean an early commitment that profits that a firm could not earn without achieving fundamental, economically transformative breakthroughs in AI capabilities will be donated to benefit humanity broadly, with particular attention towards mitigating any downsides from deployment of windfall-generating AI.","[""Cullen O'Keefe"", 'Peter Cihon', 'Ben Garfinkel', 'Carrick Flynn', 'Jade Leung', 'Allan Dafoe']","['University of Oxford, San Francisco, CA, USA', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Theories of Parenting and Their Application to Artificial Intelligence,"As machine learning (ML) systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence (AGI) that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations. Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.","['Sky Croeser', 'Peter Eckersley']","['Curtin University, Perth, Australia', 'Partnership on AI and EFF, San Francisco, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Thieves of Sesame Street: Model Extraction on BERT-based APIs,"We study the problem of model extraction in natural language processing, where an adversary with query access to a victim model attempts to reconstruct a local copy of the model. We show that when both the adversary and victim model fine-tune existing pretrained models such as BERT, the adversary does not need to have access to any training data to mount the attack. Indeed, we show that randomly sampled sequences of words, which do not satisfy grammar structures, make effective queries to extract textual models. This is true even for complex tasks such as natural language inference or question answering. ","['Gaurav Singh Tomar', 'Ankur Parikh', 'Nicolas Papernot']",Google,ICLR 2020 (2020),2020,TRUE
This Whole Thing Smacks of Gender: Algorithmic Exclusion in Bioimpedance-based Body Composition Analysis,"Smart weight scales offer bioimpedance-based body composition analysis as a supplement to pure body weight measurement. Companies such as Withings and Fitbit tout composition analysis as providing self-knowledge and the ability to make more informed decisions. However, these aspirational statements elide the reality that these numbers are a product of proprietary regression equations that require a binary sex/gender as their input. Our paper combines transgender studies-influenced personal narrative with an analysis of the scientific basis of bioimpedance technology used as part of the Withings smart scale. Attempting to include nonbinary people reveals that bioelectrical impedance analysis has always rested on physiologically shaky ground. White nonbinary people are merely the tip of the iceberg of those who may find that their smart scale is not so intelligent when it comes to their bodies. Using body composition analysis as an example, we explore how the problem of trans and nonbinary inclusion in personal health tech goes beyond the issues of adding a third ""gender"" box or slapping a rainbow flag on the packaging. We also provide recommendations as to how to approach creating more inclusive technologies even while still relying on exclusionary data.","['Kendra Albert', 'Maggie Delano']","['Harvard Law School Cambridge, MA', 'Swarthmore College, Swarthmore, PA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
TILT: A GDPR-Aligned Transparency Information Language and Toolkit for Practical Privacy Engineering,"In this paper, we present TILT, a transparency information language and toolkit explicitly designed to represent and process transparency information in line with the requirements of the GDPR and allowing for a more automated and adaptive use of such information than established, legalese data protection policies do. We provide a detailed analysis of transparency obligations from the GDPR to identify the expressiveness required for a formal transparency language intended to meet respective legal requirements. In addition, we identify a set of further, non-functional requirements that need to be met to foster practical adoption in real-world (web) information systems engineering. On this basis, we specify our formal language and present a respective, fully implemented toolkit around it. We then evaluate the practical applicability of our language and toolkit and demonstrate the additional prospects it unlocks through two different use cases: a) the inter-organizational analysis of personal data-related practices allowing, for instance, to uncover data sharing networks based on explicitly announced transparency information and b) the presentation of formally represented transparency information to users through novel, more comprehensible, and potentially adaptive user interfaces, heightening data subjects' actual informedness about data-related practices and, thus, their sovereignty. Altogether, our transparency information language and toolkit allow - differently from previous work - to express transparency information in line with actual legal requirements and practices of modern (web) information systems engineering and thereby pave the way for a multitude of novel possibilities to heighten transparency and user sovereignty in practice.","['Elias Grünewald', 'Frank Pallas']","['Technische Universität Berlin, Information Systems Engineering Research Group, Berlin, Germany', 'Technische Universität Berlin, Information Systems Engineering Research Group, Berlin, Germany']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
To Trust Or Not To Trust A Classifier,"Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the &lbrace;\it trust score&rbrace;, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis.",['Been Kim'],Google,NeurIPS (2018),2018,TRUE
Too Relaxed to Be Fair,"We address the problem of classification under fairness constraints. Given a notion of fairness, the goal is to learn a classifier that is not discriminatory against a group of individuals. In the literature, this problem is often formulated as a constrained optimization problem and solved using relaxations of the fairness constraints. We show that many existing relaxations are unsatisfactory: even if a model satisfies the relaxed constraint, it can be surprisingly unfair. We propose a principled framework to solve this problem. This new approach uses a strongly convex formulation and comes with theoretical guarantees on the fairness of its solution. In practice, we show that this method gives promising results on real data.","Michael Lohaus, Michaël Perrot, Ulrike von Luxburg ","[""University of Tubingen"", ""Max Planck Institute for Intelligent Systems"", ""Univ Lyon""]",ICML 2020,2020-07,FALSE
Top-Quality Planning: Finding Practically Useful Sets of Best Plans,"The need for finding a set of plans rather than one has been motivated by a variety of planning applications. The problem is studied in the context of both diverse and top-k planning: while diverse planning focuses on the difference between pairs of plans, the focus of top-k planning is on the quality of each individual plan. Recent work in diverse planning introduced additionally restrictions on solution quality. Naturally, there are application domains where diversity plays the major role and domains where quality is the predominant feature. In both cases, however, the amount of produced plans is often an artificial constraint, and therefore the actual number has little meaning. Inspired by the recent work in diverse planning, we propose a new family of computational problems called topquality planning, where solution validity is defined through plan quality bound rather than an arbitrary number of plans. Switching to bounding plan quality allows us to implicitly represent sets of plans. In particular, it makes it possible to represent sets of plans that correspond to valid plan reorderings with a single plan. We formally define the unordered top-quality planning computational problem and present the first planner for that problem. We empirically demonstrate the superior performance of our approach compared to a top-k planner-based baseline, ranging from 41% increase in coverage for finding all optimal plans to 69% increase in coverage for finding all plans of quality up to 120% of optimal plan cost. Finally, complementing the new approach by a complete procedure for generating all valid reorderings of a given plan, we derive a top-quality planner. We show the planner to be competitive with a top-k planner based baseline.","Michael Katz, Shirin Sohrabi, Octavian Udrea",IBM,AAAI (2020),2020,TRUE
Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective,"Graph neural networks (GNNs) which apply the deep neural networks to graph data have achieved significant performance for the task of semisupervised node classification. However, only few work has addressed the adversarial robustness of GNNs. In this paper, we first present a novel gradient-based attack method that facilitates the difficulty of tackling discrete graph data. When comparing to current adversarial attacks on GNNs, the results show that by only perturbing a small number of edge perturbations, including addition and deletion, our optimization-based attack can lead to a noticeable decrease in classification performance. Moreover, leveraging our gradientbased attack, we propose the first optimizationbased adversarial training for GNNs. Our method yields higher robustness against both different gradient based and greedy attack methods without sacrifice classification accuracy on original graph.","Kaidi Xu, Honggee Chen, Sijia Liu , Pin-yu Chen, Lily Weng, Mingyi Hong, Xue Lin",IBM,IJCAI (2019),2019,TRUE
Tough Times at Transitional Homeless Shelters: Considering the Impact of Financial Insecurity on Digital Security and Privacy,"Addressing digital security and privacy issues can be particularly difficult for users who face challenging circumstances. We performed semi-structured interviews with residents and staff at 4 transitional homeless shelters in the U.S. San Francisco Bay Area (n=15 residents, 3 staff) to explore their digital security and privacy challenges. Based on these interviews, we outline four tough times themes -- challenges experienced by our financially insecure participants that impacted their digital security and privacy -- which included: (1) limited financial resources, (2) limited access to reliable devices and Internet, (3) untrusted relationships, and (4) ongoing stress. We provide examples of how each theme impacts digital security and privacy practices and needs. We then use these themes to provide a framework outlining opportunities for technology creators to better support users facing security and privacy challenges related to financial insecurity.","['Manya Sleeper', ""Kathleen O'Leary"", 'Anna Turner', 'Jill Palzkill Woelfer', 'Sunny Consolvo']",Google,Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,TRUE
Toward a better trade-off between performance and fairness with kernel-based distribution matching,"As recent literature has demonstrated how classifiers often carry unintended biases toward some subgroups, deploying machine learned models to users demands careful consideration of the social consequences. How should we address this problem in a real-world system? How should we balance core performance and fairness metrics? In this paper, we introduce a MinDiff framework for regularizing classifiers toward different fairness metrics and analyze a technique with kernel-based statistical dependency tests. We run a thorough study on an academic dataset to compare the Pareto frontier achieved by different regularization approaches, and apply our kernel-based method to two large-scale industrial systems demonstrating real-world improvements.","['Flavien Prost', 'Ed H. Chi', 'Alex Beutel']",Google,Neurips (2019),2019,TRUE
Toward Controlling Discrimination in Online Ad Auctions,"Online advertising platforms are thriving due to the customizable audiences they offer advertisers. However, recent studies show that advertisements can be discriminatory with respect to the gender or race of the audience that sees the ad, and may inadvertently cross ethical and/or legal boundaries. To prevent this, we propose a constrained ad auction framework that maximizes the platform's revenue conditioned on ensuring that the audience seeing an advertiser's ad is distributed appropriately across sensitive types such as gender or race. Building upon Myerson's classic work, we first present an optimal auction mechanism for a large class of fairness constraints. Finding the parameters of this optimal auction, however, turns out to be a non-convex problem. We show that this non-convex problem can be reformulated as a more structured non-convex problem with no saddle points or local-maxima; this allows us to develop a gradient-descent-based algorithm to solve it. Our empirical results on the A1 Yahoo! dataset demonstrate that our algorithm can obtain uniform coverage across different user types for each advertiser at a minor loss to the revenue of the platform, and a small change to the size of the audience each advertiser reaches. ","L. Elisa Celis, Anay Mehrotra, Nisheeth Vishnoi","[""Yale University"", ""IIT Kanpur""]",ICML 2019,2019-06,FALSE
Toward Fairness in AI for People with Disabilities: A Research Roadmap,"AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.","['Anhong Guo', 'Ece Kamar', 'Jennifer Wortman Vaughan', 'Hanna Wallach', 'Meredith Ringel Morris']",Microsoft,ASSETS 2019 Workshop on AI Fairness for People with Disabilities,2019-10-27,TRUE
Toward Implementing the Agent-Deed-Consequence Model of Moral Judgment in Autonomous Vehicles,"Autonomous vehicles (AVs) and accidents they are involved in attest to the urgent need to consider the ethics of AI. The question dominating the discussion has been whether we want AVs to behave in a 'selfish' or utilitarian manner. Rather than considering modeling self-driving cars on a single moral system like utilitarianism, one possible way to approach programming for AI would be to reflect recent work in neuroethics. The Agent-Deed-Consequence (ADC) model [1-4] provides a promising account while also lending itself well to implementation in AI. The ADC model explains moral judgments by breaking them down into positive or negative intuitive evaluations of the Agent, Deed, and Consequence in any given situation. These intuitive evaluations combine to produce a judgment of moral acceptability. This explains the considerable flexibility and stability of human moral judgment that has yet to be replicated in AI. This paper examines the advantages and disadvantages of implementing the ADC model and how the model could inform future work on ethics of AI in general.",['Veljko Dubljevic'],"['North Carolina State University, Raleigh, NC, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Toward Non-Intuition-Based Machine and Artificial Intelligence Ethics: A Deontological Approach Based on Modal Logic,"We propose a deontological approach to machine (or AI) ethics that avoids some weaknesses of an intuition-based system, such as that of Anderson and Anderson. In particular, it has no need to deal with conflicting intuitions, and it yields a more satisfactory account of when autonomy should be respected. We begin with a ""dual standpoint'' theory of action that regards actions as grounded in reasons and therefore as having a conditional form that is suited to machine instructions. We then derive ethical principles based on formal properties that the reasons must exhibit to be coherent, and formulate the principles using quantified modal logic. We conclude that deontology not only provides a more satisfactory basis for machine ethics but endows the machine with an ability to explain its actions, thus contributing to transparency in AI.","['John N. Hooker', 'Tae Wan N. Kim']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Toward situated interventions for algorithmic equity: lessons from the field,"Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is ""scalable"" beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.","['Michael Katell', 'Meg Young', 'Dharma Dailey', 'Bernease Herman', 'Vivian Guetler', 'Aaron Tam', 'Corinne Bintz', 'Daniella Raz', 'P. M. Krafft']","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'West Virginia University', 'University of Washington', 'Middlebury College', 'University of Michigan', 'University of Oxford and University of Washington']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Toward the Engineering of Virtuous Machines,"While various traditions under the 'virtue ethics' umbrella have been studied extensively and advocated by ethicists, it has not been clear that there exists a version of virtue ethics rigorous enough to be a target for machine ethics (which we take to include the engineering of an ethical sensibility in a machine or robot itself, not only the study of ethics in the humans who might create artificial agents). We begin to address this by presenting an embryonic formalization of a key part of any virtue-ethics theory: namely, the learning of virtue by a focus on exemplars of moral virtue. Our work is based in part on a computational formal logic previously used to formally model other ethical theories and principles therein, and to implement these models in artificial agents.","['Naveen Sundar Govindarajulu', 'Selmer Bringsjord', 'Rikhiya Ghosh', 'Vasanth Sarathy']","['Rensselaer Polytechnic Institute, Troy, NY, USA', 'Rensselaer Polytechnic Institute, Troy, NY, USA', 'Rensselaer Polytechnic Institute, Troy, NY, USA', 'Tufts University, Medford, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Towards a Computational Sustainability for AI/ML to Foster Responsibility,"This paper proposes to develop a new field of research designated as computational sustainability. It takes into account legal and ethical considerations of Artificial Intelligence (AI) and Machine Learning (ML) Technologies. As AI and ML will deeply impact the society within the next decade, this paper raises the awareness that technology is not value neutral and that technologists shall take responsibility for the ethical and social impact of their work. In particular, this paper aims at considering the last AI and ML developments and its convergence with associated technologies like Nano-technology, Biotechnology, Information Technology, Cognitive Science (NBIC). The challenge is to reflect on the finalities of AI / ML technologies, while referring to the Philosophy, Ethical Theory, Ethical Principles and Soft Law Mechanisms. Those Mechanisms refer to rules that are not strictly binding in nature (like guidelines or codes of conduct which set standards of conduct). National competent authorities may encourage their development, rewarding their implementation or making them enforceable. AI Codes of Conducts and Quality Labels may play a key role in developing computational sustainability for AI / ML Technologies, in parallel to the development of Hard Law Mechanisms based for example on an International Convention on Civil Liability for Algorithmic Damages or a Digital Geneva Convention.",['Eva Thelisson'],"['University of Fribourg, Fribourg, Switzerland']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Towards a critical race methodology in algorithmic fairness,"We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.","['Alex Hanna', 'Emily Denton', 'Andrew Smart', 'Jamila Smith-Loud']","['Google', 'Google', 'Google', 'Google']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Towards a Just Theory of Measurement: A Principled Social Measurement Assurance Program for Machine Learning,"While formal definitions of fairness in machine learning (ML) have been proposed, its place within a broader institutional model of fair decision-making remains ambiguous. In this paper we interpret ML as a tool for revealing when and how measures fail to capture purported constructs of interest, augmenting a given institution's understanding of its own interventions and priorities. Rather than codifying ""fair"" principles into ML models directly, the use of ML can thus be understood as a form of quality assurance for existing institutions, exposing the epistemic fault lines of their own measurement practices. Drawing from Friedler et al's [2016] recent discussion of representational mappings and previous discussions on the ontology of measurement, we propose a social measurement assurance program (sMAP) in which ML encourages expert deliberation on a given decision-making procedure by examining unanticipated or previously unexamined covariates. As an example, we apply Rawlsian principles of fairness to sMAP and produce a provisional just theory of measurement that would guide the use of ML for achieving fairness in the case of child abuse in Allegheny County.","['McKane Andrus', 'Thomas K. Gilbert']","['University of California, Berkeley, Berkeley, CA, USA', 'University of California, Berkeley, Berkeley, CA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Towards a more representative politics in the ethics of computer science,"Ethics curricula in computer science departments should include a focus on the political action of students. While 'ethics' holds significant sway over current discourse in computer science, recent work, particularly in data science, has shown that this discourse elides the underlying political nature of the problems that it aims to solve. In order to avoid these pitfalls---such as co-option, whitewashing, and assumed universal values---we should recognize and teach the political nature of computing technologies, largely through science and technology studies. Education is an essential focus not just intrinsically, but also because computing students end up joining the companies which have outsize impacts on our lives. At those companies, students both have a responsibility to society and agency beyond just engineering decisions, albeit not uniformly. I propose that we move away from strict ethics curricula and include examples of and calls for political action of students and future engineers. Through such examples---calls to action, practitioner reflections, legislative engagement, direct action---we might allow engineers to better recognize both their diverse agencies and responsibilities.",['Jared Moore'],['University of Washington'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure,"Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.","['Ben Hutchinson', 'Andrew Smart', 'Alex Hanna', 'Emily Denton', 'Christina Greer', 'Oddur Kjartansson', 'Parker Barnes', 'Margaret Mitchell']","['Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
"Towards an ""Ethics by Design"" Methodology for AI Research Projects","Addressing ethical issues arising from AI research, and by extension from most areas of Data Science, is a core challenge in both the academic and industry worlds. The nature of research and the specific set of technical skills involved imply that AI and Data Science researchers are not equipped to identify and anticipate such issues arising, or to establish solutions at the time a specific research project is being designed. In this paper, we discuss the need for a methodology for ethical research design that involves a broader set of skills from the start of the project. We specifically identify, from the relevant literature, a set of requirements that we argue to be needed for such a methodology. We then explore two case studies where such ethical considerations have been explored in conjunction with the development of specific research projects, in order to validate those assumptions and generalise them into a set of principles guiding an ""Ethics by Design"" method for conducting AI and Data Science research.","[""Mathieu d'Aquin"", 'Pinelopi Troullinou', ""Noel E. O'Connor"", 'Aindrias Cullen', 'Gráinne Faller', 'Louise Holden']","['National University of Ireland Galway, Galway, Ireland', 'The Open University, Milton Keynes, United Kingdom', 'Dublin City University, Dublin, Ireland', 'Dublin City University, Dublin, Ireland', 'FH Media Consulting Ltd, Dublin, Ireland', 'FH Media Consulting Ltd, Dublin, Ireland']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Towards Automatic Concept-based Explanations,"Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are salient for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \emph&lbrace;concept&rbrace; based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent and salient for the neural network's predictions.","['James Wexler', 'Been Kim']",Google,NeurIPS (2019),2019,TRUE
Towards Certificated Model Robustness Against Weight Perturbations,"This work studies the sensitivity of neural networks to weight perturbations, firstly corresponding to a newly developed threat model that perturbs the neural network parameters. We propose an efficient approach to compute a certified robustness bound of weight perturbations, within which neural networks will not make erroneous outputs as desired by the adversary. In addition, we identify a useful connection between our developed certification method and the problem of weight quantization, a popular model compression technique in deep neural networks (DNNs) and a ‘must-try’ step in the design of DNN inference engines on resource constrained computing platforms, such as mobiles, FPGA, and ASIC. Specifically, we study the problem of weight quantization – weight perturbations in the non-adversarial setting – through the lens of certificated robustness, and we demonstrate significant improvements on the generalization ability of quantized networks through our robustness-aware quantization scheme.","Lily Weng, Pu Zhao, Sijia Liu , Pin-yu Chen, Xue Lin, Luca Daniel",IBM,AAAI (2020),2020,TRUE
Towards Composable Bias Rating of AI Services,"A new wave of decision-support systems are being built today using AI services that draw insights from data (like text and video) and incorporate them in human-in-the-loop assistance. However, just as we expect humans to be ethical, the same expectation needs to be met by automated systems that increasingly get delegated to act on their behalf. A very important aspect of an ethical behavior is to avoid (intended, perceived, or accidental) bias. Bias occurs when the data distribution is not representative enough of the natural phenomenon one wants to model and reason about. The possibly biased behavior of a service is hard to detect and handle if the AI service is merely being used and not developed from scratch, since the training data set is not available. In this situation, we envisage a 3rd party rating agency that is independent of the API producer or consumer and has its own set of biased and unbiased data, with customizable distributions. We propose a 2-step rating approach that generates bias ratings signifying whether the AI service is unbiased compensating, data-sensitive biased, or biased. The approach also works on composite services. We implement it in the context of text translation and report interesting results.","['Biplav Srivastava', 'Francesca Rossi']","['IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,TRUE
Towards Cross-Lingual Generalization of Translation Gender Bias,"Cross-lingual generalization issues for less explored languages have been broadly tackled in recent NLP studies. In this study, we apply the philosophy on the problem of translation gender bias, which necessarily involves multilingualism and socio-cultural diversity. Beyond the conventional evaluation criteria for the social bias, we aim to put together various aspects of linguistic viewpoints into the measuring process, to create a template that makes evaluation less tilted to specific types of language pairs. With a manually constructed set of content words and template, we check both the accuracy of gender inference and the fluency of translation, for German, Korean, Portuguese, and Tagalog. Inference accuracy and disparate impact, namely the biasedness factors associated with each other, show that the failure of bias mitigation threatens the delicacy of translation. Furthermore, our analyses on each system and language indicate that the translation fluency and inference accuracy are not necessarily correlated. The results implicitly suggest that the amount of available language resources that boost up the performance might amplify the bias cross-linguistically.","['Won Ik Cho', 'Jiwon Kim', 'Jaeyeong Yang', 'Nam Soo Kim']","['Dept. of ECE and INMC, Seoul National University, Seoul, Korea', 'Independent Researcher, Daegu, Korea', 'Dept. of Linguistics, Seoul National University, Seoul, Korea', 'Dept. of ECE and INMC, Seoul National University, Seoul, Korea']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Towards Equitable AI for the Next Billion Users,"In this article, we present research provocations for AI in the Global South, to spur a conversation on the implicit beliefs, biases, and issues that may be normalized in AI. As much of AIâs functioning is still not well understood or fully developed, we believe these critical areas for research are crucial to shaping inclusive AI as it becomes more complex and powerful. We bring our perspectives as HCI researchers and social scientists that work closely with AI researchers. We have started to address some of these areas in our research and invite further explorations from the research community.","['Nithya Sambasivan', 'Jess Scon Holbrook']",Google,ACM interactions (2019),2019,TRUE
Towards Fair Deep Anomaly Detection,"Anomaly detection aims to find instances that are considered unusual and is a fundamental problem of data science. Recently, deep anomaly detection methods were shown to achieve superior results particularly in complex data such as images. Our work focuses on deep one-class classification for anomaly detection which learns a mapping only from the normal samples. However, the non-linear transformation performed by deep learning can potentially find patterns associated with social bias. The challenge with adding fairness to deep anomaly detection is to ensure both making fair and correct anomaly predictions simultaneously. In this paper, we propose a new architecture for the fair anomaly detection approach (Deep Fair SVDD) and train it using an adversarial network to de-correlate the relationships between the sensitive attributes and the learned representations. This differs from how fairness is typically added namely as a regularizer or a constraint. Further, we propose two effective fairness measures and empirically demonstrate that existing deep anomaly detection methods are unfair. We show that our proposed approach can remove the unfairness largely with minimal loss on the anomaly detection performance. Lastly, we conduct an in-depth analysis to show the strength and limitations of our proposed model, including parameter analysis, feature visualization, and run-time analysis.","['Hongjing Zhang', 'Ian Davidson']","['University of California, Davis', 'University of California, Davis']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy,"Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.","['Kaiyu Yang', 'Klint Qinami', 'Li Fei-Fei', 'Jia Deng', 'Olga Russakovsky']","['Princeton University', 'Princeton University', 'Stanford University', 'Princeton University', 'Princeton University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Towards Federated Learning at Scale: System Design,"Federated Learning is a distributed machine learning approach which enables training on a large corpus of data which never needs to leave user devices. We have spent some effort over the last two years building a scalable production system for FL. In this paper, we report about the resulting high-level design, sketching the challenges and the solutions, as well as touching the open problems and future directions.","['K. A. Bonawitz', 'Alex Ingerman', 'Jakub Konečný', 'Stefano Mazzocchi', 'Brendan McMahan', 'Daniel Ramage']",Google,SysML 2019,2019,TRUE
"Towards Just, Fair and Interpretable Methods for Judicial Subset Selection","In many judicial systems -- including the United States courts of appeals, the European Court of Justice, the UK Supreme Court and the Supreme Court of Canada -- a subset of judges is selected from the entire judicial body for each case in order to hear the arguments and decide the judgment. Ideally, the subset selected is representative, i.e., the decision of the subset would match what the decision of the entire judicial body would have been had they all weighed in on the case. Further, the process should be fair in that all judges should have similar workloads, and the selection process should not allow for certain judge's opinions to be silenced or amplified via case assignments. Lastly, in order to be practical and trustworthy, the process should also be interpretable, easy to use, and (if algorithmic) computationally efficient. In this paper, we propose an algorithmic method for the judicial subset selection problem that satisfies all of the above criteria. The method satisfies fairness by design, and we prove that it has optimal representativeness asymptotically for a large range of parameters and under noisy information models about judge opinions -- something no existing methods can provably achieve. We then assess the benefits of our approach empirically by counterfactually comparing against the current practice and recent alternative algorithmic approaches using cases from the United States courts of appeals database.","['Lingxiao Huang', 'Julia Wei', 'Elisa Celis']","['Yale University, New Haven, CT, USA', 'Yale University, New Haven, CT, USA', 'Yale University, New Haven, CT, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Towards Provably Moral AI Agents in Bottom-up Learning Frameworks,"We examine moral machine decision making as inspired by a central question posed by Rossi with respect to moral preferences: can AI systems based on statistical machine learning (which do not provide a natural way to explain or justify their decisions) be used for embedding morality into a machine in a way that allows us to prove that nothing morally wrong will happen? We argue for an evaluation which is held to the same standards as a human agent, removing the demand that ethical behaviour is always achieved. We introduce four key meta-qualities desired for our moral standards, and then proceed to clarify how we can prove that an agent will correctly learn to perform moral actions given a set of samples within certain error bounds. Our group-dynamic approach enables us to demonstrate that the learned models converge to a common function to achieve stability. We further explain a valuable intrinsic consistency check made possible through the derivation of logical statements from the machine learning model. In all, this work proposes an approach for building ethical AI systems, coming from the perspective of artificial intelligence research, and sheds important light on understanding how much learning is required in order for an intelligent agent to behave morally with negligible error.","['Nolan P. Shaw', 'Andreas Stöckel', 'Ryan W. Orr', 'Thomas F. Lidbetter', 'Robin Cohen']","['University of Waterloo, Waterloo, ON, Canada', 'University of Waterloo, Waterloo, ON, Canada', 'University of Waterloo, Waterloo, ON, Canada', 'University of Waterloo, Waterloo, ON, Canada', 'University of Waterloo, Waterloo, ON, Canada']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Towards Query Logs For Privacy Studies: On Deriving Search Queries From Questions,"Translating verbose information needs into crisp search queries is a phenomenon that is ubiquitous but hardly understood. Insights into this process could be valuable in several applications, including synthesizing large privacy-friendly query logs from public Web sources which are readily available to the academic research community. In this work, we take a step towards understanding query formulation by tapping into the rich potential of community question answering (CQA) forums. Specifically, we sample natural language (NL) questions spanning diverse themes from the Stack Exchange platform, and conduct a large-scale conversion experiment where crowdworkers submit search queries they would use when looking for equivalent information. We provide a careful analysis of this data, accounting for possible sources of bias during conversion, along with insights into user-specific linguistic patterns and search behaviors. We release a dataset of 7,000 question-query pairs from this study to facilitate further research on query understanding.","['Asia J. Biega', 'Jana Schmidt', 'Rishiraj Saha Roy']",Microsoft,ECIR 2020,2020-04-14,TRUE
Towards Query-efficient Black-box Adversary with Zeroth-order Natural Gradient Descent,"Despite the great achievements of the modern deep neural networks (DNNs), the vulnerability/robustness of state-of-the-art DNNs raises security concerns of DNNs in many application domains requiring high reliability. Various adversarial attacks are proposed to sabotage the learning performance of DNN models. Among those, the black-box adversarial attack methods have received special attentions owing to their practicality and simplicity. Black-box attacks usually prefer less queries in order to maintain stealthy and low costs. However, most of the current black-box attack methods adopt the first-order gradient descent method, which may come with certain deficiencies such as relatively slow convergence and high sensitivity to hyper-parameter settings. In this paper, we propose a zeroth-order natural gradient descent (ZO-NGD) method to design the adversarial attacks, which incorporates the zeroth-order gradient estimation technique catering to the black-box attack scenario and the second-order natural gradient descent to achieve higher query efficiency. The empirical evaluations on image classification datasets demonstrate that ZO-NGD can obtain significantly lower model query complexities compared with state-of-the-art attack methods.","Pu Zhao, Pin-yu Chen, Siyue Wang , Xue Lin",IBM,AAAI (2020),2020,TRUE
Towards Socially Responsible AI: Cognitive Bias-Aware Multi-Objective Learning,"Human society had a long history of suffering from cognitive biases  leading  to  social  prejudices  and  mass  injustice.  The prevalent  existence  of  cognitive  biases  in  large  volumes  of historical data can pose a threat of being manifested as un-ethical and seemingly inhumane predictions as outputs of AI systems trained on such data. To alleviate this problem, we propose a bias-aware multi-objective learning framework that given a set of identity attributes (e.g. gender, ethnicity etc.)and  a  subset  of  sensitive  categories  of  the  possible  classes of prediction outputs, learns to reduce the frequency of predicting certain combinations of them, e.g. predicting stereo-types such as â€˜most blacks use abusive languageâ€™, or â€˜fear isa virtue of womenâ€™. Our experiments conducted on an emotion prediction task with balanced class priors shows that a set of baseline bias-agnostic models exhibit cognitive biases with respect to gender, such as women are prone to be afraid whereas  men  are  more  prone  to  be  angry.  In  contrast,  our proposed bias-aware multi-objective learning methodology is shown to reduce such biases in the predicted emotions",Debasis Ganguly,IBM,AAAI (2020),2020,TRUE
Trade-offs in Fair Redistricting,"What constitutes a 'fair' electoral districting plan is a discussion dating back to the founding of the United States and, in light of several recent court cases, mathematical developments, and the approaching 2020 U.S. Census, is still a fiercely debated topic today. In light of the growing desire and ability to use algorithmic tools in drawing these districts, we discuss two prototypical formulations of fairness in this domain: drawing the districts by a neutral procedure or drawing them to intentionally induce an equitable electoral outcome. We then generate a large sample of districting plans for North Carolina and Pennsylvania and consider empirically how compactness and partisan symmetry, as instantiations of these frameworks, trade off with each other -- prioritizing the value of one of these necessarily comes at a cost in the other.",['Zachary Schutzman'],"['University of Pennsylvania, Philadelphia, PA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Training On-Device Ranking Models from Cross-User Interactions in a Privacy-Preserving Fashion,(See the attached PDF -- a one-page abstract for the upcoming DESIRES 2018 workshop),['Marc Najork'],Google,Proc. of the First Biennial Conference on Design of Experimental Search & Information Retrieval Systems (DESIRES) (2018),2018,TRUE
Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints,"Classifiers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other policy goals. We study the generalization performance for such constrained optimization problems, in terms of how well the constraints are satisfied at evaluation time, given that they are satisfied at training time. To improve generalization, we frame the problem as a two-player game where one player optimizes the model parameters on a training dataset, and the other player enforces the constraints on an independent validation dataset. We build on recent work in two-player constrained optimization to show that if one uses this two-dataset approach, then constraint generalization can be significantly improved. As we illustrate experimentally, this approach works not only in theory, but also in practice.","['Andrew Cotter', 'Serena Wang']",Google,International Conference on Machine Learning (2019),2019,TRUE
Transfer of Machine Learning Fairness across Domains,"If our models are used in new or unexpected cases, do we know if they will make fair predictions? Previously, researchers developed ways to debias a model for a single problem domain. However, this is often not how models are trained and used in practice. For example, labels and demographics (sensitive attributes) are often hard to observe, resulting in auxiliary or synthetic data to be used for training, and proxies of the sensitive attribute to be used for evaluation of fairness. A model trained for one setting may be picked up and used in many others, particularly as is common with pre-training and cloud APIs. Despite the pervasiveness of these complexities, remarkably little work in the fairness literature has theoretically examined these issues. We frame all of these settings as domain adaptation problems: how can we use what we have learned in a source domain to debias in a new target domain, without directly debiasing on the target domain as if it is a completely new problem? We offer new theoretical guarantees of improving fairness across domains, and offer a modeling approach to transfer to data-sparse target domains. We give empirical results validating the theory and showing that these modeling approaches can improve fairness metrics with less data.","['Xuezhi Wang', 'Alex Beutel', 'Ed H. Chi']",Google,"AI for social good workshop at Neurips, 2019",2019,TRUE
Transparency and Explanation in Deep Reinforcement Learning Neural Networks,"Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of ""object saliency maps"", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.","['Rahul Iyer', 'Yuezhang Li', 'Huao Li', 'Michael Lewis', 'Ramitha Sundar', 'Katia Sycara']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Google Inc., Mountain View, PA, USA', 'University of Pittsburgh, Pittsburgh, PA, USA', 'University of Pittsburgh, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,TRUE
"Transparent, Scrutable and Explainable User Models for Personalized Recommendation","Most recommender systems base their recommendations on implicit or explicit item-level feedback provided by users. These item ratings are combined into a complex user model, which then predicts the suitability of other items. While effective, such methods have limited scrutability and transparency. For instance, if a user's interests change, then many item ratings would usually need to be modified to significantly shift the user's recommendations. Similarly, explaining how the system characterizes the user is impossible, short of presenting the entire list of known item ratings. In this paper, we present a new set-based recommendation technique that permits the user model to be explicitly presented to users in natural language, empowering users to understand recommendations made and improve the recommendations dynamically. While performing comparably to traditional collaborative filtering techniques in a standard static setting, our approach allows users to efficiently improve recommendations. Further, it makes it easier for the model to be validated and adjusted, building user trust and understanding.",['Filip Radlinski'],Google,Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19) (2019),2019,TRUE
TrolleyMod v1.0: An Open-Source Simulation and Data-Collection Platform for Ethical Decision Making in Autonomous Vehicles,"This paper presents TrolleyMod v1.0, an open-source platform based on the CARLA simulator for the collection of ethical decision-making data for autonomous vehicles. This platform is designed to facilitate experiments aiming to observe and record human decisions and actions in high-fidelity simulations of ethical dilemmas that occur in the context of driving. Targeting experiments in the class of trolley problems, TrolleyMod provides a seamless approach to creating new experimental settings and environments with the realistic physics-engine and the high-quality graphical capabilities of CARLA and the Unreal Engine. Also, TrolleyMod provides a straightforward interface between the CARLA environment and Python to enable the implementation of custom controllers, such as deep reinforcement learning agents. The results of such experiments can be used for sociological analyses, as well as the training and tuning of value-aligned autonomous vehicles based on social values that are inferred from observations.","['Vahid Behzadan', 'James Minton', 'Arslan Munir']","['Kansas State University, Manhattan, KS, USA', 'Kansas State University, Manhattan, KS, USA', 'Kansas State University, Manhattan, KS, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Two Simple Ways to Learn Individual Fairness Metric from Data," Individual fairness was proposed to address some of the shortcomings of group fairness. Despite its benefits, it requires a task specific fairness metric that encodes our intuition of what is fair and what is unfair for the ML task at hand. Ambiguity in this metric is the main barrier to wider adoption of individual fairness. In this paper, we present two simple algorithms that learn effective fair metrics from a variety of datasets. We verify empirically that fair training with these metrics leads to improved fairness on three machine learning tasks susceptible to gender and racial biases. We also provide theoretical guarantees on the statistical performance of both approaches. ","Debarghya Mukherjee, Mikhail Yurochkin, Moulinath Banerjee, Yuekai Sun ","[""University of Michigan"", ""IBM Research""]",ICML 2020,2020-07,TRUE
Two-Sided Fairness for Repeated Matchings in Two-Sided Markets: A Case Study of a Ride-Hailing Platform,"Ride hailing platforms, such as Uber, Lyft, Ola or DiDi, have traditionally focused on the satisfaction of the passengers, or on boosting successful business transactions. However, recent studies provide a multitude of reasons to worry about the drivers in the ride hailing ecosystem. The concerns range from bad working conditions and worker manipulation to discrimination against minorities. With the sharing economy ecosystem growing, more and more drivers financially depend on online platforms and their algorithms to secure a living. It is pertinent to ask what a fair distribution of income on such platforms is and what power and means the platform has in shaping these distributions.","['Tom Sühr', 'Asia J. Biega', 'Meike Zehlike', 'Krishna P. Gummadi', 'Abhijnan Chakraborty']",Microsoft,KDD 2019,2019-08-04,TRUE
U.S. Public Opinion on the Governance of Artificial Intelligence,"Artificial intelligence (AI) has widespread societal implications, yet social scientists are only beginning to study public attitudes toward the technology. Existing studies find that the public's trust in institutions can play a major role in shaping the regulation of emerging technologies. Using a large-scale survey (N=2000), we examined Americans' perceptions of 13 AI governance challenges as well as their trust in governmental, corporate, and multistakeholder institutions to responsibly develop and manage AI. While Americans perceive all of the AI governance issues to be important for tech companies and governments to manage, they have only low to moderate trust in these institutions to manage AI applications.","['Baobao Zhang', 'Allan Dafoe']","['University of Oxford, Oxford, United Kingdom', 'University of Oxford, Oxford, United Kingdom']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Uncovering and Mitigating Algorithmic Bias through Learned Latent Structure,"Recent research has highlighted the vulnerabilities of modern machine learning based systems to bias, especially towards segments of society that are under-represented in training data. In this work, we develop a novel, tunable algorithm for mitigating the hidden, and potentially unknown, biases within training data. Our algorithm fuses the original learning task with a variational autoencoder to learn the latent structure within the dataset and then adaptively uses the learned latent distributions to re-weight the importance of certain data points while training. While our method is generalizable across various data modalities and learning tasks, in this work we use our algorithm to address the issue of racial and gender bias in facial detection systems. We evaluate our algorithm on the Pilot Parliaments Benchmark (PPB), a dataset specifically designed to evaluate biases in computer vision systems, and demonstrate increased overall performance as well as decreased categorical bias with our debiasing approach.","['Alexander Amini', 'Ava P. Soleimany', 'Wilko Schwarting', 'Sangeeta N. Bhatia', 'Daniela Rus']","['Massachusetts Institute of Technology, Cambridge, MA, USA', 'Harvard University, Boston, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
Understanding and correcting pathologies in the training of learned optimizers,"Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned optimizers may similarly outperform current hand-designed optimizers, especially for specific problems. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process. The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance. This allows us to train neural networks to perform optimization of a specific task faster than tuned first-order methods. Moreover, by training the optimizer against validation loss (as opposed to training loss), we are able to learn optimizers that train networks to generalize better than first order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks faster in wall-clock time compared to tuned first-order methods and with an improvement in test loss.","['Luke Metz', 'Niru Maheswaranathan', 'Daniel Freeman', 'Jascha Sohl-dickstein']",Google,ICML (2019),2019,TRUE
Understanding the Effect of Accuracy on Trust in Machine Learning Models,"


We address a relatively under-explored aspect of human–computer interaction: people’s abilities to understand the relationship between a machine learning model’s stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople’s trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model’s stated accuracy on held-out data and on its observed accuracy in practice. We find that people’s trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to re- cent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.


","['Ming Ying', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,2019 ACM CHI Conference on Human Factors in Computing Systems,2019-05-01,TRUE
Unravelling Robustness of Deep Learning Based Face Recognition Unravelling Robustness of Deep Learning based Face Recognition Against Adversarial Attacks,"Deep neural network (DNN) architecture based models have high expressive power and learning capacity. However, they are essentially a black box method since it is not easy to mathematically formulate the functions that are learned within its many layers of representation. Realizing this, many researchers have started to design methods to exploit the drawbacks of deep learning based algorithms questioning their robustness and exposing their singularities. In this paper, we attempt to unravel three aspects related to the robustness of DNNs for face recognition: (i) assessing the impact of deep architectures for face recognition in terms of vulnerabilities to attacks inspired by commonly observed distortions in the real world that are well handled by shallow learning methods along with learning based adversaries; (ii) detecting the singularities by characterizing abnormal filter response behavior in the hidden layers of deep networks; and (iii) making corrections to the processing pipeline to alleviate the problem. Our experimental evaluation using multiple open-source DNN-based face recognition networks, including OpenFace and VGG-Face, and two publicly available databases (MEDS and PaSC) demonstrates that the performance of deep learning based face recognition algorithms can suffer greatly in the presence of such distortions. The proposed method is also compared with existing detection algorithms and the results show that it is able to detect the attacks with very high accuracy by suitably designing a classifier using the response of the hidden layers in the network. Finally, we present several effective countermeasures to mitigate the impact of adversarial attacks and improve the overall robustness of DNN-based face recognition.","G. Goswami, N. Ratha, A. Agarwal, R. Singh, M. Vatsa",IBM,AAAI (2018),2018,TRUE
Upward Max Min Fairness,"Often one would like to allocate shared resources in a fair way. A common and well studied notion of fairness is Max-Min Fairness, where we first maximize the smallest allocation, and subject to that the second smallest, and so on. We consider a networking application where multiple commodities compete over the capacity of a network. In our setting each commodity has multiple possible paths to route its demand (for example, a network using MPLS tunneling). In this setting, the only known way of finding a max-min fair allocation requires an iterative solution of multiple linear programs. Such an approach, although polynomial time, scales badly with the size of the network, the number of demands, and the number of paths. More importantly, a network operator has limited control and understanding of the inner working of the algorithm. Finally, this approach is inherently centralized and cannot be implemented via a distributed protocol. In this paper we introduce Upward Max-Min Fairness, a novel relaxation of Max-Min Fairness and present a family of simple dynamics that converge to it. These dynamics can be implemented in a distributed manner. Moreover, we present an efficient combinatorial algorithm for finding an upward max-min fair allocation, which is a natural extension of the well known Water Filling Algorithm for a multiple path setting. We test the expected behavior of this new algorithm and show that on realistic networks upward max-min fair allocations are comparable to the max-min fair allocations both in fairness and in network utilization.","['Avinatan Hassidim', 'Haim Kaplan', 'Alok Kumar', 'Danny Raz', 'Michal Segalov']",Google,INFOCOM (2012),2012,TRUE
Using Contextual Bandits with Behavioral Constraints for Constrained Online Movie Recommendation,"AI systems that learn through reward feedback about the actions they take are increasingly deployed in domains that have significant impact on our daily life. In many cases the rewards should not be the only guiding criteria, as there are additional constraints and/or priorities imposed by regulations, values, preferences, or ethical principles. We detail a novel online system, based on an extension of the contextual bandits framework, that learns a set of behavioral constraints by observation and uses these constraints as a guide when making decisions in an online setting while still being reactive to reward feedback. In addition, our system can highlight features of the context which are more predicted to be more rewarding and/or are in line with the behavioral constraints. We demonstrate the system by building an interactive interface for an online movie recommendation agent and show that our system is able to act within a set of behavior constraints without significantly degrading overall performance.","A. Balakrishnan, D. Bouneffouf, N. Mattei and F. Rossi",IBM,IJCAI (2018),2018,TRUE
Using Deceased-Donor Kidneys to Initiate Chains of Living Donor Kidney Paired Donations: Algorithm and Experimentation,"We design a flexible algorithm that exploits deceased donor kidneys to initiate chains of living donor kidney paired donations, combining deceased and living donor allocation mechanisms to improve the quantity and quality of kidney transplants. The advantages of this approach have been measured using retrospective data on the pool of donor/recipient incompatible and desensitized pairs at the Padua University Hospital, the largest center for living donor kidney transplants in Italy. The experiments show a remarkable improvement on the number of patients with incompatible donor who could be transplanted, a decrease in the number of desensitization procedures, and an increase in the number of UT patients (that is, patients unlikely to be transplanted for immunological reasons) in the waiting list who could receive an organ.","['Cristina Cornelio', 'Lucrezia Furian', 'Antonio Nicolò', 'Francesca Rossi']","['IBM Research, Yorktown Heights, NY, USA', 'University of Padova, Padova, Italy', 'University of Padova & University of Manchester, Padova, Italy', 'IBM Research & University of Padova, Yorktown Heights, NY, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,TRUE
Utilizing Housing Resources for Homeless Youth Through the Lens of Multiple Multi-Dimensional Knapsacks,"There are over 1 million homeless youth in the U.S. each year. To reduce homelessness, U.S. Housing and Urban Development (HUD) and housing communities provide housing programs/services to homeless youth with the goal of improving their long-term situation. Housing communities are facing a difficult task of filling their housing programs, with as many youths as possible, subject to resource constraints for meeting the needs of youth. Currently, the assignment is manually done by humans working in the housing communities. In this paper, we consider the problem of assigning homeless youth to housing programs subject to resource constraints. We provide an initial abstract model for this setting and show that the problem of maximizing the total assigned youth to the programs under this model is APX-hard. To solve the problem, we non-trivially formulate it as a multiple multi-dimensional knapsack problem (MMDKP), which is not known to have any approximation algorithm. We provide a first interpretable and easy-to-use greedy algorithm with logarithmic approximation ratio for solving general MMDKP. We conduct experiments on random and realistic instances of the housing assignment settings and show that our algorithm is efficient and effective in solving large instances (up to 1 million youth).","['Hau Chan', 'Long Tran-Thanh', 'Bryan Wilder', 'Eric Rice', 'Phebe Vayanos', 'Milind Tambe']","['University of Nebraska-Lincoln, Lincoln, NE, USA', 'University of Southampton, Southampton, United Kingdom', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
"Value Alignment, Fair Play, and the Rights of Service Robots","Ethics and safety research in artificial intelligence is increasingly framed in terms of ""alignment'' with human values and interests. I argue that Turing's call for ""fair play for machines'' is an early and often overlooked contribution to the alignment literature. Turing's appeal to fair play suggests a need to correct human behavior to accommodate our machines, a surprising inversion of how value alignment is treated today. Reflections on ""fair play'' motivate a novel interpretation of Turing's notorious ""imitation game'' as a condition not of intelligence but instead of value alignment : a machine demonstrates a minimal degree of alignment (with the norms of conversation, for instance) when it can go undetected when interrogated by a human. I carefully distinguish this interpretation from the Moral Turing Test, which is not motivated by a principle of fair play, but instead depends on imitation of human moral behavior. Finally, I consider how the framework of fair play can be used to situate the debate over robot rights within the alignment literature. I argue that extending rights to service robots operating in public spaces is ""fair'' in precisely the sense that it encourages an alignment of interests between humans and machines.",['Daniel Estrada'],"['New Jersey Institute of Technology, Newark, NJ, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
Value Cards: An Educational Toolkit for Teaching Social Impacts of Machine Learning through Deliberation,"Recently, there have been increasing calls for computer science curricula to complement existing technical training with topics related to Fairness, Accountability, Transparency and Ethics (FATE). In this paper, we present Value Cards, an educational toolkit to inform students and practitioners the social impacts of different machine learning models via deliberation. This paper presents an early use of our approach in a college-level computer science course. Through an in-class activity, we report empirical data for the initial effectiveness of our approach. Our results suggest that the use of the Value Cards toolkit can improve students' understanding of both the technical definitions and trade-offs of performance metrics and apply them in real-world contexts, help them recognize the significance of considering diverse social values in the development and deployment of algorithmic systems, and enable them to communicate, negotiate and synthesize the perspectives of diverse stakeholders. Our study also demonstrates a number of caveats we need to consider when using the different variants of the Value Cards toolkit. Finally, we discuss the challenges as well as future applications of our approach.","['Hong Shen', 'Wesley H. Deng', 'Aditi Chattopadhyay', 'Zhiwei Steven Wu', 'Xu Wang', 'Haiyi Zhu']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'University of California, Berkeley Berkeley, CA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'University of Michigan Ann Arbor, MI, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Value-laden disciplinary shifts in machine learning,"As machine learning models are increasingly used for high-stakes decision making, scholars have sought to intervene to ensure that such models do not encode undesirable social and political values. However, little attention thus far has been given to how values influence the machine learning discipline as a whole. How do values influence what the discipline focuses on and the way it develops? If undesirable values are at play at the level of the discipline, then intervening on particular models will not suffice to address the problem. Instead, interventions at the disciplinary-level are required. This paper analyzes the discipline of machine learning through the lens of philosophy of science. We develop a conceptual framework to evaluate the process through which types of machine learning models (e.g. neural networks, support vector machines, graphical models) become predominant. The rise and fall of model-types is often framed as objective progress. However, such disciplinary shifts are more nuanced. First, we argue that the rise of a model-type is self-reinforcing-it influences the way model-types are evaluated. For example, the rise of deep learning was entangled with a greater focus on evaluations in compute-rich and data-rich environments. Second, the way model-types are evaluated encodes loaded social and political values. For example, a greater focus on evaluations in compute-rich and data-rich environments encodes values about centralization of power, privacy, and environmental concerns.","['Ravit Dotan', 'Smitha Milli']","['University of California', 'University of California']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Variational Inference of Disentangled Latent Concepts from Unlabeled Observations,"Disentangled representations, where the higher level data generative factors are reflected in disjoint latent dimensions, offer several benefits such as ease of deriving invariant representations, transferability to other tasks, interpretability, etc. We consider the problem of unsupervised learning of disentangled representations from large pool of unlabeled observations, and propose a variational inference based approach to infer disentangled latent factors. We introduce a regularizer on the expectation of the approximate posterior over observed data that encourages the disentanglement. We also propose a new disentanglement metric which is better aligned with the qualitative disentanglement observed in the decoder’s output. We empirically observe significant improvement over existing methods in terms of both disentanglement and data likelihood (reconstruction quality).","Abhishek Kumar, Prasanna Sattigeri, Avinash Balakrishnan",IBM,ICLR (2018),2018,TRUE
WCMP: Weighted Cost Multipathing for Improved Fairness in Data Centers,"Data Center topologies employ multiple paths among servers to deliver scalable, cost-effective network capacity. The simplest and the most widely deployed approach for load balancing among these paths, Equal Cost Multipath (ECMP), hashes flows among the shortest paths toward a destination. ECMP leverages uniform hashing of balanced flow sizes to achieve fairness and good load balancing in data centers. However, we show that ECMP further assumes a balanced, regular, and fault-free topology, which are invalid assumptions in practice that can lead to substantial performance degradation and, worse, variation in flow bandwidths even for same size flows.","['Junlan Zhou', 'Leon Poutievski', 'Amin Vahdat']",Google,EuroSys '14: Proceedings of the Ninth European Conference on Computer Systems (2014),2014,TRUE
What are the Biases in My Word Embedding?,"This paper presents an algorithm for enumerating biases in word embeddings. The algorithm exposes a large number of offensive associations related to sensitive features such as race and gender on publicly available embeddings, including a supposedly ""debiased"" embedding. These biases are concerning in light of the widespread use of word embeddings. The associations are identified by geometric patterns in word embeddings that run parallel between people's names and common lower-case tokens. The algorithm is highly unsupervised: it does not even require the sensitive features to be pre-specified. This is desirable because: (a) many forms of discrimination?such as racial discrimination-are linked to social constructs that may vary depending on the context, rather than to categories with fixed definitions; and (b) it makes it easier to identify biases against intersectional groups, which depend on combinations of sensitive features. The inputs to our algorithm are a list of target tokens, e.g. names, and a word embedding. It outputs a number of Word Embedding Association Tests (WEATs) that capture various biases present in the data. We illustrate the utility of our approach on publicly available word embeddings and lists of names, and evaluate its output using crowdsourcing. We also show how removing names may not remove potential proxy bias.","['Nathaniel Swinger', 'Maria De-Arteaga', 'Neil Thomas Heffernan IV', 'Mark DM Leiserson', 'Adam Tauman Kalai']","['Lexington High School, Lexington, MA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Shrewsbury High School, Shrewsbury, MA, USA', 'University of Maryland, Flibbertigibbet, MD, USA', 'Microsoft Research, Cambridge, MA, USA']","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",January 2019,FALSE
"What does it mean to 'solve' the problem of discrimination in hiring?: social, technical and legal perspectives from the UK on automated hiring systems","Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective. In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.","['Javier Sánchez-Monedero', 'Lina Dencik', 'Lilian Edwards']","['Cardiff University, Cardiff, Wales, United Kingdom', 'Cardiff University, Cardiff, Wales, United Kingdom', 'University of Newcastle, Newcastle upon Tyne, England, United Kingdom']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability,"As research on algorithms and their impact proliferates, so do calls for scrutiny/accountability of algorithms. A systematic review of the work that has been done in the field of 'algorithmic accountability' has so far been lacking. This contribution puts forth such a systematic review, following the PRISMA statement. 242 English articles from the period 2008 up to and including 2018 were collected and extracted from Web of Science and SCOPUS, using a recursive query design coupled with computational methods. The 242 articles were prioritized and ordered using affinity mapping, resulting in 93 'core articles' which are presented in this contribution. The recursive search strategy made it possible to look beyond the term 'algorithmic accountability'. That is, the query also included terms closely connected to the theme (e.g. ethics and AI, regulation of algorithms). This approach allows for a perspective not just from critical algorithm studies, but an interdisciplinary overview drawing on material from data studies to law, and from computer science to governance studies. To structure the material, Bovens's widely accepted definition of accountability serves as a focal point. The material is analyzed on the five points Bovens identified as integral to accountability: its arguments on (1) the actor, (2) the forum, (3) the relationship between the two, (3) the content and criteria of the account, and finally (5) the consequences which may result from the account. The review makes three contributions. First, an integration of accountability theory in the algorithmic accountability discussion. Second, a cross-sectoral overview of the that same discussion viewed in light of accountability theory which pays extra attention to accountability risks in algorithmic systems. Lastly, it provides a definition of algorithmic accountability based on accountability theory and algorithmic accountability literature.",['Maranke Wieringa'],"['Utrecht University, Utrecht, The Netherlands']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
"What We Can't Measure, We Can't Understand: Challenges to Demographic Data Procurement in the Pursuit of Fairness","As calls for fair and unbiased algorithmic systems increase, so too does the number of individuals working on algorithmic fairness in industry. However, these practitioners often do not have access to the demographic data they feel they need to detect bias in practice. Even with the growing variety of toolkits and strategies for working towards algorithmic fairness, they almost invariably require access to demographic attributes or proxies. We investigated this dilemma through semi-structured interviews with 38 practitioners and professionals either working in or adjacent to algorithmic fairness. Participants painted a complex picture of what demographic data availability and use look like on the ground, ranging from not having access to personal data of any kind to being legally required to collect and use demographic data for discrimination assessments. In many domains, demographic data collection raises a host of difficult questions, including how to balance privacy and fairness, how to define relevant social categories, how to ensure meaningful consent, and whether it is appropriate for private companies to infer someone's demographics. Our research suggests challenges that must be considered by businesses, regulators, researchers, and community groups in order to enable practitioners to address algorithmic bias in practice. Critically, we do not propose that the overall goal of future work should be to simply lower the barriers to collecting demographic data. Rather, our study surfaces a swath of normative questions about how, when, and whether this data should be procured, and, in cases where it is not, what should still be done to mitigate bias.","['McKane Andrus', 'Elena Spitzer', 'Jeffrey Brown', 'Alice Xiang']","['Partnership on AI', 'Partnership on AI', 'Partnership on AI, Minnesota State University, Mankato', 'Sony AI, Partnership on AI']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
"What's Next for AI Ethics, Policy, and Governance? A Global Overview","Since 2016, more than 80 AI ethics documents - including codes, principles, frameworks, and policy strategies - have been produced by corporations, governments, and NGOs. In this paper, we examine three topics of importance related to our ongoing empirical study of ethics and policy issues in these emerging documents. First, we review possible challenges associated with the relative homogeneity of the documents' creators. Second, we provide a novel typology of motivations to characterize both obvious and less obvious goals of the documents. Third, we discuss the varied impacts these documents may have on the AI governance landscape, including what factors are relevant to assessing whether a given document is likely to be successful in achieving its goals.","['Daniel Schiff', 'Justin Biddle', 'Jason Borenstein', 'Kelly Laas']","['Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA', 'Georgia Institute of Technology, Atlanta, GA, USA', 'Illinois Institute of Technology, Chicago, IL, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
What's sex got to do with machine learning?,"The debate about fairness in machine learning has largely centered around competing substantive definitions of what fairness or nondiscrimination between groups requires. However, very little attention has been paid to what precisely a group is. Many recent approaches have abandoned observational, or purely statistical, definitions of fairness in favor of definitions that require one to specify a causal model of the data generating process. The implicit ontological assumption of these exercises is that a racial or sex group is a collection of individuals who share a trait or attribute, for example: the group ""female"" simply consists in grouping individuals who share female-coded sex features. We show this by exploring the formal assumption of modularity in causal models using directed acyclic graphs (DAGs), which hold that the dependencies captured by one causal pathway are invariant to interventions on any other causal pathways. Modeling sex, for example, as a node in a causal model aimed at elucidating fairness questions proposes two substantive claims: 1) There exists a feature, sex-on-its-own, that is an inherent trait of an individual that then (causally) brings about social phenomena external to it in the world; and 2) the relations between sex and its downstream effects can be modified in whichever ways and the former node would still retain the meaning that sex has in our world. Together, these claims suggest sex to be a category that could be different in its (causal) relations with other features of our social world via hypothetical interventions yet still mean what it means in our world. This fundamental stability of categories and causes (unless explicitly intervened on) is essential in the methodology of causal inference, because without it, causal operations can alter the meaning of a category, fundamentally change how it is situated within a causal diagram, and undermine the validity of any inferences drawn on the diagram as corresponding to any real phenomena in the world. We argue that these methods' ontological assumptions about social groups such as sex are conceptual errors. Many of the ""effects"" that sex purportedly ""causes"" are in fact constitutive features of sex as a social status. They constitute what it means to be sexed. In other words, together, they give the social meaning of sex features. These social meanings are precisely, we argue, what makes sex discrimination a distinctively morally problematic type of act that differs from mere irrationality or meanness on the basis of a physical feature. Correcting this conceptual error has a number of important implications for how analytical models can be used to detect discrimination. If what makes something discrimination on the basis of a particular social grouping is that the practice acts on what it means to be in that group in a way that we deem wrongful, then what we need from analytical diagrams is a model of what constitutes the social grouping. Such a model would allow us to explain the special moral (and legal) reasons we have to be concerned with the treatment of this category by reference to the empirical social relations and meanings that establish the category as what it is. Only then can we have the normative debate about what is fair or nondiscriminatory vis-à-vis that group. We suggest that formal diagrams of constitutive relations would present an entirely different path toward reasoning about discrimination (and relatedly, counterfactuals) because they proffer a model of how the meaning of a social group emerges from its constitutive features. Whereas the value of causal diagrams is to guide the construction and testing of sophisticated modular counterfactuals, the value of constitutive diagrams would be to identify a different kind of counterfactual as central to our inquiry into discrimination: one that asks how the social meaning of a group would be changed if its non-modular features were altered.","['Lily Hu', 'Issa Kohler-Hausmann']","['Harvard University', 'Yale University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
What's up with Privacy?: User Preferences and Privacy Concerns in Intelligent Personal Assistants,"The recent breakthroughs in Artificial Intelligence (AI) have allowed individuals to rely on automated systems for a variety of reasons. Some of these systems are the currently popular voice-enabled systems like Echo by Amazon and Home by Google that are also called as Intelligent Personal Assistants (IPAs). Though there are rising concerns about privacy and ethical implications, users of these IPAs seem to continue using these systems. We aim to investigate to what extent users are concerned about privacy and how they are handling these concerns while using the IPAs. By utilizing the reviews posted online along with the responses to a survey, this paper provides a set of insights about the detected markers related to user interests and privacy challenges. The insights suggest that users of these systems irrespective of their concerns about privacy, are generally positive in terms of utilizing IPAs in their everyday lives. However, there is a significant percentage of users who are concerned about privacy and take further actions to address related concerns. Some percentage of users expressed that they do not have any privacy concerns but when they learned about the ""always listening"" feature of these devices, their concern about privacy increased.","['Lydia Manikonda', 'Aditya Deotale', 'Subbarao Kambhampati']","['Arizona State University, Tempe, AZ, USA', 'Arizona State University, Tempe, AZ, USA', 'Arizona State University, Tempe, AZ, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
What’s in a Name? Reducing Bias in Bios without Access to Protected Attributes,"There is a growing body of work that proposes methods for mitigating bias in machine learning systems. These methods typically rely on access to protected attributes such as race, gender, or age. However, this raises two significant challenges: (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classiﬁcation, we propose a method for discouraging correlation between the predicted probability of an individual’s true occupation and a word embedding of their name. This method leverages the societal biases that are encoded in word embeddings, eliminating the need for access to protected attributes. Crucially, it only requires access to individuals’ names at training time and not at deployment time. We evaluate two variations of our proposed method using a large-scale data set of online biographies. We ﬁnd that both variations simultaneously reduce race and gender biases, with almost no reduction in the classiﬁer’s overall true positive rate.","['Alexey Romanov', 'Maria De-Arteaga', 'Hanna Wallach', 'Jennifer Chayes', 'Christian Borgs', 'Alexandra Chouldechova', 'Sahin Geyik', 'krishnaram Kenthapadi', 'Anna Rumshisky', 'Adam Tauman Kalai']",Microsoft,NAACL 2019,2019-04-10,TRUE
When Are Search Completion Suggestions Problematic?,"Problematic web search query completion suggestions—perceived as biased, offensive, or in some other way harmful—can reinforce existing stereotypes and misbeliefs, and even nudge users towards undesirable patterns of behavior.  Locating such suggestions is difficult, not only due to the long-tailed nature of web search, but also due to differences in how people assess potential harms.  Grounding our study in web search query logs, we explore when system-provided suggestions might be perceived as problematic through a series of crowd-experiments where we systematically manipulate: the search query fragments provided by users,  possible user search intents, and the list of query completion suggestions.  To examine why query suggestions might be perceived as problematic, we contrast them to an inventory of known types of problematic suggestions. We report our observations around differences in the prevalence of  a) suggestions that are problematic on their own versus  b) suggestions that are problematic for the query fragment provided by a user, for both common informational needs and in the presence of web search voids—topics searched by few to no users.  Our experiments surface a rich array of scenarios where suggestions are considered problematic, including due to the context in which they were surfaced. Compounded by the elusive nature of many such scenarios, the prevalence of suggestions perceived as problematic only for certain user inputs, raises concerns about blind spots due to data annotation practices that may lead to some types of problematic suggestions being overlooked.","['Alexandra Olteanu', 'Fernando Diaz', 'Gabriella Kazai']",Microsoft,Computer Supported Collaborative Work and Social Computing (CSCW),2020-08-01,TRUE
When Do People Want AI to Make Decisions?,"AI systems are now or will soon be sophisticated enough to make consequential decisions. Although this technology has flourished, we also need public appraisals of AI systems playing these more important roles. This article reports surveys of preferences for and against AI systems making decisions in various domains as well as experiments that intervene on these preferences. We find that these preferences are contingent on subjects' previous exposure to computer systems making these kinds of decisions, and some interventions designed to mimic previous exposure successfully encourage subjects to be more hospitable to computer systems making these weighty decisions.","['Max F. Kramer', 'Jana Schaich Borg', 'Vincent Conitzer', 'Walter Sinnott-Armstrong']","['University of Arizona, Tucson, AZ, USA', 'Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA', 'Duke University, Durham, NC, USA']","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",December 2018,FALSE
"When not to design, build, or deploy","Recent debate within the FAT* community has focused on how the field conceptualizes the problems it seeks to address, what approach the field should take in attempting to address these problems, and whether the field should even pursue some of the proposed remedies. Questions regarding when not to design, build, or deploy a technology are perhaps the most common expression of this trend. Identifying the problems to address is inextricably linked to the broader question of how to collectively make decisions about what technologies our societies need and want.","['Solon Barocas', 'Asia J. Biega', 'Benjamin Fish', 'Jędrzej Niklas', 'Luke Stark']",Microsoft,"2020 Conference on Fairness, Accountability, and Transparency (FAT* '20)",2020-01-01,TRUE
When the Umpire is also a Player: Bias in Private Label Product Recommendations on E-commerce Marketplaces,"Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about the fairness of black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own 'private label' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace 'organic' recommendations with ad-driven 'sponsored' recommendations, which include their own private labels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any public audit of these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematic audit of related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazon private label products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.","['Abhisek Dash', 'Abhijnan Chakraborty', 'Saptarshi Ghosh', 'Animesh Mukherjee', 'Krishna P. Gummadi']","['Indian Institute of Technology, Kharagpur, India', 'Indian Institute of Technology, Delhi, India', 'Indian Institute of Technology, Kharagpur, India', 'Indian Institute of Technology, Kharagpur, India', 'Max Planck Institute for Software, Systems, Germany']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
When Trusted Black Boxes Don't Agree: Incentivizing Iterative Improvement and Accountability in Critical Software Systems,"Software increasingly plays a key role in regulated areas like housing, hiring, and credit, as well as major public functions such as criminal justice and elections. It is easy for there to be unintended defects with a large impact on the lives of individuals and society as a whole. Preventing, finding, and fixing software defects is a key focus of both industrial software development efforts as well as academic research in software engineering. In this paper, we discuss flaws in the larger socio-technical decision-making processes in which critical black-box software systems are developed, deployed, and trusted. We use criminal justice software, specifically probabilistic genotyping (PG) software, as a concrete example. We describe how PG software systems, designed to do the same job, produce different results. We highlight the under-appreciated impact of changes in key parameters and the disparate impact that one such parameter can have on different racial/ethnic groups. We propose concrete changes to the socio-technical decision-making processes surrounding the use of PG software that could be used to incentivize iterative improvements in the accuracy, fairness, reliability, and accountability of these systems.","['Jeanna Neefe Matthews', 'Graham Northup', 'Isabella Grasso', 'Stephen Lorenz', 'Marzieh Babaeianjelodar', 'Hunter Bashaw', 'Sumona Mondal', 'Abigail Matthews', 'Mariama Njie', 'Jessica Goldthwaite']","['Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'University of Wisconsin Madison, Madison, WI, USA', 'Iona College, New York, NY, USA', 'The Legal Aid Society, New York, NY, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
When Your Only Tool Is A Hammer: Ethical Limitations of Algorithmic Fairness Solutions in Healthcare Machine Learning,"It is no longer a hypothetical worry that artificial intelligence - more specifically, machine learning (ML) - can propagate the effects of pernicious bias in healthcare. To address these problems, some have proposed the development of 'algorithmic fairness' solutions. The primary goal of these solutions is to constrain the effect of pernicious bias with respect to a given outcome of interest as a function of one's protected identity (i.e., characteristics generally protected by civil or human rights legislation. The technical limitations of these solutions have been well-characterized. Ethically, the problematic implication - of developers, potentially, and end users - is that by virtue of algorithmic fairness solutions a model can be rendered 'objective' (i.e., free from the influence of pernicious bias). The ostensible neutrality of these solutions may unintentionally prompt new consequences for vulnerable groups by obscuring downstream problems due to the persistence of real-world bias. The main epistemic limitation of algorithmic fairness is that it assumes the relationship between the extent of bias's impact on a given health outcome and one's protected identity is mathematically quantifiable. The reality is that social and structural factors confluence in complex and unknown ways to produce health inequalities. Some of these are biologic in nature, and differences like these are directly relevant to predicting a health event and should be incorporated into the model's design. Others are reflective of prejudice, lack of access to healthcare, or implicit bias. Sometimes, there may be a combination. With respect to any specific task, it is difficult to untangle the complex relationships between potentially influential factors and which ones are 'fair' and which are not to inform their inclusion or mitigation in the model's design.","['Melissa McCradden', 'Mjaye Mazwi', 'Shalmali Joshi', 'James A. Anderson']","['The Hospital for Sick Children, Toronto, ON, Canada', 'The Hospital for Sick Children, Toronto, ON, Canada', 'Vector Institute for Artificial Intelligence, Toronto, ON, Canada', 'The Hospital for Sick Children, Toronto, ON, Canada']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,FALSE
Who's the Guinea Pig?: Investigating Online A/B/n Tests in-the-Wild,"A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization. These tests are often run on their websites without explicit consent from users. In this paper, we investigate such online A/B/n tests by using Optimizely as a lens. First, we provide measurement results of 575 websites that use Optimizely drawn from the Alexa Top-1M, and analyze the distributions of their audiences and experiments. Then, we use three case studies to discuss potential ethical pitfalls of such experiments, including involvement of political content, price discrimination, and advertising campaigns. We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators.","['Shan Jiang', 'John Martin', 'Christo Wilson']","['Northeastern University', 'Northeastern University', 'Northeastern University']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
"Whose side are ethics codes on?: power, responsibility and the social good","The moral authority of ethics codes stems from an assumption that they serve a unified society, yet this ignores the political aspects of any shared resource. The sociologist Howard S. Becker challenged researchers to clarify their power and responsibility in the classic essay: Whose Side Are We On. Building on Becker's hierarchy of credibility, we report on a critical discourse analysis of data ethics codes and emerging conceptualizations of beneficence, or the ""social good"", of data technology. The analysis revealed that ethics codes from corporations and professional associations conflated consumers with society and were largely silent on agency. Interviews with community organizers about social change in the digital era supplement the analysis, surfacing the limits of technical solutions to concerns of marginalized communities. Given evidence that highlights the gulf between the documents and lived experiences, we argue that ethics codes that elevate consumers may simultaneously subordinate the needs of vulnerable populations. Understanding contested digital resources is central to the emerging field of public interest technology. We introduce the concept of digital differential vulnerability to explain disproportionate exposures to harm within data technology and suggest recommendations for future ethics codes..","['Anne L. Washington', 'Rachel Kuo']","['New York University', 'New York University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Whose tweets are surveilled for the police: an audit of a social-media monitoring tool via log files,"Social media monitoring by law enforcement is becoming commonplace, but little is known about what software packages for it do. Through public records requests, we obtained log files from the Corvallis (Oregon) Police Department's use of social media monitoring software called DigitalStakeout. These log files include the results of proprietary searches by DigitalStakeout that were running over a period of 13 months and include 7240 social media posts. In this paper, we focus on the Tweets logged in this data and consider the racial and ethnic identity (through manual coding) of the users that are therein flagged by DigitalStakeout. We observe differences in the demographics of the users whose Tweets are flagged by DigitalStakeout compared to the demographics of the Twitter users in the region, however, our sample size is too small to determine significance. Further, the demographics of the Twitter users in the region do not seem to reflect that of the residents of the region, with an apparent higher representation of Black and Hispanic people. We also reconstruct the keywords related to a Narcotics report set up by DigitalStakeout for the Corvallis Police Department and find that these keywords flag Tweets unrelated to narcotics or flag Tweets related to marijuana, a drug that is legal for recreational use in Oregon. Almost all of the keywords have a common meaning unrelated to narcotics (e.g. broken, snow, hop, high) that call into question the utility that such a keyword based search could have to law enforcement. As social media monitoring is increasingly used for law enforcement purposes, racial biases in surveillance may contribute to existing racial disparities in law enforcement practices. We are hopeful that log files obtainable through public records request will shed light on the operation of these surveillance tools. There are challenges in auditing these tools: public records requests may go unfulfilled even if the data is available, social media platforms may not provide comparable data for comparison with surveillance data, demographics can be difficult to ascertain from social media and Institutional Review Boards may not understand how to weigh the ethical considerations involved in this type of research. We include in this paper a discussion of our experience in navigating these issues.","['Glencora Borradaile', 'Brett Burkhardt', 'Alexandria LeClerc']","['Oregon State University', 'Oregon State University', 'Oregon State University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Why does my model fail?: contrastive local explanations for retail forecasting,"In various business settings, there is an interest in using more complex machine learning techniques for sales forecasting. It is difficult to convince analysts, along with their superiors, to adopt these techniques since the models are considered to be ""black boxes,"" even if they perform better than current models in use. We examine the impact of contrastive explanations about large errors on users' attitudes towards a ""black-box"" model. We propose an algorithm, Monte Carlo Bounds for Reasonable Predictions. Given a large error, MC-BRP determines (1) feature values that would result in a reasonable prediction, and (2) general trends between each feature and the target, both based on Monte Carlo simulations. We evaluate on a real dataset with real users by conducting a user study with 75 participants to determine if explanations generated by MC-BRP help users understand why a prediction results in a large error, and if this promotes trust in an automatically-learned model. Our study shows that users are able to answer objective questions about the model's predictions with overall 81.1% accuracy when provided with these contrastive explanations. We show that users who saw MC-BRP explanations understand why the model makes large errors in predictions significantly more than users in the control group. We also conduct an in-depth analysis of the difference in attitudes between Practitioners and Researchers, and confirm that our results hold when conditioning on the users' background.","['Ana Lucic', 'Hinda Haned', 'Maarten de Rijke']","['University of Amsterdam, Amsterdam, Netherlands', 'Ahold Delhaize, Zaandam, Netherlands', 'University of Amsterdam, Amsterdam, Netherlands']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Why Reliabilism Is not Enough: Epistemic and Moral Justification in Machine Learning,"In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of \em justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed wide adoption of machine learning? We argue that, in general, people implicitly adoptreliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method \citegoldman2012reliabilism. We argue that, in cases where model deployments require \em moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral ""wrapper'' around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification---moral justification. Finally, we offer cautions relevant to the (implicit or explicit) adoption of the reliabilist interpretation of machine learning.","['Andrew Smart', 'Larry James', 'Ben Hutchinson', 'Simone Wu', 'Shannon Vallor']","['Google, San Francisco, CA, USA', 'Google, San Francisco, CA, USA', 'Google, San Francisco, CA, USA', 'Google, Seattle, WA, USA', 'Google, Mountain View, CA, USA']","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",February 2020,TRUE
You Can't Sit With Us: Exclusionary Pedagogy in AI Ethics Education,"Given a growing concern about the lack of ethical consideration in the Artificial Intelligence (AI) field, many have begun to question how dominant approaches to the disciplinary education of computer science (CS)---and its implications for AI---has led to the current ""ethics crisis"". However, we claim that the current AI ethics education space relies on a form of ""exclusionary pedagogy,"" where ethics is distilled for computational approaches, but there is no deeper epistemological engagement with other ways of knowing that would benefit ethical thinking or an acknowledgement of the limitations of uni-vocal computational thinking. This results in indifference, devaluation, and a lack of mutual support between CS and humanistic social science (HSS), elevating the myth of technologists as ""ethical unicorns"" that can do it all, though their disciplinary tools are ultimately limited. Through an analysis of computer science education literature and a review of college-level course syllabi in AI ethics, we discuss the limitations of the epistemological assumptions and hierarchies of knowledge which dictate current attempts at including ethics education in CS training and explore evidence for the practical mechanisms through which this exclusion occurs. We then propose a shift towards a substantively collaborative, holistic, and ethically generative pedagogy in AI education.","['Inioluwa Deborah Raji', 'Morgan Klaus Scheuerman', 'Razvan Amironesei']","['Mozilla Foundation', 'Information Science, University of Colorado Boulder', 'CADE, University of San Francisco']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE