title,abstract,authors,institutions,conference,date,industry
"""I Can’t Reply with That"": Characterizing Problematic Email Reply Suggestions","In email interfaces, providing users with reply suggestions may simplify or accelerate correspondence.  While the “success'” of such systems is typically quantified using the number of suggestions selected by users, this ignores the impact of social context, which can change how suggestions are perceived.  To address this, we developed a mixed-methods framework involving qualitative interviews and crowdsourced experiments to characterize problematic email reply suggestions.  Our interviews revealed issues with over-positive, dissonant, cultural, and gender-assuming replies, as well as contextual politeness.  In our experiments, crowdworkers assessed email scenarios that we generated and systematically controlled, showing that contextual factors like social ties and the presence of salutations impacts users’ perceptions of email correspondence.  These assessments created a novel dataset of human-authored corrections for problematic email replies. Our study highlights the social complexity of providing suggestions for email correspondence, raising issues that may apply to all social messaging systems.","['Ronald Robertson', 'Alexandra Olteanu', 'Fernando Diaz', 'Milad Shokouhi', 'Peter Bailey']",Microsoft,CHI Conference on Human Factors in Computing Systems (CHI ’21),2021-05-01,TRUE
"""The human body is a black box"": supporting clinical decision-making with deep learning","Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.","['Mark Sendak', 'Madeleine Clare Elish', 'Michael Gao', 'Joseph Futoma', 'William Ratliff', 'Marshall Nichols', 'Armando Bedoya', 'Suresh Balu', ""Cara O'Brien""]","['Duke Institute for Health Innovation', 'Data & Society Research Institute', 'Duke Institute for Health Innovation', 'Harvard University and Duke University', 'Duke Institute for Health Innovation', 'Duke Institute for Health Innovation', 'Duke School of Medicine', 'Duke Institute for Health Innovation', 'Duke School of Medicine']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
“I Don't Want Someone to Watch Me While I'm Working”: Gendered Views of Facial Recognition Technology in Workplace Surveillance,"Employers are increasingly using information and communication technologies to monitor employees. Such workplace surveillance is extensive in the United States, but its experience and potential consequences differ across groups based on gender. We thus sought to identify whether self‐reported male and female employees differ in the extent to which they find the use of workplace cameras equipped with facial recognition technology (FRT) acceptable, and examine the role of privacy attitudes more generally in mediating views on workplace surveillance. Using data from a nationally representative survey conducted by the Pew Research Center, we find that women are much less likely than men to approve of the use of cameras using FRT in the workplace. We then further explore whether men and women think differently about privacy, and if perceptions of privacy moderate the relationship between gender and approval of workplace surveillance. Finally, we consider the implications of these findings for privacy and surveillance via embedded technologies, and how the consequences of surveillance and technologies like FRT may be gendered. Note: We recognize evaluations based on a binary definition of gender are invariably partial and exclusionary. As we note in our discussion of the study’s limitations, we were constrained by the survey categories provided by Pew.","['Luke Stark', 'Amanda Stanhaus', 'Denise L. Anthony']",Microsoft,Journal of the Association for Information Science and Technology,2020-03-10,TRUE
50 Years of Test (Un)fairness: Lessons for Machine Learning,"Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.","['Ben Hutchinson', 'Margaret Mitchell']","['Google', 'Google']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
50 Years of Test (Un)fairness: Lessons for Machine Learning,"Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.",['Ben Hutchinson'],Google,Proceedings of FAT* 2019.,2019,TRUE
A Bayesian Model of Cash Bail Decisions,"The use of cash bail as a mechanism for detaining defendants pretrial is an often-criticized system that many have argued violates the presumption of ""innocent until proven guilty."" Many studies have sought to understand both the long-term effects of cash bail's use and the disparate rate of cash bail assignments along demographic lines (race, gender, etc). However, such work is often susceptible to problems of infra-marginality - that the data we observe can only describe average outcomes, and not the outcomes associated with the marginal decision. In this work, we address this problem by creating a hierarchical Bayesian model of cash bail assignments. Specifically, our approach models cash bail decisions as a probabilistic process whereby judges balance the relative costs of assigning cash bail with the cost of defendants potentially skipping court dates, and where these skip probabilities are estimated based upon features of the individual case. We then use Monte Carlo inference to sample the distribution over these costs for different magistrates and across different races. We fit this model to a data set we have collected of over 50,000 court cases in the Allegheny and Philadelphia counties in Pennsylvania. Our analysis of 50 separate judges shows that they are uniformly more likely to assign cash bail to black defendants than to white defendants, even given identical likelihood of skipping a court appearance. This analysis raises further questions about the equity of the practice of cash bail, irrespective of its underlying legal justification.","['Joshua Williams', 'J. Zico Kolter']","['Computer Science Department, Carnegie Mellon University', 'Computer Science Department, Carnegie Mellon University Bosch Center for AI']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
A comparative study of fairness-enhancing interventions in machine learning,"Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption. We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought.","['Sorelle A. Friedler', 'Carlos Scheidegger', 'Suresh Venkatasubramanian', 'Sonam Choudhary', 'Evan P. Hamilton', 'Derek Roth']","['Haverford College', 'University of Arizona', 'University of Utah', 'University of Utah', 'Haverford College', 'Haverford College']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
A General Approach to Adding Differential Privacy to Iterative Training Procedures,"In this work we address the practical challenges of training machine learning models on privacy-sensitive datasets by introducing a modular approach that minimizes changes to training algorithms, provides a variety of configuration strategies for the privacy mechanism, and then isolates and simplifies the critical logic that computes the final privacy guarantees. A key challenge is that training algorithms often require estimating many different quantities (vectors) from the same set of examples --- for example, gradients of different layers in a deep learning architecture, as well as metrics and batch normalization parameters. Each of these may have different properties like dimensionality, magnitude, and tolerance to noise. By extending previous work on the Moments Accountant for the subsampled Gaussian mechanism, we can provide privacy for such heterogeneous sets of vectors, while also structuring the approach to minimize software engineering challenges.","['Brendan McMahan', 'Nicolas Papernot', 'Peter Kairouz']",Google,NIPS (2018),2018,TRUE
A Human in the Loop is Not Enough: The Need for Human-Subject Experiments in Facial Recognition,"The deployment of facial recognition systems in high-stakes scenarios has sparked widespread concerns about privacy, fairness, and accountability. A common response to these concerns is the suggestion of adding a human in the loop to provide oversight and ensure fairness and accountability. However, the effectiveness of this approach is seldom studied empirically, and humans are known to have biases of their own. In this position paper, we argue for the necessity of empirical studies of human-in-the-loop facial recognition systems. We outline several technical and ethical challenges that arise when conducting such empirical studies and when interpreting their results. Our goal is to initiate a discussion about ways for AI and HCI researchers to work together on human-centered approaches to empirically studying human-in-the-loop facial recognition systems.","['Forough Poursabzi-Sangdeh', 'Samira Samadi', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,UNKNOWN,2020-04-01,TRUE
A Human-Centered Agenda for Intelligible Machine Learning,"To build machine learning systems that are reliable, trustworthy, and fair, we must be able to provide relevant stakeholders with an understanding of how these systems work. Yet what makes a system “intelligible” is difficult to pin down. Intelligibility is a fundamentally human-centered concept that lacks a one-size-fits-all solution. Although many intelligibility techniques have been proposed in the machine learning literature, there are many more open questions about how best to provide stakeholders with the information they need to achieve their desired goals. In this chapter, we begin with an overview of the intelligible machine learning landscape and give several examples of the diverse ways in which needs for intelligibility can arise. We provide an overview of the techniques for achieving intelligibility that have been proposed in the machine learning literature. We discuss the importance of taking a human-centered strategy when designing intelligibility techniques or when verifying that these techniques achieve their intended goals. We also argue that the notion of intelligibility should be expanded beyond machine learning models to other components of machine learning systems, such as datasets and performance metrics. Finally, we emphasize the necessity of tight integration between the machine learning and human–computer interaction communities.","['Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,UNKNOWN,2021-04-27,TRUE
A Moral Framework for Understanding Fair ML through Economic Models of Equality of Opportunity,"We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity (EOP)---an extensively studied ideal of fairness in political philosophy. We formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new family of measures for algorithmic fairness. We illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.","['Hoda Heidari', 'Michele Loi', 'Krishna P. Gummadi', 'Andreas Krause']","['ETH Zürich', 'University of Zürich', 'MPI-SWS', 'ETH Zürich']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
A Pilot Study in Surveying Clinical Judgments to Evaluate Radiology Report Generation,"The recent release of many Chest X-Ray datasets has prompted a lot of interest in radiology report generation. To date, this has been framed as an image captioning task, where the machine takes an RGB image as input and generates a 2-3 sentence summary of findings as output. The quality of these reports has been canonically measured using metrics from the NLP community for language generation such as Machine Translation and Summarization. However, the evaluation metrics (e.g. BLEU, CIDEr) are inappropriate for the medical domain, where clinical correctness is critical. To address this, our team brought together machine learning experts with radiologists for a pilot study in co-designing a better metric for evaluating the quality of an algorithmically-generated radiology report. The interdisciplinary collaborative process involved multiple interviews, outreach, and preliminary annotation to design a larger scale study - which is now underway - to build a more meaningful evaluation tool.","['William Boag', 'Hassan Kané', 'Saumya Rawat', 'Jesse Wei', 'Alexander Goehler']","['MIT, USA', 'WL Research, USA', 'MIT, USA', 'Beth Israel Deaconess Medical Center, Department of Radiology, USA', 'Beth Israel Deaconess Medical Center, Department of Radiology, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
A practical algorithm for balancing the max-min fairness and throughput objectives in traffic engineering,"One of the goals of traffic engineering is to achieve a
flexible trade-off between fairness and throughput so that users
are satisfied with their bandwidth allocation and the network
operator is satisfied with the utilization of network resources. In
this paper, we propose a novel way to balance the throughput
and fairness objectives with linear programming. It allows the
network operator to precisely control the trade-off by bounding
the fairness degradation for each commodity compared to the
max-min fair solution or the throughput degradation compared
to the optimal throughput. We also present improvements to a
previous algorithm that achieves max-min fairness by solving a
series of linear programs. We significantly reduce the number
of steps needed when the access rate of commodities is limited.
We extend the algorithm to two important practical use cases:
importance weights and piece-wise linear utility functions for
commodities. Our experiments on synthetic and real networks
show that our algorithms achieve a significant speedup and
provide practical insights on the trade-off between fairness and
throughput.",['Subhasree Mandal'],Google,INFOCOM (2012),2012,TRUE
A Qualitative Exploration of Perceptions of Algorithmic Fairness,"Algorithmic systems increasingly shape information people are exposed to as well as influence decisions about employment, finances, and other opportunities. In some cases, algorithmic systems may be more or less favorable to certain groups or individuals, sparking substantial discussion of algorithmic fairness in public policy circles, academia, and the press. We broaden this discussion by exploring how members of potentially affected communities feel about algorithmic fairness. We conducted workshops and interviews with 44 participants from several populations traditionally marginalized by categories of race or class in the United States. While the concept of algorithmic fairness was largely unfamiliar, learning about algorithmic (un)fairness elicited negative feelings that connect to current national discussion about racial injustice and economic inequality. In addition to their concerns about potential harms to themselves and society, participants also indicated that algorithmic fairness (or lack thereof) could substantially affect their trust in a company or product.",[],Google,CHI 2018 (2018),2018,TRUE
A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development,"One of the important challenges regarding the development of morally responsible and ethically qualified digital technologies is how to support designers and developers in producing those technologies, especially when conceptualizing their vision of what the technology will be, how it will benefit users, and avoid doing harm. However, traditional software design and development life cycles do not explicitly support the reflection upon either ethical or moral issues. In this paper we look at how a number of ethical issues may be dealt with during digital technology design and development, to prevent damage and improve technological fairness, accountability, and transparency. Starting from mature work on semiotic theory and methods in human-computer interaction, we propose to extend the core artifact used in semiotic engineering of human-centered technology design, so as to directly address moral responsibility and ethical issues. The resulting extension is an epistemic tool, that is, an instrument to create and elaborate on this specific kind of knowledge. The paper describes the tool, illustrates how it is to be used, and discusses its promises and limitations against the background of related work. It also includes proposed empirical studies, accompanied by briefly described methodological challenges and considerations that deserve our attention.","['Simone Diniz Junqueira Barbosa', 'Gabriel Diniz Junqueira Barbosa', 'Clarisse Sieckenius de Souza', 'Carla Faria Leitão']","['Department of Informatics, PUC-Rio Rio de Janeiro, RJ', 'Department of Informatics, PUC-Rio Rio de Janeiro, RJ', 'Department of Informatics, PUC-Rio Rio de Janeiro, RJ', 'Department of Psychology, PUC-Rio Rio de Janeiro, RJ']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
A Statistical Test for Probabilistic Fairness,"Algorithms are now routinely used to make consequential decisions that affect human lives. Examples include college admissions, medical interventions or law enforcement. While algorithms empower us to harness all information hidden in vast amounts of data, they may inadvertently amplify existing biases in the available datasets. This concern has sparked increasing interest in fair machine learning, which aims to quantify and mitigate algorithmic discrimination. Indeed, machine learning models should undergo intensive tests to detect algorithmic biases before being deployed at scale. In this paper, we use ideas from the theory of optimal transport to propose a statistical hypothesis test for detecting unfair classifiers. Leveraging the geometry of the feature space, the test statistic quantifies the distance of the empirical distribution supported on the test samples to the manifold of distributions that render a pre-trained classifier fair. We develop a rigorous hypothesis testing mechanism for assessing the probabilistic fairness of any pre-trained logistic classifier, and we show both theoretically as well as empirically that the proposed test is asymptotically correct. In addition, the proposed framework offers interpretability by identifying the most favorable perturbation of the data so that the given classifier becomes fair.","['Bahar Taskesen', 'Jose Blanchet', 'Daniel Kuhn', 'Viet Anh Nguyen']","['Ecole Polytechnique Fédérale de Lausanne, Switzerland', 'Stanford University, USA', 'Ecole Polytechnique Fédérale de Lausanne, Switzerland', 'Stanford University, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
A Taxonomy of Ethical Tensions in Inferring Mental Health States from Social Media,"Powered by machine learning techniques, social media provides an unobtrusive lens into individual behaviors, emotions, and psychological states. Recent research has successfully employed social media data to predict mental health states of individuals, ranging from the presence and severity of mental disorders like depression to the risk of suicide. These algorithmic inferences hold great potential in supporting early detection and treatment of mental disorders and in the design of interventions. At the same time, the outcomes of this research can pose great risks to individuals, such as issues of incorrect, opaque algorithmic predictions, involvement of bad or unaccountable actors, and potential biases from intentional or inadvertent misuse of insights. Amplifying these tensions, there are also divergent and sometimes inconsistent methodological gaps and under-explored ethics and privacy dimensions. This paper presents a taxonomy of these concerns and ethical challenges, drawing from existing literature, and poses questions to be resolved as this research gains traction. We identify three areas of tension: ethics committees and the gap of social media research; questions of validity, data, and machine learning; and implications of this research for key stakeholders. We conclude with calls to action to begin resolving these interdisciplinary dilemmas.","['Stevie Chancellor', 'Michael L. Birnbaum', 'Eric D. Caine', 'Vincent M. B. Silenzio', 'Munmun De Choudhury']","['Georgia Tech, Atlanta, GA, US', 'Northwell Health, Glen Oaks, NY, US', 'University of Rochester, Rochester, NY, US', 'University of Rochester, Rochester, NY, US', 'Georgia Tech, Atlanta, GA, US']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Access to Population-Level Signaling as a Source of Inequality,"We identify and explore differential access to population-level signaling (also known as information design) as a source of unequal access to opportunity. A population-level signaler has potentially noisy observations of a binary type for each member of a population and, based on this, produces a signal about each member. A decision-maker infers types from signals and accepts those individuals whose type is high in expectation. We assume the signaler of the disadvantaged population reveals her observations to the decision-maker, whereas the signaler of the advantaged population forms signals strategically. We study the expected utility of the populations as measured by the fraction of accepted members, as well as the false positive rates (FPR) and false negative rates (FNR). We first show the intuitive results that for a fixed environment, the advantaged population has higher expected utility, higher FPR, and lower FNR, than the disadvantaged one (despite having identical population quality), and that more accurate observations improve the expected utility of the advantaged population while harming that of the disadvantaged one. We next explore the introduction of a publicly-observable signal, such as a test score, as a potential intervention. Our main finding is that this natural intervention, intended to reduce the inequality between the populations' utilities, may actually exacerbate it in settings where observations and test scores are noisy.","['Nicole Immorlica', 'Katrina Ligett', 'Juba Ziani']","['Microsoft Research', 'Hebrew University of Jerusalem', 'California Institute of Technology']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Actionable Recourse in Linear Classification,"Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood. In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.","['Berk Ustun', 'Alexander Spangher', 'Yang Liu']","['Harvard University, Cambridge, MA', 'Carnegie Mellon University, Pittsburgh, PA', 'UC Santa Cruz CSE, Santa Cruz, CA']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Advances and Open Problems in Federated Learning,"Federated learning (FL) is a machine learning setting where many clients (e.g., mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g., service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and mitigates many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents a comprehensive list of open problems and challenges.","['Peter Kairouz', 'H. Brendan McMahan', 'K. A. Bonawitz', 'Zachary Charles', 'Zachary Garrett', 'Badih Ghazi', 'Ben Hutchinson', 'Jakub Konečný', 'Mehryar Mohri', 'Hang Qi', 'Daniel Ramage', 'Ananda Theertha Suresh', 'Zheng Xu', 'Sen Zhao']",Google,Arxiv (2019),2019,TRUE
Adversarial Examples as an Input-Fault Tolerance Problem,"We analyze the adversarial examples problem in terms of a modelâs fault tolerance
with respect to its input. Whereas previous work focuses on arbitrarily strict threat
models, i.e., -perturbations, we consider arbitrary valid inputs and propose an
information-based characteristic for evaluating tolerance to diverse input faults.",[],Google,NeurIPS Workshop on Security in Machine Learning (2018),2018,TRUE
Adversarial Spheres,"State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size O(1/sqrt(d)). Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.","['Luke Metz', 'Maithra Raghu', 'Martin Wattenberg']",Google,ICLR Workshop (2018),2018,TRUE
Adversarially Robust Generalization Requires More Data,"Machine learning models are often susceptible to adversarial perturbations of their inputs.
Even small perturbations can cause state-of-the-art classifiers with high âstandardâ accuracy to
produce an incorrect prediction with high confidence. To better understand this phenomenon, we
study adversarially robust learning from the viewpoint of generalization. We show that already
in a simple natural data model, the sample complexity of robust learning can be significantly
larger than that of âstandardâ learning. This gap is information theoretic and holds irrespective
of the training algorithm or the model family. We complement our theoretical results with
experiments on popular image classification datasets and show that a similar gap exists here as
well. We postulate that the difficulty of training robust classifiers stems, at least partially, from
this inherently larger sample complexity.",[],Google,NeurIPS (Spotlight) (2018),2018,TRUE
Algorithmic accountability in public administration: the GDPR paradox,"The EU General Data Protection Regulation (""GDPR"") is often represented as a larger than life behemoth that will fundamentally transform the world of big data. Abstracted from its constituent parts of corresponding rights, responsibilities, and exemptions, the operative scope of the GDPR can be unduly aggrandized, when in reality, it caters to the specific policy objectives of legislators and institutional stakeholders. With much uncertainty ahead on the precise implementation of the GDPR, academic and policy discussions are debating the adequacy of protections for automated decision-making in GDPR Articles 13 (right to be informed of automated treatment), 15 (right of access by the data subject), and 22 (safeguards to profiling). Unfortunately, the literature to date disproportionately focuses on the impact of AI in the private sector, and deflects any extensive review of automated enforcement tools in public administration. Even though the GDPR enacts significant safeguards against automated decisions, it does so with deliberate design: to balance the interests of data protection with the growing demand for algorithms in the administrative state. In order to facilitate inter-agency data flows and sensitive data processing that fuel the predictive power of algorithmic enforcement tools, the GDPR decisively surrenders to the procedural autonomy of Member States to authorize these practices. Yet, due to a dearth of research on the GDPR's stance on government deployed algorithms, it is not widely known that public authorities can benefit from broadly worded exemptions to restrictions on automated decision-making, and even circumvent remedies for data subjects through national legislation. The potential for public authorities to invoke derogations from the GDPR must be contained by the fundamental guarantees of due process, judicial review, and equal treatment. This paper examines the interplay of these principles within the prospect of algorithmic decision-making by public authorities.",['Sunny Seon Kang'],['Inpher'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Algorithmic Fairness in Predicting Opioid Use Disorder using Machine Learning,"There has been recent interest by payers, health care systems, and researchers in the development of machine learning and artificial intelligence models that predict an individual's probability of developing opioid use disorder. The scores generated by these algorithms can be used by physicians to tailor the prescribing of opioids for the treatment of pain, reducing or foregoing prescribing to individuals deemed to be at high risk, or increasing prescribing for patients deemed to be at low risk. This paper constructs a machine learning algorithm to predict opioid use disorder risk using commercially available claims data similar to those utilized in the development of proprietary opioid use disorder prediction algorithms. We study risk scores generated by the machine learning model in a setting with quasi-experimental variation in the likelihood that doctors prescribe opioids, generated by changes in the legal structure for monitoring physician prescribing. We find that machine-predicted risk scores do not appear to correlate at all with the individual-specific heterogeneous treatment effect of receiving opioids. The paper identifies a new source of algorithmic unfairness in machine learning applications for health care and precision medicine, arising from the researcher's choice of objective function. While precision medicine should guide physician treatment decisions based on the heterogeneous causal impact of a course of treatment for an individual, allocating treatments to individuals receiving the most benefit and recommending caution for those most likely to experience harmful side effects, ML models in health care are often trained on proxies like individual baseline risk, and are not necessarily informative in deciding who would most benefit, or be harmed, by a course of treatment.",['Angela E. Kilby'],"['Northeastern University Boston, Massachusetts, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Algorithmic Impact Assessments and Accountability: The Co-construction of Impacts,"Algorithmic impact assessments (AIAs) are an emergent form of accountability for organizations that build and deploy automated decision-support systems. They are modeled after impact assessments in other domains. Our study of the history of impact assessments shows that ""impacts"" are an evaluative construct that enable actors to identify and ameliorate harms experienced because of a policy decision or system. Every domain has different expectations and norms around what constitutes impacts and harms, how potential harms are rendered as impacts of a particular undertaking, who is responsible for conducting such assessments, and who has the authority to act on them to demand changes to that undertaking. By examining proposals for AIAs in relation to other domains, we find that there is a distinct risk of constructing algorithmic impacts as organizationally understandable metrics that are nonetheless inappropriately distant from the harms experienced by people, and which fall short of building the relationships required for effective accountability. As impact assessments become a commonplace process for evaluating harms, the FAccT community, in its efforts to address this challenge, should A) understand impacts as objects that are co-constructed accountability relationships, B) attempt to construct impacts as close as possible to actual harms, and C) recognize that accountability governance requires the input of various types of expertise and affected communities. We conclude with lessons for assembling cross-expertise consensus for the co-construction of impacts and building robust accountability relationships.","['Jacob Metcalf', 'Emanuel Moss', 'Elizabeth Anne Watkins', 'Ranjit Singh', 'Madeleine Clare Elish']","['Data & Society Research Institute New York, NY', 'Data & Society Research Institute New York, NY CUNY Graduate Center New York, NY', 'Princeton Center for Information Technology Policy Princeton, NJ Data & Society Research Institute New York, NY', 'Data & Society Research Institute New York, NY', 'Data & Society Research Institute New York, NY']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Algorithmic realism: expanding the boundaries of algorithmic thought,"Although computer scientists are eager to help address social problems, the field faces a growing awareness that many well-intentioned applications of algorithms in social contexts have led to significant harm. We argue that addressing this gap between the field's desire to do good and the harmful impacts of many of its interventions requires looking to the epistemic and methodological underpinnings of algorithms. We diagnose the dominant mode of algorithmic reasoning as ""algorithmic formalism"" and describe how formalist orientations lead to harmful algorithmic interventions. Addressing these harms requires pursuing a new mode of algorithmic thinking that is attentive to the internal limits of algorithms and to the social concerns that fall beyond the bounds of algorithmic formalism. To understand what a methodological evolution beyond formalism looks like and what it may achieve, we turn to the twentieth century evolution in American legal thought from legal formalism to legal realism. Drawing on the lessons of legal realism, we propose a new mode of algorithmic thinking---""algorithmic realism""---that provides tools for computer scientists to account for the realities of social life and of algorithmic impacts. These realist approaches, although not foolproof, will better equip computer scientists to reduce algorithmic harms and to reason well about doing good.","['Ben Green', 'Salomé Viljoen']","['Harvard University', 'Cornell University & New York University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Algorithmic Recourse: from Counterfactual Explanations to Interventions,"As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -""how the world would have (had) to be different for a desirable outcome to occur""- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.","['Amir-Hossein Karimi', 'Bernhard Schölkopf', 'Isabel Valera']","['MPI-IS, Germany, ETH Zürich, Switzerland', 'MPI-IS, Germany', 'MPI-IS, Germany, Saarland University, Germany']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
"Algorithmic targeting of social policies: fairness, accuracy, and distributed governance","Targeted social policies are the main strategy for poverty alleviation across the developing world. These include targeted cash transfers (CTs), as well as targeted subsidies in health, education, housing, energy, childcare, and others. Due to the scale, diversity, and widespread relevance of targeted social policies like CTs, the algorithmic rules that decide who is eligible to benefit from them---and who is not---are among the most important algorithms operating in the world today. Here we report on a year-long engagement towards improving social targeting systems in a couple of developing countries. We demonstrate that a shift towards the use of AI methods in poverty-based targeting can substantially increase accuracy, extending the coverage of the poor by nearly a million people in two countries, without increasing expenditure. However, we also show that, absent explicit parity constraints, both status quo and AI-based systems induce disparities across population subgroups. Moreover, based on qualitative interviews with local social institutions, we find a lack of consensus on normative standards for prioritization and fairness criteria. Hence, we close by proposing a decision-support platform for distributed governance, which enables a diversity of institutions to customize the use of AI-based insights into their targeting decisions.","['Alejandro Noriega-Campero', 'Bernardo Garcia-Bulle', 'Luis Fernando Cantu', 'Michiel A. Bakker', 'Luis Tejerina', 'Alex Pentland']","['Prosperia Labs', 'MIT', 'ITAM, Mexico City, Mexico', 'MIT', 'IADB', 'MIT']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Allocating Opportunities in a Dynamic Model of Intergenerational Mobility,"Opportunities such as higher education can promote intergenerational mobility, leading individuals to achieve levels of socioeconomic status above that of their parents. We develop a dynamic model for allocating such opportunities in a society that exhibits bottlenecks in mobility; the problem of optimal allocation reflects a trade-off between the benefits conferred by the opportunities in the current generation and the potential to elevate the socioeconomic status of recipients, shaping the composition of future generations in ways that can benefit further from the opportunities. We show how optimal allocations in our model arise as solutions to continuous optimization problems over multiple generations, and we find in general that these optimal solutions can favor recipients of low socioeconomic status over slightly higher-performing individuals of high socioeconomic status --- a form of socioeconomic affirmative action that the society in our model discovers in the pursuit of purely payoff-maximizing goals. We characterize how the structure of the model can lead to either temporary or persistent affirmative action, and we consider extensions of the model with more complex processes modulating the movement between different levels of socioeconomic status.","['Hoda Heidari', 'Jon Kleinberg']","['Carnegie Mellon University', 'Cornell University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity,"Sensitive statistics are often collected across sets of users, with repeated collection of reports done over time. For example, trends in users' private preferences or software usage may be monitored via such reports. We study the collection of such statistics in the local differential privacy (LDP) model, and describe an algorithm whose privacy cost is polylogarithmic in the number of changes to a user's value. 
",[],Google,ACM-SIAM Symposium on Discrete Algorithms (SODA) (2019),2019,TRUE
An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists,"Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.","['P. M. Krafft', 'Meg Young', 'Michael Katell', 'Jennifer E. Lee', 'Shankar Narayan', 'Micah Epstein', 'Dharma Dailey', 'Bernease Herman', 'Aaron Tam', 'Vivian Guetler', 'Corinne Bintz', 'Daniella Raz', 'Pa Ousman Jobe', 'Franziska Putz', 'Brian Robick', 'Bissan Barghouti']","['Creative Computing Institute, University of Arts London', 'Digital Life Initiative, Cornell Tech', 'Public Policy Programme, Alan Turing Institute', 'ACLU of Washington', 'MIRA', 'Coveillance Collective', 'Human Centered Design & Engineering, University of Washington', 'eScience Institute, University of Washington', 'Evans School of Public Policy & Governance, University of Washington', 'Department of Sociology, West Virginia University', 'Department of Computer Science, Middlebury College', 'School of Information, University of Michigan', 'Albers School of Business & Economics, Seattle University', 'Oxford Department of International Development, University of Oxford', 'ACLU of Washington', 'ACLU of Washington']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
An Agent-based Model to Evaluate Interventions on Online Dating Platforms to Decrease Racial Homogamy,"Perhaps the most controversial questions in the study of online platforms today surround the extent to which platforms can intervene to reduce the societal ills perpetrated on them. Up for debate is whether there exist any effective and lasting interventions a platform can adopt to address, e.g., online bullying, or if other, more far-reaching change is necessary to address such problems. Empirical work is critical to addressing such questions. But it is also challenging, because it is time-consuming, expensive, and sometimes limited to the questions companies are willing to ask. To help focus and inform this empirical work, we here propose an agent-based modeling (ABM) approach. As an application, we analyze the impact of a set of interventions on a simulated online dating platform on the lack of long-term interracial relationships in an artificial society. In the real world, a lack of interracial relationships are a critical vehicle through which inequality is maintained. Our work shows that many previously hypothesized interventions online dating platforms could take to increase the number of interracial relationships from their website have limited effects, and that the effectiveness of any intervention is subject to assumptions about sociocultural structure. Further, interventions that are effective in increasing diversity in long-term relationships are at odds with platforms' profit-oriented goals. At a general level, the present work shows the value of using an ABM approach to help understand the potential effects and side effects of different interventions that a platform could take.","['Stefania Ionescu', 'Anikó Hannák', 'Kenneth Joseph']","['University of Zürich, Zürich, Switzerland', 'University of Zürich, Zürich, Switzerland', 'University at Buffalo, Buffalo, NY, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
An Empirical Study of Rich Subgroup Fairness for Machine Learning,"Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.","['Michael Kearns', 'Seth Neel', 'Aaron Roth', 'Zhiwei Steven Wu']","['Department of Computer and Information Sciences, University of Pennsylvania', 'Department of Statistics, University of Pennsylvania', 'Department of Computer and Information Sciences, University of Pennsylvania', 'Department of Computer Science and Engineering, University of Minnesota']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
"An empirical study on the perceived fairness of realistic, imperfect machine learning models","There are many competing definitions of what statistical properties make a machine learning model fair. Unfortunately, research has shown that some key properties are mutually exclusive. Realistic models are thus necessarily imperfect, choosing one side of a trade-off or the other. To gauge perceptions of the fairness of such realistic, imperfect models, we conducted a between-subjects experiment with 502 Mechanical Turk workers. Each participant compared two models for deciding whether to grant bail to criminal defendants. The first model equalized one potentially desirable model property, with the other property varying across racial groups. The second model did the opposite. We tested pairwise trade-offs between the following four properties: accuracy; false positive rate; outcomes; and the consideration of race. We also varied which racial group the model disadvantaged. We observed a preference among participants for equalizing the false positive rate between groups over equalizing accuracy. Nonetheless, no preferences were overwhelming, and both sides of each trade-off we tested were strongly preferred by a non-trivial fraction of participants. We observed nuanced distinctions between participants considering a model ""unbiased"" and considering it ""fair."" Furthermore, even when a model within a trade-off pair was seen as fair and unbiased by a majority of participants, we did not observe consensus that a machine learning model was preferable to a human judge. Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations.","['Galen Harrison', 'Julia Hanson', 'Christine Jacinto', 'Julio Ramirez', 'Blase Ur']","['University of Chicago', 'University of Chicago', 'University of Chicago', 'University of Chicago', 'University of Chicago']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Analyzing Biases in Perception of Truth in News Stories and Their Implications for Fact Checking,"Recently, social media sites like Facebook and Twitter have been severely criticized by policy makers, and media watchdog groups for allowing fake news stories to spread unchecked on their platforms. In response, these sites are encouraging their users to report any news story they encounter on the site, which they perceive as fake. Stories that are reported as fake by a large number of users are prioritized for fact checking by (human) experts at fact checking organizations like Snopes and PolitiFact. Thus, social media sites today are relying on their users' perceptions of the truthfulness of news stories to select stories to fact check. However, few studies have focused on understanding how users perceive truth in news stories, or how biases in their perceptions might affect current strategies to detect and label fake news stories. To this end, we present an in-depth analysis on users' perceptions of truth in news stories. Specifically, we analyze users' truth perception biases for 150 stories fact checked by Snopes. Based on their ground truth and the truth value perceived by users, we can classify the stories into four categories -- (i) C1: false stories perceived as false by most users, (ii) C2: true stories perceived as false by most users, (iii) C3: false stories perceived as true by most users, and (iv) C4: true stories perceived as true by most users. The stories that are likely to be reported (flagged) for fact checking are from the two classes C1 and C2 that have the lowest perceived truth levels. We argue that there is little to be gained by fact checking stories from C1 whose truth value is correctly perceived by most users. Although stories in C2 reveal the cynicality of users about true stories, social media sites presently do not explicitly mark them as true to resolve the confusion. On the contrary, stories in C3 are false stories, yet perceived as true by most users. Arguably, these stories are more damaging than C1 because the truth values of the the story in former situation is incorrectly perceived while truth values of the latter is correctly perceived. Nevertheless, the stories in C1 is likely to be fact checked with greater priority than the stories in C3! In fact, in today's social media sites, the higher the gullibility of users towards believing a false story, the less likely it is to be reported for fact checking. In summary, we make the following contributions in this work. 1. Methodological: We develop a novel method for assessing users' truth perceptions of news stories. We design a test for users to rapidly assess (i.e., at the rate of a few seconds per story) how truthful or untruthful the claims in a news story are. We then conduct our truth perception tests on-line and gather truth perceptions of 100 US-based Amazon Mechanical Turk workers for each story. 2. Empirical: Our exploratory analysis of users' truth perceptions reveal several interesting insights. For instance, (i) for many stories, the collective wisdom of the crowd (average truth rating) differs significantly from the actual truth of the story, i.e., wisdom of crowds is inaccurate, (ii) across different stories, we find evidence for both false positive perception bias (i.e., a gullible user perceiving the story to be more true than it is in reality) and false negative perception bias (i.e., a cynical user perceiving a story to be more false than it is in reality), and (iii) users' political ideologies influence their truth perceptions for the most controversial stories, it is frequently the result of users' political ideologies influencing their truth perceptions. 3. Practical: Based on our observations, we call for prioritizing stories to fact check in order to achieve the following three important goals: (i) Remove false news stories from circulation, (ii) Correct the misperception of the users, and (iii) Decrease the disagreement between different users' perceptions of truth. Finally, we provide strategies which utilize users' truth perceptions (and predictive analysis of their biases) to achieve the three goals stated above while prioritizing stories for fact checking. The full paper is available at: https://bit.ly/2T7raFO","['Mahmoudreza Babaei', 'Abhijnan Chakraborty', 'Juhi Kulshrestha', 'Elissa M. Redmiles', 'Meeyoung Cha', 'Krishna P. Gummadi']","['MPI-SWS, Germany', 'MPI-SWS, Germany', 'GESIS, Germany', 'University of Maryland, US', 'KAIST, South Korea', 'MPI-SWS, Germany']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Anatomy of a Privacy-Safe Large-Scale Information Extraction System Over Email,"Extracting structured data from emails can enable several assistive experiences, such as reminding the user when a bill payment is due, answering queries about the departure time of a booked flight, or proactively surfacing an emailed discount coupon while the user is at that store.
This paper presents Juicer, a system for extracting information from email that is serving over a billion Gmail users daily. We describe how the design of the system was informed by three key principles: scaling to a planet-wide email service, isolating the complexity to provide a simple experience for the developer, and safeguarding the privacy of users (our team and the developers we support are not allowed to view any single email). We describe the design tradeoffs made in building this system, the challenges faced and the approaches used to tackle them. We present case studies of three extraction tasks implemented on this platformâbill reminders, commercial offers, and hotel reservationsâto illustrate the effectiveness of the platform despite challenges unique to each task. Finally, we outline several areas of ongoing research in large-scale machine-learned information extraction from email.","['Ying Sheng', 'Sandeep Tata', 'James B. Wendt', 'Jing Xie', 'Qi Zhao', 'Marc Najork']",Google,24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2018),2018,TRUE
Artificial mental phenomena: psychophysics as a framework to detect perception biases in AI models,"Detecting biases in artificial intelligence has become difficult because of the impenetrable nature of deep learning. The central difficulty is in relating unobservable phenomena deep inside models with observable, outside quantities that we can measure from inputs and outputs. For example, can we detect gendered perceptions of occupations (e.g., female librarian, male electrician) using questions to and answers from a word embedding-based system? Current techniques for detecting biases are often customized for a task, dataset, or method, affecting their generalization. In this work, we draw from Psychophysics in Experimental Psychology---meant to relate quantities from the real world (i.e., ""Physics"") into subjective measures in the mind (i.e., ""Psyche"")---to propose an intellectually coherent and generalizable framework to detect biases in AI. Specifically, we adapt the two-alternative forced choice task (2AFC) to estimate potential biases and the strength of those biases in black-box models. We successfully reproduce previously-known biased perceptions in word embeddings and sentiment analysis predictions. We discuss how concepts in experimental psychology can be naturally applied to understanding artificial mental phenomena, and how psychophysics can form a useful methodological foundation to study fairness in AI.","['Lizhen Liang', 'Daniel E. Acuna']","['Syracuse University', 'Syracuse University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Assessing algorithmic fairness with unobserved protected class using data combination,"The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts, or performance disparities in general, in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, that includes protected class labels but not decisions or outcomes. We show that a variety of common disparity measures are generally unidentifiable aside for some unrealistic cases, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the sharpest-possible partial identification set of disparities either under no assumptions or when we incorporate mild smoothness constraints. We further provide optimization-based algorithms for computing and visualizing these sets of simultaneously achievable pairwise disparties for assessing disparities that arise between multiple groups, which enables reliable and robust assessments - an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.","['Nathan Kallus', 'Xiaojie Mao', 'Angela Zhou']","['Cornell University', 'Cornell University', 'Cornell University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Assessing and mitigating unfairness in credit models with the Fairlearn toolkit,"As AI plays an increasing role in the financial services industry, it is essential that financial services organizations anticipate and mitigate unintended consequences, including fairness-related harms, such as denying people services, initiating predatory lending, amplifying gender or racial biases, or violating laws such as the United States’ Equal Credit Opportunity Act (ECOA). To address these kinds of harms, fairness must be explicitly prioritized throughout the AI development and deployment lifecycle.","['Miro Dudík', 'William Chen', 'Solon Barocas', 'Mario Inchiosa', 'Nick Lewins', 'Miruna Oprescu', 'Joy Qiao', 'Mehrnoosh Sameki', 'Mario Schlener', 'Jason Tuo', 'Hanna Wallach']",Microsoft,UNKNOWN,2020-09-22,TRUE
Attribute-based Propensity for Unbiased Learning in Recommender Systems: Algorithm and Case Studies,"Many modern recommender systems train their models based on a large amount of implicit user feedback data. Due to the inherent bias in this data (e.g., position bias), learning from it directly can lead to suboptimal models. Recently, unbiased learning was proposed to address such problems by leveraging counterfactual techniques like inverse propensity weighting (IPW). In these methods, propensity scores estimation is usually limited to item's display position in a single user interface (UI).","['Zhen Qin', 'Don Metzler', 'Xuanhui Wang']",Google,26TH ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) (2020),2020,TRUE
Audio De-identification: A New Entity Recognition Task,"Named Entity Recognition (NER) has been mostly studied in the context of written text. Specifically, NER is an important step in de-identification (de-ID) of medical records, many of which are recorded conversations between a patient and a doctor. In such recordings, audio spans with personal information should be redacted, similar to the redaction of sensitive character spans in de-ID for written text. The application of NER in the context of audio de-identification has yet to be fully investigated. To this end, we define the task of audio de-ID, in which audio spans with entity mentions should be detected. We then present our pipeline for this task, which involves Automatic Speech Recognition (ASR), NER on the transcript text, and text-to-audio alignment. Finally, we introduce a novel metric for audio de-ID and a new evaluation benchmark consisting of a large labeled segment of the Switchboard and Fisher audio datasets and detail our pipeline's results on it.","['Ido Cohn', 'Itay Laish', 'Genady Beryozkin', 'Gang Li', 'Izhak Shafran', 'Idan Szpektor', 'Avinatan Hassidim', 'Yossi Matias']",Google,NAACL (2019),2019,TRUE
Auditing radicalization pathways on YouTube,"Non-profits, as well as the media, have hypothesized the existence of a radicalization pipeline on YouTube, claiming that users systematically progress towards more extreme content on the platform. Yet, there is to date no substantial quantitative evidence of this alleged pipeline. To close this gap, we conduct a large-scale audit of user radicalization on YouTube. We analyze 330,925 videos posted on 349 channels, which we broadly classified into four types: Media, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right. According to the aforementioned radicalization hypothesis, channels in the I.D.W. and the Alt-lite serve as gateways to fringe far-right ideology, here represented by Alt-right channels. Processing 72M+ comments, we show that the three channel types indeed increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube's recommendation algorithm, looking at more than 2M video and channel recommendations between May/July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels, while Alt-right videos are reachable only through channel recommendations. Overall, we paint a comprehensive picture of user radicalization on YouTube.","['Manoel Horta Ribeiro', 'Raphael Ottoni', 'Robert West', 'Virgílio A. F. Almeida', 'Wagner Meira']","['EPFL', 'UFMG', 'EPFL', 'UFMG', 'UFMG']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Avoiding Disparity Amplification under Different Worldviews,"We mathematically compare four competing definitions of group-level nondiscrimination: demographic parity, equalized odds, predictive parity, and calibration. Using the theoretical framework of Friedler et al., we study the properties of each definition under various worldviews, which are assumptions about how, if at all, the observed data is biased. We argue that different worldviews call for different definitions of fairness, and we specify the worldviews that, when combined with the desire to avoid a criterion for discrimination that we call disparity amplification, motivate demographic parity and equalized odds. We also argue that predictive parity and calibration are insufficient for avoiding disparity amplification because predictive parity allows an arbitrarily large inter-group disparity and calibration is not robust to post-processing. Finally, we define a worldview that is more realistic than the previously considered ones, and we introduce a new notion of fairness that corresponds to this worldview.","['Samuel Yeom', 'Michael Carl Tschantz']","['Carnegie Mellon University', 'International Computer Science Institute']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Awareness in practice: tensions in access to sensitive attribute data for antidiscrimination,"Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted. This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities. This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.","['Miranda Bogen', 'Aaron Rieke', 'Shazeda Ahmed']","['Upturn', 'Upturn', 'University of California']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Better Together?: How Externalities of Size Complicate Notions of Solidarity and Actuarial Fairness,"Consider a cost-sharing game with players of different costs: an example might be an insurance company calculating premiums for a population of mixed-risk individuals. Two natural and competing notions of fairness might be to a) charge each individual the same or b) charge each individual according to the cost that they bring to the pool. In the insurance literature, these approaches are referred to as ""solidarity"" and ""actuarial fairness"" and are commonly viewed as opposites. However, in insurance (and many other natural settings), the cost-sharing game also exhibits externalities of size: all else being equal, larger groups have lower average cost. In the insurance case, we analyze model where costs strictly decreases with pooling due to a reduction in the variability of losses. In this paper, we explore how this complicates traditional understandings of fairness, drawing on literature in cooperative game theory. First, we explore solidarity: we show that it is possible for both groups (high risk and low risk) to strictly benefit by joining an insurance pool where costs are evenly split, as opposed to being in separate risk pools. We build on this by producing a pricing scheme that maximally subsidizes the high risk group, while maintaining an incentive for lower risk people to stay in the insurance pool. Next, we demonstrate that with this new model, the price charged to each individual has to depend on the risk of other participants, making naive actuarial fairness inefficient. Furthermore, we prove that stable pricing schemes must be ones where players have the antisocial incentive desiring riskier partners, contradicting motivations for using actuarial fairness. Finally, we describe how these results relate to debates about fairness in machine learning and potential avenues for future research.","['Kate Donahue', 'Solon Barocas']","['Cornell University', 'Microsoft Research and Cornell University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing,"Data too sensitive to be ""open"" for analysis and re-purposing typically remains ""closed"" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.","['Meg Young', 'Luke Rodriguez', 'Emily Keller', 'Feiyang Sun', 'Boyang Sa', 'Jan Whittington', 'Bill Howe']","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting,"We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators---such as first names and pronouns---in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are ""scrubbed,"" and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.","['Maria De-Arteaga', 'Alexey Romanov', 'Hanna Wallach', 'Jennifer Chayes', 'Christian Borgs', 'Alexandra Chouldechova', 'Sahin Geyik', 'Krishnaram Kenthapadi', 'Adam Tauman Kalai']","['Carnegie Mellon University', 'University of Massachusetts Lowell', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'Carnegie Mellon University', 'LinkedIn', 'LinkedIn', 'Microsoft Research']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Bias in word embeddings,"Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings. We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data. We investigate bias diffusion and prove that existing biases are transferred to further machine learning models. We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient. Finally, we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data. Given that word embeddings are widely used by commercial companies, we discuss the challenges and required actions towards fair algorithmic implementations and applications.","['Orestis Papakyriakopoulos', 'Simon Hegelich', 'Juan Carlos Medina Serrano', 'Fabienne Marco']","['Technical University of Munich, Munich, Germany', 'Technical University of Munich, Munich, Germany', 'Technical University of Munich, Munich, Germany', 'Technical University of Munich, Munich, Germany']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Biases in Generative Art: A Causal Look from the Lens of Art History,"With rapid progress in artificial intelligence (AI), popularity of generative art has grown substantially. From creating paintings to generating novel art styles, AI based generative art has showcased a variety of applications. However, there has been little focus concerning the ethical impacts of AI based generative art. In this work, we investigate biases in the generative art AI pipeline right from those that can originate due to improper problem formulation to those related to algorithm design. Viewing from the lens of art history, we discuss the socio-cultural impacts of these biases. Leveraging causal models, we highlight how current methods fall short in modeling the process of art creation and thus contribute to various types of biases. We illustrate the same through case studies, in particular those related to style transfer. To the best of our knowledge, this is the first extensive analysis that investigates biases in the generative art AI pipeline from the perspective of art history. We hope our work sparks interdisciplinary discussions related to accountability of generative art.","['Ramya Srinivasan', 'Kanji Uchino']","['Fujitsu Laboratories of America', 'Fujitsu Laboratories of America']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Bidding strategies with gender nondiscrimination constraints for online ad auctions,"Interactions between bids to show ads online can lead to an advertiser's ad being shown to more men than women even when the advertiser does not target towards men. We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism. We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination, proving our strategies to be optimal in some settings. We use simulations to understand other settings.","['Milad Nasr', 'Michael Carl Tschantz']","['University of Massachusetts Amherst', 'International Computer Science Institute']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Black Feminist Musings on Algorithmic Oppression,"This paper uses a theory of oppression to ground and extend algorithmic oppression. Algorithmic oppression is then situated through a Black feminist lens part of which entails highlighting the double bind of technology. To reconcile algorithmic oppression with respect to the fairness, accountability, and transparency community, I critique the language of the community. Lastly, I place algorithmic oppression in a broader conversation of feminist science, technology, and society studies to ground the discussion of ways forward through abolition and empowering marginalized communities.",['Lelia Marie Hampton'],"['EECS and CSAIL, MIT, Cambridge, MA, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,"Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.","['Jwala Dhamala', 'Tony Sun', 'Varun Kumar', 'Satyapriya Krishna', 'Yada Pruksachatkun', 'Kai-Wei Chang', 'Rahul Gupta']","['Amazon Alexa AI-NU, USA', 'UC Santa Barbara, USA', 'Amazon Alexa AI-NU, USA', 'Amazon Alexa AI-NU, USA', 'Amazon Alexa AI-NU, USA', 'Amazon Alexa AI-NU, UCLA USA', 'Amazon Alexa AI-NU, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness,"Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.","['Jessie Finocchiaro', 'Roland Maio', 'Faidra Monachou', 'Gourab K Patro', 'Manish Raghavan', 'Ana-Andreea Stoica', 'Stratis Tsirtsis']","['CU Boulder', 'Columbia University', 'Stanford University', 'IIT Kharagpur', 'Cornell University', 'Columbia University', 'Max Planck Institute for Software Systems']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Bringing the People Back In: Contesting Benchmark Machine Learning Datasets,"In response to algorithmic unfairness embedded in sociotechnical systems, significant attention has been focused on the contents of machine learning datasets which have revealed biases towards white, cisgender, male, and Western data subjects. In contrast, comparatively less attention has been paid to the histories, values, and norms embedded in such datasets. In this work, we outline a research program - a genealogy of machine learning data - for investigating how and why these datasets have been created, what and whose values influence the choices of data to collect, the contextual and contingent conditions of their creation. We describe the ways in which benchmark datasets in machine learning operate as infrastructure and pose four research questions for these datasets. This interrogation forces us to ""bring the people back in"" by aiding us in understanding the labor embedded in dataset construction, and thereby presenting new avenues of contestation for other researchers encountering the data.",['Alex Hanna'],Google,"Participatory Approaches to Machine Learning, ICML 2020 Workshop (2020)",2020,TRUE
Building and Auditing Fair Algorithms: A Case Study in Candidate Screening,"Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of ""fairness"" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps. In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool. We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.","['Christo Wilson', 'Avijit Ghosh', 'Shan Jiang', 'Alan Mislove', 'Lewis Baker', 'Janelle Szary', 'Kelly Trindel', 'Frida Polli']","['Northeastern University', 'Northeastern University', 'Northeastern University', 'Northeastern University', 'pymetrics, inc.', 'pymetrics, inc.', 'pymetrics, inc.', 'pymetrics, inc.']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Can You Fake It Until You Make It?: Impacts of Differentially Private Synthetic Data on Downstream Classification Fairness,"The recent adoption of machine learning models in high-risk settings such as medicine has increased demand for developments in privacy and fairness. Rebalancing skewed datasets using synthetic data created by generative adversarial networks (GANs) has shown potential to mitigate disparate impact on minoritized subgroups. However, such generative models are subject to privacy attacks that can expose sensitive data from the training dataset. Differential privacy (DP) is the current leading solution for privacy-preserving machine learning. Differentially private GANs (DP GANs) are often considered a potential solution for improving model fairness while maintaining privacy of sensitive training data. We investigate the impact of using synthetic images from DP GANs on downstream classification model utility and fairness. We demonstrate that existing DP GANs cannot simultaneously maintain model utility, privacy, and fairness. The images generated from GAN models trained with DP exhibit extreme decreases in image quality and utility which leads to poor downstream classification model performance. Our evaluation highlights the friction between privacy, fairness, and utility and how this directly translates into real loss of performance and representation in common machine learning settings. Our results show that additional work improving the utility and fairness of DP generative models is required before they can be utilized as a potential solution to privacy and fairness issues stemming from lack of diversity in the training dataset.","['Victoria Cheng', 'Vinith M. Suriyakumar', 'Natalie Dullerud', 'Shalmali Joshi', 'Marzyeh Ghassemi']","['Vector Institute, University of Toronto, Snap Inc.', 'Vector Institute, University of Toronto', 'Vector Institute, University of Toronto', 'Vector Institute', 'Vector Institute, University of Toronto Canadian CIFAR AI Chair']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Case study: predictive fairness to reduce misdemeanor recidivism through social service interventions,"The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses. Often due to constraints of caseload and poor record linkage, prior interactions with an individual may not be considered when an individual comes back into the system, let alone in a proactive manner through the application of diversion programs. The Los Angeles City Attorney's Office recently created a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing recidivism in this population. Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting. The program seeks to improve outcomes by developing individually-tailored social service interventions (i.e., diversions, conditional plea agreements, stayed sentencing, or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods) for individuals likely to experience subsequent interactions with the criminal justice system, a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case. Seeking to achieve both efficiency (through predictive accuracy) and equity (improving outcomes in traditionally under-served communities and working to mitigate existing disparities in criminal justice outcomes), we discuss the equity outcomes we seek to achieve, describe the corresponding choice of a metric for measuring predictive fairness in this context, and explore a set of options for balancing equity and efficiency when building and selecting machine learning models in an operational public policy setting.","['Kit T. Rodolfa', 'Erika Salomon', 'Lauren Haynes', 'Iván Higuera Mendieta', 'Jamie Larson', 'Rayid Ghani']","['Carnegie Mellon University', 'University of Chicago', 'University of Chicago', 'University of Chicago', ""Los Angeles City Attorney's Office"", 'Carnegie Mellon University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation,"NLP models are shown to suffer from robustness issues, for example, a model's prediction can be easily changed under small perturbations to the input. In this work, we aim to present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, it can generate adversarial texts through controllable attributes that are known to be invariant to task labels. For example, for a main task like sentiment classification, an example attribute can be different categories/domains, and a model should have similar performance across them; for a coreference resolution task, a model's performance should not differ across different demographic attributes. Different from many existing adversarial text generation approaches, we show that our model can generate adversarial texts that are more fluent, diverse, and with better task-label invariance guarantees. We aim to use this model to generate counterfactual texts that could better improve robustness in NLP models (e.g., through adversarial training), and we argue that our generation can create more natural attacks.","['Xuezhi Wang', 'Alex Beutel', 'Ed H. Chi']",Google,EMNLP 2020,2020,TRUE
Censorship of Online Encyclopedias: Implications for NLP Models,"While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them.","['Eddie Yang', 'Margaret E. Roberts']","['University of California, San Diego La Jolla, California', 'University of California, San Diego La Jolla, California']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Characterizing Sources of Uncertainty to Proxy Calibration and Disambiguate Annotator and Data Bias,"Supporting model interpretability for complex phenomena where annotators can legitimately disagree, such as emotion recognition, is a challenging machine learning task. In this work, we show that explicitly quantifying the uncertainty in such settings has interpretability benefits. We use a simple modification of a classical network inference using Monte Carlo dropout to give measures of epistemic and aleatoric uncertainty. We identify a significant correlation between aleatoric uncertainty and human annotator disagreement (r â .3). Additionally, we demonstrate how difficult and subjective training samples can be identified using aleatoric uncertainty and how epistemic uncertainty can reveal data bias that could result in unfair predictions. We identify the total uncertainty as a suitable surrogate for model calibration, i.e. the degree we can trust model's predicted confidence. In addition to explainability benefits, we observe modest performance boosts from incorporating model uncertainty.",['Brendan Jou'],Google,ICCV Workshop on Interpreting and Explaining Visual Artificial Intelligence Models (2019),2019,TRUE
Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings,"Machine learning models in health care are often deployed in settings where it is important to protect patient privacy. In such settings, methods for differentially private (DP) learning provide a general-purpose approach to learn models with privacy guarantees. Modern methods for DP learning ensure privacy through the addition of calibrated noise. The resulting privacy-preserving models are unable to learn too much information about the tails of a data distribution, resulting in a loss of accuracy that can disproportionately affect small groups. In this paper, we study the effects of DP learning in health care. We use state-of-the-art methods for DP learning to train privacy-preserving models in clinical prediction tasks, including x-ray classification of images and mortality prediction in time series data. We use these models to perform a comprehensive empirical investigation of the tradeoffs between privacy, utility, robustness to dataset shift and fairness. Our results highlight lesser-known limitations of methods for DP learning in health care, models that exhibit steep tradeoffs between privacy and utility, and models whose predictions are disproportionately influenced by large demographic groups in the training data. We discuss the costs and benefits of differentially private learning in health care with open directions for differential privacy, machine learning and health care.","['Vinith M. Suriyakumar', 'Nicolas Papernot', 'Anna Goldenberg', 'Marzyeh Ghassemi']","['University of Toronto, Vector Institute', 'University of Toronto, Vector Institute', 'University of Toronto, Vector Institute', 'University of Toronto, Vector Institute']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees,"Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts. Several recent works have focused on studying classification with respect to specific fairness metrics, modeled the corresponding fair classification problem as constrained optimization problems, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which there are no fair classifiers with theoretical guarantees; primarily because the resulting optimization problem is non-convex. The main contribution of this paper is a meta-algorithm for classification that can take as input a general class of fairness constraints with respect to multiple non-disjoint and multi-valued sensitive attributes, and which comes with provable guarantees. In particular, our algorithm can handle non-convex ""linear fractional"" constraints (which includes fairness constraints such as predictive parity) for which no prior algorithm was known. Key to our results is an algorithm for a family of classification problems with convex constraints along with a reduction from classification problems with linear fractional constraints to this family. Empirically, we observe that our algorithm is fast, can achieve near-perfect fairness with respect to various fairness metrics, and the loss in accuracy due to the imposed fairness constraints is often small.","['L. Elisa Celis', 'Lingxiao Huang', 'Vijay Keswani', 'Nisheeth K. Vishnoi']","['Yale University', 'EPFL', 'EPFL', 'Yale University']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
"Clear Sanctions, Vague Rewards: How China's Social Credit System Currently Defines ""Good"" and ""Bad"" Behavior","China's Social Credit System (SCS, 社会信用体系 or shehui xinyong tixi) is expected to become the first digitally-implemented nationwide scoring system with the purpose to rate the behavior of citizens, companies, and other entities. Thereby, in the SCS, ""good"" behavior can result in material rewards and reputational gain while ""bad"" behavior can lead to exclusion from material resources and reputational loss. Crucially, for the implementation of the SCS, society must be able to distinguish between behaviors that result in reward and those that lead to sanction. In this paper, we conduct the first transparency analysis of two central administrative information platforms of the SCS to understand how the SCS currently defines ""good"" and ""bad"" behavior. We analyze 194,829 behavioral records and 942 reports on citizens' behaviors published on the official Beijing SCS website and the national SCS platform ""Credit China"", respectively. By applying a mixed-method approach, we demonstrate that there is a considerable asymmetry between information provided by the so-called Redlist (information on ""good"" behavior) and the Blacklist (information on ""bad"" behavior). At the current stage of the SCS implementation, the majority of explanations on blacklisted behaviors includes a detailed description of the causal relation between inadequate behavior and its sanction. On the other hand, explanations on redlisted behavior, which comprise positive norms fostering value internalization and integration, are less transparent. Finally, this first SCS transparency analysis suggests that socio-technical systems applying a scoring mechanism might use different degrees of transparency to achieve particular behavioral engineering goals.","['Severin Engelmann', 'Mo Chen', 'Felix Fischer', 'Ching-yu Kao', 'Jens Grossklags']","['Technical University of Munich', 'Technical University of Munich', 'Technical University of Munich and IF, London', 'Fraunhofer Institute for Applied and Integrated Security', 'Technical University of Munich']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing,"Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source. In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.","['Inioluwa Deborah Raji', 'Andrew Smart', 'Rebecca N. White', 'Margaret Mitchell', 'Timnit Gebru', 'Ben Hutchinson', 'Jamila Smith-Loud', 'Daniel Theron', 'Parker Barnes']","['Partnership on AI', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI,"Many organizations have published principles intended to guide the ethical development and deployment of AI systems; however, their abstract nature makes them difficult to operationalize. Some organizations have therefore produced AI ethics checklists, as well as checklists for more specific concepts, such as fairness, as applied to AI systems. But unless checklists are grounded in practitioners’ needs, they may be misused. To understand the role of checklists in AI ethics, we conducted an iterative co-design process with 48 practitioners, focusing on fairness. We co-designed an AI fairness checklist and identified desiderata and concerns for AI fairness checklists in general. We found that AI fairness checklists could provide organizational infrastructure for formalizing ad-hoc processes and empowering individual advocates. We discuss aspects of organizational culture that may impact the efficacy of such checklists, and highlight future research directions.","['Michael Madaio', 'Luke Stark', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,CHI Conference on Human Factors in Computing Systems,2020-03-01,TRUE
"Computer Science Communities: Who is Speaking, and Who is Listening to the Women? Using an Ethics of Care to Promote Diverse Voices","Those working on policy, digital ethics and governance often refer to issues in 'computer science', that includes, but is not limited to, common subfields such as Artificial Intelligence (AI), Computer Science (CS) Computer Security (InfoSec), Computer Vision (CV), Human Computer Interaction (HCI), Information Systems, (IS), Machine Learning (ML), Natural Language Processing (NLP) and Systems Architecture. Within this framework, this paper is a preliminary exploration of two hypotheses, namely 1) Each community has differing inclusion of minoritised groups (using women as our test case, by identifying female-sounding names); and 2) Even where women exist in a community, they are not published representatively. Using data from 20,000 research records, totalling 503,318 names, preliminary data supported our hypothesis. We argue that ACM has an ethical duty of care to its community to increase these ratios, and to hold individual computing communities to account in order to do so, by providing incentives and a regular reporting system, in order to uphold its own Code.","['Marc Cheong', 'Kobi Leins', 'Simon Coghlan']","['Centre for AI and Digital Ethics, University of Melbourne Parkville, VIC, Australia', 'Centre for AI and Digital Ethics, University of Melbourne Parkville, VIC, Australia', 'Centre for AI and Digital Ethics, University of Melbourne Parkville, VIC, Australia']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Controlling Polarization in Personalization: An Algorithmic Framework,"Personalization is pervasive in the online space as it leads to higher efficiency for the user and higher revenue for the platform by individualizing the most relevant content for each user. However, recent studies suggest that such personalization can learn and propagate systemic biases and polarize opinions; this has led to calls for regulatory mechanisms and algorithms that are constrained to combat bias and the resulting echo-chamber effect. We propose a versatile framework that allows for the possibility to reduce polarization in personalized systems by allowing the user to constrain the distribution from which content is selected. We then present a scalable algorithm with provable guarantees that satisfies the given constraints on the types of the content that can be displayed to a user, but -- subject to these constraints -- will continue to learn and personalize the content in order to maximize utility. We illustrate this framework on a curated dataset of online news articles that are conservative or liberal, show that it can control polarization, and examine the trade-off between decreasing polarization and the resulting loss to revenue. We further exhibit the flexibility and scalability of our approach by framing the problem in terms of the more general diverse content selection problem and test it empirically on both a News dataset and the MovieLens dataset.","['L. Elisa Celis', 'Sayash Kapoor', 'Farnood Salehi', 'Nisheeth Vishnoi']","['Yale University', 'IIT Kanpur', 'École Polytechnique Fédérale de Lausanne (EPFL)', 'Yale University']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Corporate Social Responsibility via Multi-Armed Bandits,"We propose a multi-armed bandit setting where each arm corresponds to a subpopulation, and pulling an arm is equivalent to granting an opportunity to this subpopulation. In this setting the decision-maker's fairness policy governs the number of opportunities each subpopulation should receive, which typically depends on the (unknown) reward from granting an opportunity to this subpopulation. The decision-maker can decide whether to provide these opportunities, or pay a pre-defined monetary value for every withheld opportunity. The decision-maker's objective is to maximize her utility, which is the sum of rewards minus the cost paid for withheld opportunities. We provide a no-regret algorithm that maximizes the decision-maker's utility and complement our analysis with an almost-tight lower bound. Finally, we discuss the fairness policy and demonstrate its downstream implications on the utility and opportunities via simulations.","['Tom Ron', 'Omer Ben-Porat', 'Uri Shalit']","['Technion - Israel Institute of Technology', 'Tel-Aviv University', 'Technion - Israel Institute of Technology']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Counterfactual Fairness in Text Classification through Robustness,"In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that ""Some people are gay'' is toxic while ""Some people are straight'' is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.","['Vincent Perot', 'Ed H. Chi', 'Alex Beutel']",Google,"AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) (2019)",2019,TRUE
"Counterfactual risk assessments, evaluation, and fairness","Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome. Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may---and as we show empirically, do---induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.","['Amanda Coston', 'Alan Mishler', 'Edward H. Kennedy', 'Alexandra Chouldechova']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Data in New Delhi's predictive policing system,"In 2015, Delhi Police announced plans for predictive policing. The Crime Mapping, Analytics and Predictive System (CMAPS) would be implemented in India's capital, for live spatial hotspot mapping of crime, criminal behavior patterns and suspect analysis. Four years later, there is little known about the effect of CMAPS due to the lack of public accountability mechanisms and large exceptions for law enforcement under India's Right to Information Act. Through an ethnographic study of Delhi Police's data collection practices, and analysing the institutional and legal reality within which CMAPS will function, this paper presents one of the first accounts of smart policing in India. Through our findings and discussion we show what kinds of biases are present within Delhi Police's data collection practices currently and how they translate and transfer into initiatives like CMAPS. We further discuss what the biases in CMAPS can teach us about future public sector deployment of socio-technical systems in India and other global South geographies. We also offer methodological considerations for studying AI deployments in non-western contexts. We conclude with a set of recommendations for civil society and social justice actors to consider when engaging with opaque systems implemented in the public sector.","['Vidushi Marda', 'Shivangi Narayan']","['Article 19', 'Jawaharlal Nehru University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Data Leverage: A Framework for Empowering the Public in its Relationship with Technology Companies,"Many powerful computing technologies rely on implicit and explicit data contributions from the public. This dependency suggests a potential source of leverage for the public in its relationship with technology companies: by reducing, stopping, redirecting, or otherwise manipulating data contributions, the public can reduce the effectiveness of many lucrative technologies. In this paper, we synthesize emerging research that seeks to better understand and help people action this data leverage. Drawing on prior work in areas including machine learning, human-computer interaction, and fairness and accountability in computing, we present a framework for understanding data leverage that highlights new opportunities to change technology company behavior related to privacy, economic inequality, content moderation and other areas of societal concern. Our framework also points towards ways that policymakers can bolster data leverage as a means of changing the balance of power between the public and tech companies.","['Nicholas Vincent', 'Hanlin Li', 'Nicole Tilly', 'Stevie Chancellor', 'Brent Hecht']","['Northwestern University', 'Northwestern University', 'Northwestern University', 'University of Minnesota', 'Northwestern University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Datasheets for Datasets,"The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.","['Timnit Gebru', 'Jamie Morgenstern', 'Briana Vecchione', 'Jennifer Wortman Vaughan', 'Hanna Wallach', 'Hal Daumé III', 'Kate Crawford']",Microsoft,UNKNOWN,2018-03-01,TRUE
Debiasing Embeddings for Fairer Text Classification,"(Bolukbasi et al., 2016) demonstrated that pre-trained  word embeddings  can  inherit  gender bias from the data they were trained on.  We investigate  how  this  bias  affects  downstream classification  tasks,  using  the  case  study  of occupation  classification  (De-Arteaga  et  al.,2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy  channel  for  communicating  gender information.   With  a  relatively  minor  adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and obtain high classification accuracy.","['Flavien Prost', 'Nithum Thain', 'Tolga Bolukbasi']",Google,1st ACL Workshop on Gender Bias for Natural Language Processing (2019),2019,TRUE
Deep determinantal generative classifier: robustness on noisy and adversarial samples,"Large-scale datasets may contain significant proportions of noisy (incorrect) class labels, and it is well-known that modern deep neural networks (DNNs) poorly generalize from such noisy training datasets. To mitigate the issue, we propose a novel inference method, termed Robust Generative classifier (RoG), applicable to any discriminative (e.g., softmax) neural classifier pre-trained on noisy datasets. In particular, we induce a generative classifier on top of hidden feature spaces of the pre-trained DNNs, for obtaining a more robust decision boundary. By estimating the parameters of generative classifier using the minimum covariance determinant estimator, we significantly improve the classification accuracy with neither re-training of the deep model nor changing its architectures. With the assumption of Gaussian distribution for features, we prove that RoG generalizes better than baselines under noisy labels. Finally, we propose the ensemble version of RoG to improve its performance by investigating the layer-wise characteristics of DNNs. Our extensive experimental results demonstrate the superiority of RoG given different learning models optimized by several training techniques to handle diverse scenarios of noisy labels.",[],Google,ICML (2019),2019,TRUE
Deep Weighted Averaging Classifiers,"Recent advances in deep learning have achieved impressive gains in classification accuracy on a variety of types of data, including images and text. Despite these gains, however, concerns have been raised about the calibration, robustness, and interpretability of these models. In this paper we propose a simple way to modify any conventional deep architecture to automatically provide more transparent explanations for classification decisions, as well as an intuitive notion of the credibility of each prediction. Specifically, we draw on ideas from nonparametric kernel regression, and propose to predict labels based on a weighted sum of training instances, where the weights are determined by distance in a learned instance-embedding space. Working within the framework of conformal methods, we propose a new measure of nonconformity suggested by our model, and experimentally validate the accompanying theoretical expectations, demonstrating improved transparency, controlled error rates, and robustness to out-of-domain data, without compromising on accuracy or calibration.","['Dallas Card', 'Michael Zhang', 'Noah A. Smith']","['Machine Learning Department, Carnegie Mellon University, Pittsburgh, Pennsylvania', 'Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, Washington', 'Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, Washington and Allen Institute for Artificial, Intelligence, Seattle, Washington']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Deontological Ethics By Monotonicity Shape Constraints,"We demonstrate how easy it is for modern machine-learned systems to violate common deontological ethical principles and social norms such as ""favor the less fortunate,"" and ""do not penalize good attributes."" We propose that in some cases such ethical principles can be incorporated into a machine-learned model by adding shape constraints that constrain the model to respond only positively to relevant inputs. We analyze the relationship between these deontological constraints that act on individuals and the consequentialist group-based fairness goals of one-sided statistical parity and equal opportunity. This strategy works with sensitive attributes that are Boolean or real-valued such as income and age, and can help produce more responsible and trustworthy AI.",['Serena Wang'],Google,AISTATS (2020),2020,TRUE
Designing Accountable Systems,"Accountability is an often called for property of technical systems. It is a requirement for algorithmic decision systems, autonomous cyber-physical systems, and for software systems in general. As a concept, accountability goes back to the early history of Liberalism and is suggested as a tool to limit the use of power. This long history has also given us many, often slightly differing, definitions of accountability. The problem that software developers now face is to understand what accountability means for their systems and how to reflect it in a system's design. To enable the rigorous study of accountability in a system, we need models that are suitable for capturing such a varied concept. In this paper, we present a method to express and compare different definitions of accountability using Structural Causal Models. We show how these models can be used to evaluate a system's design and present a small use case based on an autonomous car.","['Severin Kacianka', 'Alexander Pretschner']","['Technical University of Munich, Garching, Germany', 'Technical University of Munich, Garching, Germany']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Designing an Online Infrastructure for Collecting AI Data From People With Disabilities,"AI technology offers opportunities to expand virtual and physical access for people with disabilities. However, an important part of bringing these opportunities to fruition is ensuring that upcoming AI technology works well for people with a wide range of abilities. In this paper, we identify the lack of data from disabled populations as one of the challenges to training and benchmarking fair and inclusive AI systems. As a potential solution, we envision an online infrastructure that can enable large-scale, remote data contributions from disability communities. We investigate the motivations, concerns, and challenges that people with disabilities might experience when asked to collect and upload various forms of AI-relevant data through a semi-structured interview and an online survey that simulated a data contribution process by collecting example data files through an online portal. Based on our findings, we outline design guidelines for developers creating online infrastructures for gathering data from people with disabilities.","['Joon Sung Park', 'Danielle Bragg', 'Ece Kamar', 'Meredith Ringel Morris']","['Microsoft Research - Redmond, Stanford University, Stanford, CA, USA', 'Microsoft Research - New England Cambridge, MA, USA', 'Microsoft Research - Redmond Redmond, WA, USA', 'Microsoft Research - Redmond Redmond, WA, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Designing Unbiased Surveys for HCI Research,"Surveys are a commonly used method within HCI research. While it initially appears easy and inexpensive to conduct surveys, overlooking key considerations in questionnaire design and the survey research process can yield skewed, biased, or entirely invalid survey results. Fortunately decades of academic research and analysis exist on optimizing the validity and reliability of survey data, from which this course will draw. To enable the creation of unbiased surveys, this course demonstrates questionnaire design biases and pitfalls, provides best practices for minimizing these, and reviews different uses of surveys within HCI.","['Hendrik Müller', 'Aaron Sedley']",Google,CHI '14 Extended Abstracts on Human Factors in Computing Systems (2014),2014,TRUE
Detecting Bias with Generative Counterfactual Face  Attribute Augmentation,"We introduce a simple framework for identifying biases of a smiling attribute classifier. Our method poses counterfactual questions of the form: how would the prediction change if this face characteristic had been different? We leverage recent advances in generative adversarial networks to build a realistic generative model of faces that affords controlled manipulation of specific facial characteristics. Empirically, we identify several different factors of variation (that we believe should be in-dependent of a smiling) that  affect the predictions of a smiling classifier trained on CelebA.",['Ben Hutchinson'],Google,"Fairness, Accountability, Transparency and Ethics in Computer Vision Workshop (in conjunction with CVPR) (2019)",2019,TRUE
Detecting discriminatory risk through data annotation based on Bayesian inferences,"Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination.","['Elena Beretta', 'Antonio Vetrò', 'Bruno Lepri', 'Juan Carlos De Martin']","['Nexa Center for Internet & Society, Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy Fondazione Bruno Kessler Trento, Italy', 'Nexa Center for Internet & Society, Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy', 'Fondazione Bruno Kessler, Trento, Italy', 'Nexa Center for Internet & Society, Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Differential Tweetment: Mitigating Racial Dialect Bias in Harmful Tweet Detection,"Automated systems for detecting harmful social media content are afflicted by a variety of biases, some of which originate in their training datasets. In particular, some systems have been shown to propagate racial dialect bias: they systematically classify content aligned with the African American English (AAE) dialect as harmful at a higher rate than content aligned with White English (WE). This perpetuates prejudice by silencing the Black community. Towards this problem we adapt and apply two existing bias mitigation approaches: preferential sampling pre-processing and adversarial debiasing in-processing. We analyse the impact of our interventions on model performance and propagated bias. We find that when bias mitigation is employed, a high degree of predictive accuracy is maintained relative to baseline, and in many cases bias against AAE in harmful tweet predictions is reduced. However, the specific effects of these interventions on bias and performance vary widely between dataset contexts. This variation suggests the unpredictability of autonomous harmful content detection outside of its development context. We argue that this, and the low performance of these systems at baseline, raise questions about the reliability and role of such systems in high-impact, real-world settings.","['Ari Ball-Burack', 'Michelle Seng Ah Lee', 'Jennifer Cobbe', 'Jatinder Singh']","['Compliant & Accountable Systems Group University of Cambridge, UK', 'Compliant & Accountable Systems Group University of Cambridge, UK', 'Compliant & Accountable Systems Group University of Cambridge, UK', 'Compliant & Accountable Systems Group University of Cambridge, UK']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Diminishing Returns Shape Constraints for Interpretability and Regularization,"We investigate machine learning models that can provide diminishing returns
and accelerating returns guarantees to capture prior knowledge or policies
about how outputs should depend on inputs. We show that one can build
flexible, nonlinear, multi-dimensional models using lattice functions with any
combination of concavity/convexity and monotonicity constraints on any
subsets of features, and compare to new shape-constrained neural networks.
We demonstrate on real-world examples that these shape constrained models
can provide tuning-free regularization and improve model understandability.","['Dara Bahri', 'Andy Cotter', 'Kevin Canini']",Google,NIPS 2018 (2018),2018,TRUE
Direct Uncertainty Prediction for Medical Second Opinions,"The issue of disagreements amongst human experts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresponds to doctor disagreements on a patient diagnosis. In this work, we show that machine learning models can be successfully trained to give uncertainty scores to data instances that result in high expert disagreements. In particular, they can identify patient cases that would benefit most from a medical second opinion. Our central methodological finding is that Direct Uncertainty Prediction (DUP), training a model to predict an uncertainty score directly from the raw patient features, works better than Uncertainty Via Classification, the two step process of training a classifier and postprocessing the output distribution to give an uncertainty score. We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging application.","['Maithra Raghu', 'Rory Abbott Sayres']",Google,ICML (2019),2019,TRUE
Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability,"Nowadays, the use of machine learning models is becoming a utility in many applications. Companies deliver pre-trained models encapsulated as application programming interfaces (APIs) that developers combine with third-party components and their own models and data to create complex data products to solve specific problems. The complexity of such products and the lack of control and knowledge of the internals of each component used unavoidable cause effects, such as lack of transparency, difficulty in auditability, and the emergence of potential uncontrolled risks. They are effectively black-boxes. Accountability of such solutions is a challenge for the auditors and the machine learning community. In this work, we propose a wrapper that given a black-box model enriches its output prediction with a measure of uncertainty when applied to a target domain. To develop the wrapper, we follow these steps: Modeling the distribution of the output. In a text classification setting, the output is a probability distribution p(y|X, w*) over the different classes to predict, y, given an input text X and the pre-trained model with parameters w*. We model this output by a random variable to measure the variability that the data noise causes in the output. Here we consider the output distribution coming from a Dirichlet probability density function, thus p(y|X, w*) ~ Dir(α). Decomposition of the Dirichlet concentration parameter. To relate the output of the classifier with the concentration parameter in the Dirichlet distribution, we propose a decomposition of the concentration parameter in two terms: α = βy. The role of this scalar β is to control the spread of the distribution around the expected value, i.e. the original prediction y. Training the wrapper. Sentences are represented as the average value of their word embeddings. This representation feeds a neural network that outputs a single regression value that models the parameter β. For each input, we combine β and the black-box prediction to obtain the corresponding distribution for the output ym,i ~ Dir(αi). By using Monte Carlo sampling, we approximate the expected value of the classification probabilities, [EQUATION] and we train the model applying a cross-entropy loss over the predictions and the labels. Obtaining an uncertainty score from the wrapper. To obtain a numerical value for the uncertainty of a prediction, we draw samples from the resulting Dir(α) to evaluate the predictive entropy with [EQUATION], thus obtaining a numerical score for the uncertainty of each prediction. Using uncertainty for rejection. Based on this wrapper, we provide an actionable mechanism to mitigate risk in the form of decision rejection: once equipped with a value for the uncertainty of a given prediction, we can choose not to issue that prediction when the risk or uncertainty in that decision is significant. This results in a rejection system that selects the more confident predictions, discards those more uncertain, and leads to an improvement in the trustability of the resulting system. We showcase the proposed technique and methodology in a practical scenario where we apply a simulated sentiment analysis API based on NLP to different domains. On each experiment, we train a sentiment classifier using text reviews of products in a source domain. We apply the pre-trained black-box to obtain the predictions for the reviews from a target domain. The tuples of review plus black-box predictions are then used for training the wrapper to obtain the uncertainty. Finally, we use the uncertainty score to sort the predictions from more to less uncertain, and we search for a rejection point that maximizes the three performance measures: non-rejected accuracy, and classification and rejection quality. Experiments demonstrate the effectiveness of the uncertainty measure computed by the wrapper and shows its high correlation to bad quality predictions and misclassifications. In all the cases, the uncertainty metric here proposed outperforms traditional uncertainty measures.","['José Mena Roldán', 'Oriol Pujol Vila', 'Jordi Vitrià Marca']","['Universitat de Barcelona', 'Universitat de Barcelona', 'Universitat de Barcelona']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Discovering User Bias in Ordinal Voting Systems,"Crowdsourcing systems increasingly rely on users to provide more
subjective ground truth for intelligent systems - e.g. ratings, aspect
of quality and perspectives on how expensive or lively a place feels,
etc. We focus on the ubiquitous implementation of online user ordinal voting (e.g 1-5, 1 star-4 stars) on some aspect of an entity, to
extract a relative truth, measured by a selected metric such as vote
plurality or mean. We argue that this methodology can aggregate
results that yield little information to the end user. In particular,
ordinal user rankings often converge to a indistinguishable rating.
This is demonstrated by the trend in certain cities for the majority of restaurants to all have a 4 star rating. Similarly, the rating of an establishment can be significantly affected by a few users.
User bias in voting is not spam, but rather a preference that can
be harnessed to provide more information to users. We explore
notions of both global skew and user bias. Leveraging these bias
and preference concepts, the paper suggests explicit models for
better personalization and more informative ratings.","['Alyssa Whitlock Lees', 'Chris Welty']",Google,"SAD-2019: Workshop on Subjectivity, Ambiguity and Disagreement",2019,TRUE
Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments,"Despite vigorous debates about the technical characteristics of risk assessments being deployed in the U.S. criminal justice system, remarkably little research has studied how these tools affect actual decision-making processes. After all, risk assessments do not make definitive decisions---they inform judges, who are the final arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a first step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk. We found several behaviors that call into question the supposed efficacy and fairness of risk assessments: our study participants 1) underperformed the risk assessment even when presented with its predictions, 2) could not effectively evaluate the accuracy of their own or the risk assessment's predictions, and 3) exhibited behaviors fraught with ""disparate interactions,"" whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants. These results suggest the need for a new ""algorithm-in-the-loop"" framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract. If risk assessments are to be used at all, they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential.","['Ben Green', 'Yiling Chen']","['Harvard University', 'Harvard University']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Dissecting Racial Bias in an Algorithm that Guides Health Decisions for 70 Million People,"A single algorithm drives an important health care decision for over 70 million people in the US. When health systems anticipate that a patient will have especially complex and intensive future health care needs, she is enrolled in a 'care management' program, which provides considerable additional resources: greater attention from trained providers and help with coordination of her care. To determine which patients will have complex future health care needs, and thus benefit from program enrollment, many systems rely on an algorithmically generated commercial risk score. In this paper, we exploit a rich dataset to study racial bias in a commercial algorithm that is deployed nationwide today in many of the US's most prominent Accountable Care Organizations (ACOs). We document significant racial bias in this widely used algorithm, using data on primary care patients at a large hospital. Blacks and whites with the same algorithmic risk scores have very different realized health. For example, the highest-risk black patients (those at the threshold where patients are auto-enrolled in the program), have significantly more chronic illnesses than white enrollees with the same risk score. We use detailed physiological data to show the pervasiveness of the bias: across a range of biomarkers, from HbA1c levels for diabetics to blood pressure control for hypertensives, we find significant racial health gaps conditional on risk score. This bias has significant material consequences for patients: it effectively means that white patients with the same health as black patients are far more likely be enrolled in the care management program, and benefit from its resources. If we simulated a world without this gap in predictions, blacks would be auto-enrolled into the program at more than double the current rate. An unusual aspect of our dataset is that we observe not just the risk scores but also the input data and objective function used to construct it. This provides a unique window into the mechanisms by which bias arises. The algorithm is given a data frame with (1) Yit (label), total medical expenditures ('costs') in year t; and (2) Xi,t--1 (features), fine-grained care utilization data in year t -- 1 (e.g., visits to cardiologists, number of x-rays, etc.). The algorithm's predicted risk of developing complex health needs is thus in fact predicted costs. And by this metric, one could easily call the algorithm unbiased: costs are very similar for black and white patients with the same risk scores. So far, this is inconsistent with algorithmic bias: conditional on risk score, predictions do not favor whites or blacks. The fundamental problem we uncover is that when thinking about 'health care needs,' hospitals and insurers focus on costs. They use an algorithm whose specific objective is cost prediction, and from this perspective, predictions are accurate and unbiased. Yet from the social perspective, actual health -- not just costs -- also matters. This is where the problem arises: costs are not the same as health. While costs are a reasonable proxy for health (the sick do cost more, on average), they are an imperfect one: factors other than health can drive cost -- for example, race. We find that blacks cost more than whites on average; but this gap can be decomposed into two countervailing effects. First, blacks bear a different and larger burden of disease, making them costlier. But this difference in illness is offset by a second factor: blacks cost less, holding constant their exact chronic conditions, a force that dramatically reduces the overall cost gap. Perversely, the fact that blacks cost less than whites conditional on health means an algorithm that predicts costs accurately across racial groups will necessarily also generate biased predictions on health. The root cause of this bias is not in the procedure for prediction, or the underlying data, but the algorithm's objective function itself. This bias is akin to, but distinct from, 'mis-measured labels': it arises here from the choice of labels, not their measurement, which is in turn a consequence of the differing objective functions of private actors in the health sector and society. From the private perspective, the variable they focus on -- cost -- is being appropriately optimized. But our results hint at how algorithms may amplify a fundamental problem in health care as a whole: externalities produced when health care providers focus too narrowly on financial motives, optimizing on costs to the detriment of health. In this sense, our results suggest that a pervasive problem in health care -- incentives that induce health systems to focus on dollars rather than health -- also has consequences for the way algorithms are built and monitored.","['Ziad Obermeyer', 'Sendhil Mullainathan']","['UC Berkeley, Berkeley, CA', 'University of Chicago, Chicago, IL']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Diversity and Inclusion Metrics for Subset Selection,"The concept of fairness has recently been applied in machine learning settings to describe a wide range of constraints and objectives. When applied to ranking, recommendation, or subset selection problems for an individual, it becomes less clear that fairness goals are more applicable than goals that prioritize diverse outputs and instances that represent the individual's goals well.  In this work, we discuss the relevance of the concept of fairness to the concepts of diversity and inclusion, and introduce metrics that quantify the diversity and inclusion of an instance or set.  Diversity and inclusion metrics can be used in tandem, including additional fairness constraints, or may be used separately, and we detail how the different metrics interact.  Results from human subject experiments demonstrate that the proposed criteria for diversity and inclusion are consistent with social notions of these two concepts, and human judgments on the diversity and inclusion of example instances are correlated with the defined metrics.","['Dylan Baker', 'Ben Hutchinson', 'Alex Hanna']",Google,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES) (2020)",2020,TRUE
Diversity-Sensitive Conditional Generative Adversarial Networks,"We propose a simple yet highly effective method that addresses the mode-collapse
problem in the Conditional Generative Adversarial Network (cGAN). Although
conditional distributions are multi-modal (i.e., having many modes) in practice,
most cGAN approaches tend to learn an overly simplified distribution where an
input is always mapped to a single output regardless of variations in latent code.
To address such issue, we propose to explicitly regularize the generator to produce
diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives.
Additionally, explicit regularization on generator allows our method to control a
balance between visual quality and diversity. We demonstrate the effectiveness
of our method on three conditional generation tasks: image-to-image translation,
image inpainting, and future video prediction. We show that simple addition of
our regularization to existing models leads to surprisingly diverse generations,
substantially outperforming the previous approaches for multi-modal conditional
generation specifically designed in each individual task.",[],Google,ICLR (2019),2019,TRUE
Doctor XAI: an ontology-based approach to black-box sequential data classification explanations,"Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.","['Cecilia Panigutti', 'Alan Perotti', 'Dino Pedreschi']","['Scuola Normale Superiore', 'ISI foundation', 'University of Pisa']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Documenting Computer Vision Datasets: An Invitation to Reflexive Data Practices,"In industrial computer vision, discretionary decisions surrounding the production of image training data remain widely undocumented. Recent research taking issue with such opacity has proposed standardized processes for dataset documentation. In this paper, we expand this space of inquiry through fieldwork at two data processing companies and thirty interviews with data workers and computer vision practitioners. We identify four key issues that hinder the documentation of image datasets and the effective retrieval of production contexts. Finally, we propose reflexivity, understood as a collective consideration of social and intellectual factors that lead to praxis, as a necessary precondition for documentation. Reflexive documentation can help to expose the contexts, relations, routines, and power structures that shape data.","['Milagros Miceli', 'Tianling Yang', 'Laurens Naudts', 'Martin Schuessler', 'Diana Serbanescu', 'Alex Hanna']","['Technische Universität Berlin', 'Technische Universität Berlin', 'Centre for IT & IP Law (CiTiP), KU Leuven', 'Technische Universität Berlin', 'Technische Universität Berlin', 'Google Research']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Downstream Effects of Affirmative Action,"We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions. We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. We show that both goals can be achieved when the college does not report grades. On the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy.","['Sampath Kannan', 'Aaron Roth', 'Juba Ziani']","['University of Pennsylvania', 'University of Pennsylvania', 'California Institute of Technology']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making,"Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.","['Yunfeng Zhang', 'Q. Vera Liao', 'Rachel K. E. Bellamy']","['IBM Research AI', 'IBM Research AI', 'IBM Research AI']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Efficient Search for Diverse Coherent Explanations,"This paper proposes new search algorithms for counterfactual explanations based upon mixed integer programming. We are concerned with complex data in which variables may take any value from a contiguous range or an additional set of discrete states. We propose a novel set of constraints that we refer to as a ""mixed polytope"" and show how this can be used with an integer programming solver to efficiently find coherent counterfactual explanations i.e. solutions that are guaranteed to map back onto the underlying data structure, while avoiding the need for brute-force enumeration. We also look at the problem of diverse explanations and show how these can be generated within our framework.",['Chris Russell'],['The University of Surrey and The Alan Turing Institute'],"FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Ensuring Fairness in Machine Learning to Advance Health Equity,"A central promise of machine learning (ML) is to use historical data to project the future trajectories of patients.  Will they have a good or bad outcome? What diagnoses will they have? What treatments should they be given?  But in many cases, we do not want the future to look like the past, especially when the past contains patterns of human or structural biases against vulnerable populations.","['Alvin Rishi Rajkomar', 'Greg Corrado', 'Michael Howell']",Google,Annals of Internal Medicine (2018),2018,TRUE
Epistemic values in feature importance methods: Lessons from feminist epistemology,"As the public seeks greater accountability and transparency from machine learning algorithms, the research literature on methods to explain algorithms and their outputs has rapidly expanded. Feature importance methods form a popular class of explanation methods. In this paper, we apply the lens of feminist epistemology to recent feature importance research. We investigate what epistemic values are implicitly embedded in feature importance methods and how or whether they are in conflict with feminist epistemology. We offer some suggestions on how to conduct research on explanations that respects feminist epistemic values, taking into account the importance of social context, the epistemic privileges of subjugated knowers, and adopting more interactional ways of knowing","['Leif Hancox-Li', 'I. Elizabeth Kumar']","['Capital One, New York, New York, USA', 'University of Utah, Salt Lake City, UT, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Equality of Voice: Towards Fair Representation in Crowdsourced Top-K Recommendations,"To help their users to discover important items at a particular time, major websites like Twitter, Yelp, TripAdvisor or NYTimes provide Top-K recommendations (e.g., 10 Trending Topics, Top 5 Hotels in Paris or 10 Most Viewed News Stories), which rely on crowdsourced popularity signals to select the items. However, different sections of a crowd may have different preferences, and there is a large silent majority who do not explicitly express their opinion. Also, the crowd often consists of actors like bots, spammers, or people running orchestrated campaigns. Recommendation algorithms today largely do not consider such nuances, hence are vulnerable to strategic manipulation by small but hyper-active user groups. To fairly aggregate the preferences of all users while recommending top-K items, we borrow ideas from prior research on social choice theory, and identify a voting mechanism called Single Transferable Vote (STV) as having many of the fairness properties we desire in top-K item (s)elections. We develop an innovative mechanism to attribute preferences of silent majority which also make STV completely operational. We show the generalizability of our approach by implementing it on two different real-world datasets. Through extensive experimentation and comparison with state-of-the-art techniques, we show that our proposed approach provides maximum user satisfaction, and cuts down drastically on items disliked by most but hyper-actively promoted by a few users.","['Abhijnan Chakraborty', 'Gourab K. Patro', 'Niloy Ganguly', 'Krishna P. Gummadi', 'Patrick Loiseau']","['MPI for Software Systems, Germany IIT Kharagpur, India', 'IIT Kharagpur, India', 'IIT Kharagpur, India', 'MPI for Software Systems, Germany', 'Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG & MPI SWS']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Evading the Curse of Dimensionality in Unconstrained Private Generalized Linear Problems,"Differentially private gradient descent (DP-GD) has been extremely effective both theoretically, and in practice, for solving private empirical risk minimization (ERM) problems. In this paper, we focus on understanding the impact of the clipping norm, a critical component of DP-GD, on its convergence. We provide the first formal convergence analysis of clipped DP-GD.","['Shuang Song', 'Om Thakkar']",Google,24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021) (2020),2020,TRUE
Evaluating Fairness of Machine Learning Models Under Uncertain and Incomplete Information,"Training and evaluation of fair classifiers is a challenging problem. This is partly due to the fact that most fairness metrics of interest depend on both the sensitive attribute information and label information of the data points. In many scenarios it is not possible to collect large datasets with such information. An alternate approach that is commonly used is to separately train an attribute classifier on data with sensitive attribute information, and then use it later in the ML pipeline to evaluate the bias of a given classifier. While such decoupling helps alleviate the problem of demographic scarcity, it raises several natural questions such as: how should the attribute classifier be trained?, and how should one use a given attribute classifier for accurate bias estimation? In this work we study this question from both theoretical and empirical perspectives. We first experimentally demonstrate that the test accuracy of the attribute classifier is not always correlated with its effectiveness in bias estimation for a downstream model. In order to further investigate this phenomenon, we analyze an idealized theoretical model and characterize the structure of the optimal classifier. Our analysis has surprising and counter-intuitive implications where in certain regimes one might want to distribute the error of the attribute classifier as unevenly as possible among the different subgroups. Based on our analysis we develop heuristics for both training and using attribute classifiers for bias estimation in the data scarce regime. We empirically demonstrate the effectiveness of our approach on real and simulated data.","['Pranjal Awasthi', 'Alex Beutel', 'Matthäus Kleindessner', 'Jamie Morgenstern', 'Xuezhi Wang']","['Rutgers University & Google', 'Google', 'Amazon', 'University of Washington & Google', 'Google']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Evaluating Stochastic Rankings with Expected Exposure,"We introduce the concept of expected exposure as the average attention ranked items receive from users over repeated samples of the same query. Furthermore, we advocate for the adoption of the principle of equal expected exposure: given a fixed information need, no item receive more or less expected exposure compared to any other item of the same relevance grade. We argue that this principle is desirable for many retrieval objectives and scenarios, including topical diversity and fair ranking. Leveraging user models from existing retrieval metrics, we propose a general evaluation methodology based on expected exposure and draw connections to related metrics in information retrieval evaluation. Importantly, this methodology relaxes classic information retrieval assumptions, allowing a system, in response to a query, to produce a distribution over rankings instead of a single fixed ranking. We study the behavior of the expected exposure metric and stochastic rankers across a variety of information access conditions, including ad hoc retrieval and recommendation. We believe that measuring and optimizing expected exposure metrics using randomization opens a new area for retrieval algorithm development and progress.","['Fernando Diaz', 'Bhaskar Mitra', 'Michael D. Ekstrand', 'Asia J. Biega', 'Ben Carterette']",Microsoft,Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM),2020-07-01,TRUE
Explainability fact sheets: a framework for systematic assessment of explainable approaches,"Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.","['Kacper Sokol', 'Peter Flach']","['University of Bristol, Bristol, United Kingdom', 'University of Bristol, Bristol, United Kingdom']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Explainable machine learning in deployment,"Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.","['Umang Bhatt', 'Alice Xiang', 'Shubham Sharma', 'Adrian Weller', 'Ankur Taly', 'Yunhan Jia', 'Joydeep Ghosh', 'Ruchir Puri', 'José M. F. Moura', 'Peter Eckersley']","['Carnegie Mellon University and Partnership on AI and University of Cambridge and Leverhulme CFI', 'Partnership on AI', 'University of Texas at Austin', 'University of Cambridge and Leverhulme CFI and The Alan Turing Institute', 'Fiddler Labs', 'Baidu', 'University of Texas at Austin and CognitiveScale', 'IBM Research', 'Carnegie Mellon University', 'Partnership on AI']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Explaining Deep Neural Networks using Unsupervised Clustering,"We propose a novel method to explain trained deep neural networks (DNNs), by distilling them into surrogate models using unsupervised clustering. Our method can be flexibly applied to any subset of layers of a DNN architecture and can incorporate low-level and high-level information. On image datasets given pre-trained DNNs, we demonstrate strength of our method in finding similar training samples, and shedding light on the concepts the DNN bases its decision on. Via user studies, we show that our model can improve user trust in modelâs prediction.",['Sercan Arik'],Google,2020 Workshop on Human Interpretability in Machine Learning (2020),2020,TRUE
Explaining Explanations in AI,"Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that ""All models are wrong but some are useful."" We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a ""do it yourself kit"" for explanations, allowing a practitioner to directly answer ""what if questions"" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.","['Brent Mittelstadt', 'Chris Russell', 'Sandra Wachter']","['University of Oxford, The Alan Turing Institute', 'University of Surrey, The Alan Turing Institute', 'University of Oxford, The Alan Turing Institute']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Explaining machine learning classifiers through diverse counterfactual explanations,"Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.","['Ramaravind K. Mothilal', 'Amit Sharma', 'Chenhao Tan']","['Microsoft Research India', 'Microsoft Research India', 'University of Colorado Boulder']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
"FACTS-IR: Fairness, Accountability, Confidentiality, Transparency, and Safety in Information Retrieval","The purpose of the SIGIR 2019 workshop on Fairness, Accountability, Confidentiality, Transparency, and Safety (FACTS-IR) was to explore challenges in responsible information retrieval system development and deployment. To this end, the workshop aimed to crowdsource from the larger SIGIR community and draft an actionable research agenda on five key dimensions of responsible information retrieval: fairness, accountability, confidentiality, transparency, and safety. Such an agenda can guide others in the community that are interested in pursuing FACTS-IR research, as well as inform potential funders about relevant research avenues. The workshop brought together a diverse set of researchers and practitioners interested in contributing to the development of a technical research agenda for responsible information retrieval.","['Alexandra Olteanu', 'Jean Garcia-Gathright', 'Maarten de Rijke', 'Michael D. Ekstrand']",Microsoft,ACM SIGIR Forum,2019-12-02,TRUE
Failure Modes of Variational Inference for Decision Making,"In this paper we highlight the risks of relying on
mean-field variational inference to learn models
that are used as simulators for decision making.
We study the role of accurate inference for latent
variable models in terms of cumulative reward
performance. We show how naive mean-field
variational inference at test time can lead to poor
decisions in basic but fundamental quadratic control problems with continuous actions, as relevant
correlations in the latent space are ignored. We
then extend these examples to a more complex
non-linear scenario with asymmetric costs, where
regret is even more significant.","['Carlos Riquelme', 'Matt Hoffman']",Google,ICML Workshop (2018),2018,TRUE
Fair Algorithms for Learning in Allocation Problems,"Settings such as lending and policing can be modeled by a centralized agent allocating a scarce resource (e.g. loans or police officers) amongst several groups, in order to maximize some objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such problems fairness is also a concern. One natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource in question, the probability of actually receiving it is approximately independent of the individual's group. For example, in lending this would mean that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it would mean that two individuals committing the same crime in different districts would have roughly equal chances of being arrested. In this paper, we formalize this general notion of fairness for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the allocator does not know the frequency of candidates (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates in each group. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to and do not learn about crimes committed if the police presence in a district is low. As an application of our framework and algorithm, we consider the predictive policing problem, in which the resource being allocated to each group is the number of police officers assigned to each district. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. In this case, the fairness constraint asks that the probability that an individual who has committed a crime is arrested should be independent of the district in which they live. We investigate the performance of our learning algorithm on the Philadelphia Crime Incidents dataset.","['Hadi Elzayn', 'Shahin Jabbari', 'Christopher Jung', 'Michael Kearns', 'Seth Neel', 'Aaron Roth', 'Zachary Schutzman']","['University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Fair Allocation through Competitive Equilibrium from Generic Incomes,"Two food banks catering to populations of different sizes with different needs must divide among themselves a donation of food items. What constitutes a ""fair"" allocation of the items among them? Competitive equilibrium from equal incomes (CEEI) is a classic solution to the problem of fair and efficient allocation of goods among equally entitled agents [Foley 1967, Varian 1974]. Every agent (foodbank) receives an equal endowment of artificial currency with which to ""purchase"" bundles of goods (food items). Prices for the goods are set high enough such that the agents can simultaneously get their favorite within-budget bundle, and low enough such that all goods are allocated (no waste). A CEEI satisfies mathematical notions of fairness like fair-share, and also has built-in transparency -- prices can be published so the agents can verify they're being treated equally. However, a CEEI is not guaranteed to exist when the items are indivisible. We study competitive equilibrium from generic incomes (CEGI), which is based on the idea of slightly perturbed endowments, and enjoys similar fairness, efficiency and transparency properties as CEEI. We show that when the two agents have almost equal endowments and additive preferences for the items, a CEGI always exists. We then consider agents who are a priori non-equal (like different-sized foodbanks); we formulate a new notion of fair allocation among non-equals satisfied by CEGI, and show existence in cases of interest (like when the agents have identical preferences). Experiments on simulated and Spliddit data (a popular fair division website) indicate more general existence. Our results open opportunities for future research on fairness through generic endowments, and on fair treatment of non-equals.","['Moshe Babaioff', 'Noam Nisan', 'Inbal Talgam-Cohen']","['Microsoft Research', 'Hebrew University of Jerusalem', 'Technion']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Fair classification and social welfare,"Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Δ'e perturbations to a fairness parameter 'e in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of ""fairness-to-welfare"" solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring ""more fair"" classifiers does not abide by the Pareto Principle---a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.","['Lily Hu', 'Yiling Chen']","['Harvard University', 'Harvard University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Fair Classification with Group-Dependent Label Noise,"This work examines how to train fair classifiers in settings where training labels are corrupted with random noise, and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup. Heterogeneous label noise models systematic biases towards particular groups when generating annotations. We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures, without accounting for heterogeneous and group-dependent error rates, can decrease both the accuracy and the fairness of the resulting classifier. Our experiments demonstrate these issues arise in practice as well. We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise. We provide both theoretical and empirical justifications for the efficacy of our methods. We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good.","['Jialu Wang', 'Yang Liu', 'Caleb Levy']","['UC Santa Cruz, Santa Cruz, CA, USA', 'UC Santa Cruz, Santa Cruz, CA, USA', 'UC Santa Cruz Santa Cruz, CA, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Fair Clustering via Equitable Group Representations,"What does it mean for a clustering to be fair? One popular approach seeks to ensure that each cluster contains groups in (roughly) the same proportion in which they exist in the population. The normative principle at play is balance: any cluster might act as a representative of the data, and thus should reflect its diversity. But clustering also captures a different form of representativeness. A core principle in most clustering problems is that a cluster center should be representative of the cluster it represents, by being ""close"" to the points associated with it. This is so that we can effectively replace the points by their cluster centers without significant loss in fidelity, and indeed is a common ""use case"" for clustering. For such a clustering to be fair, the centers should ""represent"" different groups equally well. We call such a clustering a group-representative clustering. In this paper, we study the structure and computation of group-representative clusterings. We show that this notion naturally parallels the development of fairness notions in classification, with direct analogs of ideas like demographic parity and equal opportunity. We demonstrate how these notions are distinct from and cannot be captured by balance-based notions of fairness. We present approximation algorithms for group representative k-median clustering and couple this with an empirical evaluation on various real-world data sets. We also extend this idea to facility location, motivated by the current problem of assigning polling locations for voting","['Mohsen Abbasi', 'Aditya Bhaskara', 'Suresh Venkatasubramanian']","['University of Utah', 'University of Utah', 'University of Utah']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Fair decision making using privacy-protected data,"Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a trade-off between protecting privacy and the accuracy of decisions, we initiate a first-of-its-kind study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making. We empirically investigate novel tradeoffs on two real-world decisions made using U.S. Census data (allocation of federal funds and assignment of voting rights benefits) as well as a classic apportionment problem. Our results show that if decisions are made using an ∈-differentially private version of the data, under strict privacy constraints (smaller ∈), the noise added to achieve privacy may disproportionately impact some groups over others. We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities. We also explore improved algorithms to remedy the unfairness observed.","['David Pujol', 'Ryan McKenna', 'Satya Kuppam', 'Michael Hay', 'Ashwin Machanavajjhala', 'Gerome Miklau']","['Duke University', 'University of Massachusetts, Amherst', 'University of Massachusetts, Amherst', 'Colgate University', 'Duke University', 'University of Massachusetts, Amherst']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Fair Regression: Quantitative Definitions and Reduction-Based Algorithms,"In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems fair regression. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness–accuracy frontiers on several standard datasets.","['Alekh Agarwal', 'Miro Dudík', 'Zhiwei Steven Wu']",Microsoft,36th International Conference on Machine Learning,2019-09-01,TRUE
Fairlearn: A toolkit for assessing and improving fairness in AI,"We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.","['Sarah Bird', 'Miro Dudík', 'Richard Edgar', 'Brandon Horn', 'Roman Lutz', 'Vanessa Milan', 'Mehrnoosh Sameki', 'Hanna Wallach', 'Kathleen Walker']",Microsoft,UNKNOWN,2020-05-18,TRUE
Fairness and Abstraction in Sociotechnical Systems,"A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce ""fair"" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five ""traps"" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.","['Andrew D. Selbst', 'Danah Boyd', 'Sorelle A. Friedler', 'Suresh Venkatasubramanian', 'Janet Vertesi']","['Data & Society Research Institute, New York, NY', 'Microsoft Research and Data & Society Research Institute, New York, NY', 'Haverford College, Haverford, PA', 'University of Utah, Salt Lake City, UT', 'Princeton University, Princeton, NJ']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Fairness and utilization in allocating resources with uncertain demand,"Resource allocation problems are a fundamental domain in which to evaluate the fairness properties of algorithms. The trade-offs between fairness and utilization have a long history in this domain. A recent line of work has considered fairness questions for resource allocation when the demands for the resource are distributed across multiple groups and drawn from probability distributions. In such cases, a natural fairness requirement is that individuals from different groups should have (approximately) equal probabilities of receiving the resource. A largely open question in this area has been to bound the gap between the maximum possible utilization of the resource and the maximum possible utilization subject to this fairness condition. Here, we obtain some of the first provable upper bounds on this gap. We obtain an upper bound for arbitrary distributions, as well as much stronger upper bounds for specific families of distributions that are typically used to model levels of demand. In particular, we find --- somewhat surprisingly --- that there are natural families of distributions (including Exponential and Weibull) for which the gap is non-existent: it is possible to simultaneously achieve maximum utilization and the given notion of fairness. Finally, we show that for power-law distributions, there is a non-trivial gap between the solutions, but this gap can be bounded by a constant factor independent of the parameters of the distribution.","['Kate Donahue', 'Jon Kleinberg']","['Cornell University', 'Cornell University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Fairness in Recommendation Ranking through Pairwise Comparisons,"Recommender systems are one of the most pervasive applications of machine learning in industry, with many services using them to match users to products or information.  As such it is important to ask: what are the possible fairness risks, how can we quantify them, and how should we address them?","['Alex Beutel', 'Zhe Zhao', 'Ed H. Chi']",Google,KDD (2019),2019,TRUE
Fairness in Risk Assessment Instruments: Post-Processing to Achieve Counterfactual Equalized Odds,"In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed. The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria. In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.","['Alan Mishler', 'Edward H. Kennedy', 'Alexandra Chouldechova']","['Department of Statistics, Carnegie Mellon University', 'Department of Statistics, Carnegie Mellon University', 'Heinz College Carnegie Mellon University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Fairness Indicators Demo: Scalable Infrastructure for Fair ML Systems,"The rise of machine learning around the globe in fields like medicine, education, employment, credit lending, and criminal sentencing has the potential to reflect and reinforce societal biases at large scale through the models deployed. While fairness concerns are multifaceted, technical evaluations and improvements of models are a critical aspect of a developer's role. And, for these considerations to truly scale, they must integrate into existing processes. In particular, we focus on seamlessly integrating known technical methods with existing libraries used for the training, evaluation, and deployment of models.  To showcase the suite of tools built in Tensorflow, we present an interactive case study demo in conjunction with Conversation AI, an ML research initiative to make online conversations more inclusive.",['Manasi N Joshi'],Google,(2020) (2020),2020,TRUE
Fairness is not static: deeper understanding of long term fairness via simulation studies,"As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets. To address this structural difficulty in the field, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.","[""Alexander D'Amour"", 'Hansa Srinivasan', 'James Atwood', 'Pallavi Baljekar', 'D. Sculley', 'Yoni Halpern']","['Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
"Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives","How should we decide which fairness criteria or
definitions to adopt in machine learning systems?
To answer this question, we must study the fair-
ness preferences of actual users of machine learn-
ing systems. Stringent parity constraints on treat-
ment or impact can come with trade-offs, and
may not even be preferred by the social groups
in question (Zafar et al., 2017). Thus it might
be beneficial to elicit what the groupâs prefer-
ences are, rather than rely on a priori defined
mathematical fairness constraints. Simply asking
for self-reported rankings of users is challenging
because research has shown that there are often
gaps between peopleâs stated and actual prefer-
ences(Bernheim et al., 2013).",['Ben Hutchinson'],Google,Proceedings of ICML 2020 Workshop on Participatory Approaches to Machine Learning,2020,TRUE
Fairness Sample Complexity and the Case for Human Intervention,"With the aim of building machine learning systems that incorporate standards of fairness and accountability, we explore explicit subgroup sample complexity bounds. The work is motivated by the observation that classifier predictions for real world datasets often demonstrate drastically different metrics, such as accuracy, when subdivided by specific sensitive variable subgroups.  The reasons for these discrepancies are varied and not limited to the influence of mitigating variables, institutional bias, underlying population distributions as well as selection bias. Among the numerous definitions of fairness that exist, we argue that at a minimum, principled ML practices should ensure that classification predictions are able to mirror the underlying sub-population distributions as a prelude to bias mitigation, and not amplify discrepancies due to sampling/selection bias. However, as the number of sensitive variables grow, populations meeting at the intersectionality of these variables may simply not exist or be large enough to accurately sample from. In theses increasingly likely scenarios, the case for human intervention and applying situational and individual definitions of fairness should be made..    In this paper we explore, setting Pareto-efficient subgroup sample complexity lower bounds based on the complexity of the ML classifier using VC dimension and Rademacher complexity.  We demonstrate that for a classifier to approach a definition of fairness in terms of specific sensitive variables, adequate subgroup population samples need to exist and the model dimensionality has to be aligned with subgroup population distributions.  In cases where this is not feasible human intervention is explored.  We look at two commonly explored UCI datasets under this lens.",['Alyssa Whitlock Lees'],Google,Where is the Human? Bridging the Gap Between AI and HCI at Chi (2019),2019,TRUE
Fairness through Causal Awareness: Learning Causal Latent-Variable Models for Biased Data,"How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.","['David Madras', 'Elliot Creager', 'Toniann Pitassi', 'Richard Zemel']","['University of Toronto, Vector Institute', 'University of Toronto, Vector Institute', 'University of Toronto, Vector Institute', 'University of Toronto, Vector Institute']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning,"Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness","['Vedant Nanda', 'Samuel Dooley', 'Sahil Singla', 'Soheil Feizi', 'John P. Dickerson']","['University of Maryland MPI-SWS', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Fairness Under Unawareness: Assessing Disparity When Protected Class Is Unobserved,"Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.","['Jiahao Chen', 'Nathan Kallus', 'Xiaojie Mao', 'Geoffry Svacha', 'Madeleine Udell']","['', 'Cornell Tech, New York, New York, USA', 'Cornell Tech, New York, New York, USA', '', 'Cornell University, Ithaca, New York, USA']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Fairness Violations and Mitigation under Covariate Shift,"We study the problem of learning fair prediction models for unseen test sets distributed differently from the train set. Stability against changes in data distribution is an important mandate for responsible deployment of models. The domain adaptation literature addresses this concern, albeit with the notion of stability limited to that of prediction accuracy. We identify sufficient conditions under which stable models, both in terms of prediction accuracy and fairness, can be learned. Using the causal graph describing the data and the anticipated shifts, we specify an approach based on feature selection that exploits conditional independencies in the data to estimate accuracy and fairness metrics for the test set. We show that for specific fairness definitions, the resulting model satisfies a form of worst-case optimality. In context of a healthcare task, we illustrate the advantages of the approach in making more equitable decisions.","['Harvineet Singh', 'Rina Singh', 'Vishwali Mhasawade', 'Rumi Chunara']","['Center for Data Science, New York University, New York City, NY, USA', 'Tandon School of Engineering, New York University, New York City, NY, USA', 'Tandon School of Engineering, New York University, New York City, NY, USA', 'Tandon School of Engineering; School of Global Public Health, New York University, New York City, NY, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Fairness warnings and fair-MAML: learning fairly with minimal data,"Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.","['Dylan Slack', 'Sorelle A. Friedler', 'Emile Givental']","['University of California, Irvine', 'Haverford College', 'Haverford College']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Fairness-Aware Programming,"Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness. We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature, as well as others. As the decision-making program executes, the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated, reporting such violations to the developer. The advantages of this approach are two fold: (i) Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness, as the programmer does not have to write ad hoc code for maintaining statistics. (ii) Compared to existing techniques for checking and ensuring fairness, our approach monitors a decision-making program in the wild, which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested. We describe an implementation of our proposed methodology as a library in the Python programming language and illustrate its use on case studies from the algorithmic fairness literature.","['Aws Albarghouthi', 'Samuel Vinitsky']","['University of Wisconsin-Madison', 'University of Wisconsin-Madison']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
"Fairness, Equality, and Power in Algorithmic Decision-Making","Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same ""merit."" Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by ""merit;"" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.","['Maximilian Kasy', 'Rediet Abebe']","['University of Oxford, Department of Economics', 'University of California, Berkeley, Department of Electrical Engineering & Computer Sciences']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
"Fairness, Welfare, and Equity in Personalized Pricing","We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a ""triple bottom line"": personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.","['Nathan Kallus', 'Angela Zhou']","['Cornell University and Cornell Tech', 'Cornell University and Cornell Tech']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Federated Heavy Hitters with Differential Privacy,"The discovery of heavy hitters (most frequent items) in user-generated data streams drives improvements in the app and web ecosystems, but can incur substantial privacy risks if not done with care. To address these risks, we propose a distributed and privacy-preserving algorithm for discovering the heavy hitters in a population of user-generated data streams. We leverage the sampling property of our distributed algorithm to prove that it is inherently differentially private, without requiring additional noise. We also examine the trade-off between privacy and utility, and show that our algorithm provides excellent utility while also achieving strong privacy guarantees. A significant advantage of this approach is that it eliminates the need to centralize raw data while also avoiding the significant loss in utility incurred by local differential privacy. We validate our findings both theoretically, using worst-case analyses, and practically, using a Twitter dataset with 1.6M tweets and over 650k users. Finally, we carefully compare our approach to Apple's local differential privacy method for discovering heavy hitters.","['Wennan Zhu', 'Peter Kairouz', 'Brendan McMahan']",Google,International Conference on Artificial Intelligence and Statistics (AISTATS) 2020,2020,TRUE
Fifty Shades of Grey: In Praise of a Nuanced Approach Towards Trustworthy Design,"Environmental data science is uniquely placed to respond to essentially complex and fantastically worthy challenges related to arresting planetary destruction. Trust is needed for facilitating collaboration between scientists who may share datasets and algorithms, and for crafting appropriate science-based policies. Achieving this trust is particularly challenging because of the numerous complexities, multi-scale variables, interdependencies and multi-level uncertainties inherent in environmental data science. Virtual Labs---easily accessible online environments provisioning access to datasets, analysis and visualisations---are socio-technical systems which, if carefully designed, might address these challenges and promote trust in a variety of ways. In addition to various system properties that can be utilised in support of effective collaboration, certain features which are commonly seen to benefit trust---transparency and provenance in particular---appear applicable to promoting trust in and through Virtual Labs. Attempting to realise these features in their design reveals, however, that their implementation is more nuanced and complex than it would appear. Using the lens of affordances, we argue for the need to carefully articulate these features, with consideration of multiple stakeholder needs on balance, so that these Virtual Labs do in fact promote trust. We argue that these features not be conceived as widgets that can be imported into a given context to promote trust; rather, whether they promote trust is a function of how systematically designers consider various (potentially conflicting) stakeholder trust needs.","['Lauren Thornton', 'Bran Knowles', 'Gordon Blair']","['Lancaster University, Lancaster, UK', 'Lancaster University, Lancaster, UK', 'Lancaster University, Lancaster, UK']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Flexibly Fair Representation Learning by Disentanglement,"We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also \emph&lbrace;flexibly fair&rbrace;, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder---which does not require the sensitive attributes for inference---allows for the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.",['Kevin Jordan Swersky'],Google,ICML (2019),2019,TRUE
FlipTest: fairness testing via optimal transport,"We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.","['Emily Black', 'Samuel Yeom', 'Matt Fredrikson']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
"Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI","Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.","['Alon Jacovi', 'Ana Marasović', 'Tim Miller', 'Yoav Goldberg']","['Bar Ilan University', 'Allen Institute for Artificial Intelligence, University of Washington', 'School of Computing and Information Systems, The University of Melbourne', 'Bar Ilan University, Allen Institute for Artificial Intelligence']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy,"The word 'ethics' is under siege in technology policy circles. Weaponized in support of deregulation, self-regulation or handsoff governance, ""ethics"" is increasingly identified with technology companies' self-regulatory efforts and with shallow appearances of ethical behavior. So-called ""ethics washing"" by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech community at large. In parallel to the growth of ethics washing, its condemnation has led to a tendency to engage in ""ethics bashing."" This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards, self-governance schemes or stakeholder groups. The misunderstandings underlying ethics bashing are at least threefold: (a) philosophy and ""ethics"" are seen as a communications strategy and as a form of instrumentalized cover-up or façade for unethical behavior, (b) philosophy is understood in opposition and as alternative to political representation and social organizing and (c) the role and importance of moral philosophy is downplayed and portrayed as mere ""ivory tower"" intellectualization of complex problems that need to be dealt with in practice. This paper argues that the rhetoric of ethics and morality should not be reductively instrumentalized, either by the industry in the form of ""ethics washing,"" or by scholars and policy-makers in the form of ""ethics bashing."" Grappling with the role of philosophy and ethics requires moving beyond both tendencies and seeing ethics as a mode of inquiry that facilitates the evaluation of competing tech policy strategies. In other words, we must resist narrow reductivism of moral philosophy as instrumentalized performance and renew our faith in its intrinsic moral value as a mode of knowledgeseeking and inquiry. Far from mandating a self-regulatory scheme or a given governance structure, moral philosophy in fact facilitates the questioning and reconsideration of any given practice, situating it within a complex web of legal, political and economic institutions. Moral philosophy indeed can shed new light on human practices by adding needed perspective, explaining the relationship between technology and other worthy goals, situating technology within the human, the social, the political. It has become urgent to start considering technology ethics also from within and not only from outside of ethics.",['Elettra Bietti'],['Harvard Law School'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
From Fair Decision Making To Social Equality,"The study of fairness in intelligent decision systems has mostly ignored long-term influence on the underlying population. Yet fairness considerations (e.g. affirmative action) have often the implicit goal of achieving balance among groups within the population. The most basic notion of balance is eventual equality between the qualifications of the groups. How can we incorporate influence dynamics in decision making? How well do dynamics-oblivious fairness policies fare in terms of reaching equality? In this paper, we propose a simple yet revealing model that encompasses (1) a selection process where an institution chooses from multiple groups according to their qualifications so as to maximize an institutional utility and (2) dynamics that govern the evolution of the groups' qualifications according to the imposed policies. We focus on demographic parity as the formalism of affirmative action. We first give conditions under which an unconstrained policy reaches equality on its own. In this case, surprisingly, imposing demographic parity may break equality. When it doesn't, one would expect the additional constraint to reduce utility, however, we show that utility may in fact increase. In real world scenarios, unconstrained policies do not lead to equality. In such cases, we show that although imposing demographic parity may remedy it, there is a danger that groups settle at a worse set of qualifications. As a silver lining, we also identify when the constraint not only leads to equality, but also improves all groups. These cases and trade-offs are instrumental in determining when and how imposing demographic parity can be beneficial in selection processes, both for the institution and for society on the long run.","['Hussein Mouzannar', 'Mesrob I. Ohannessian', 'Nathan Srebro']","['American University of Beirut', 'Toyota Technological Institute at Chicago', 'Toyota Technological Institute at Chicago']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
From Optimizing Engagement to Measuring Value,"Most recommendation engines today are based on predicting user engagement, e.g. predicting whether a user will click on an item or not. However, there is potentially a large gap between engagement signals and a desired notion of value that is worth optimizing for. We use the framework of measurement theory to (a) confront the designer with a normative question about what the designer values, (b) provide a general latent variable model approach that can be used to operationalize the target construct and directly optimize for it, and (c) guide the designer in evaluating and revising their operationalization. We implement our approach on the Twitter platform on millions of users. In line with established approaches to assessing the validity of measurements, we perform a qualitative evaluation of how well our model captures a desired notion of ""value"".","['Smitha Milli', 'Luca Belli', 'Moritz Hardt']","['UC Berkeley', 'Twitter', 'UC Berkeley']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
"From Papers to Programs: Courts, Corporations, Clinics and the Battle over Computerized Psychological Testing","This paper examines the role of technology firms in computerizing personality tests from the early 1960s to late 1980s. It focuses on the National Computer Systems (NCS) and their development of an automated interpretation for the Minnesota Multiphasic Personality inventory (MMPI). NCS trumpeted their computerized interpretation as a way to free up clerical labor and mitigate human bias. Yet psychologists cautioned that proprietary algorithms risked obscuring decision rules. I show how clinics, courtrooms, and businesses all had competing interests in the use of computerized personality tests. As I argue, the development of computerized psychological tests was shaped both by business concerns about intellectual property and profits and psychologists' concerns with validity and access to algorithms. Across these domains, the common claim was that computerized psychological testing could provide a technical fix for bias. This paper contributes to histories of computing emphasizing the importance of IP, the relationship between labor, technology, and expertise, and to histories of algorithms.",['Kira Lussier'],['University of Toronto'],"FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
From Soft Classifiers to Hard Decisions: How fair can we be?,"A popular methodology for building binary decision-making classifiers in the presence of imperfect information is to first construct a calibrated non-binary ""scoring"" classifier, and then to post-process this score to obtain a binary decision. We study various fairness (or, error-balance) properties of this methodology, when the non-binary scores are calibrated over all protected groups, and with a variety of post-processing algorithms. Specifically, we show: First, there does not exist a general way to post-process a calibrated classifier to equalize protected groups' positive or negative predictive value (PPV or NPV). For certain ""nice"" calibrated classifiers, either PPV or NPV can be equalized when the post-processor uses different thresholds across protected groups. Still, when the post-processing consists of a single global threshold across all groups, natural fairness properties, such as equalizing PPV in a nontrivial way, do not hold even for ""nice"" classifiers. Second, when the post-processing stage is allowed to defer on some decisions (that is, to avoid making a decision by handing off some examples to a separate process), then for the non-deferred decisions, the resulting classifier can be made to equalize PPV, NPV, false positive rate (FPR) and false negative rate (FNR) across the protected groups. This suggests a way to partially evade the impossibility results of Chouldechova and Kleinberg et al., which preclude equalizing all of these measures simultaneously. We also present different deferring strategies and show how they affect the fairness properties of the overall system. We evaluate our post-processing techniques using the COMPAS data set from 2016.","['Ran Canetti', 'Aloni Cohen', 'Nishanth Dikkala', 'Govind Ramnarayan', 'Sarah Scheffler', 'Adam Smith']","['Boston University and Tel Aviv University', 'MIT', 'MIT', 'MIT', 'Boston University', 'Boston University']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
"Garbage in, garbage out?: do machine learning application papers in social computing report where human-labeled training data comes from?","Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper's authors labeling the data themselves. Such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. In this paper, we investigate to what extent a sample of machine learning application papers in social computing --- specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data --- give specific details about whether such best practices were followed. Our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: Does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. We find a wide divergence in whether such practices were followed and documented. Much of machine learning research and education focuses on what is done once a ""gold standard"" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.","['R. Stuart Geiger', 'Kevin Yu', 'Yanlai Yang', 'Mindy Dai', 'Jie Qiu', 'Rebekah Tang', 'Jenny Huang']","['University of California', 'University of California', 'University of California', 'University of California', 'University of California', 'University of California', 'University of California']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
"Generative Models for Effective ML on Private, Decentralized Datasets","To improve real-world applications of machine learning, experienced practitioners develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data - of representative samples, of outliers, of misclassifications, or alike - is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses, and c) assigning human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the  practitioner may only access aggregated outputs such as metrics or model parameters. This paper outlines a research agenda to address data-oriented tooling needs of ML practitioners who work with privacy-sensitive or decentralized datasets. We demonstrate that generative models - trained using federated methods and with formal differential privacy guarantees - can be used to effectively debug data issues even when the data cannot be directly inspected.","['Sean Augenstein', 'Brendan McMahan', 'Daniel Ramage', 'Swaroop Ramaswamy', 'Peter Kairouz', 'Mingqing Chen', 'Rajiv Mathews', 'Blaise Aguera-Arcas']",Google,Arxiv (2019),2019,TRUE
Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy,"Purpose
Use adjudication to quantify errors in diabetic retinopathy (DR) grading based on individual graders and majority decision, and to train an improved automated algorithm for DR grading.","['Jonathan Krause', 'Varun Gulshan', 'Kasumi Widner', 'Greg Corrado', 'Lily Peng', 'Dale Webster']",Google,Ophthalmology (2018),2018,TRUE
Group Fairness: Independence Revisited,"This paper critically examines arguments against independence, a measure of group fairness also known as statistical parity and as demographic parity. In recent discussions of fairness in computer science, some have maintained that independence is not a suitable measure of group fairness. This position is at least partially based on two influential papers (Dwork et al., 2012, Hardt et al., 2016) that provide arguments against independence. We revisit these arguments, and we find that the case against independence is rather weak. We also give arguments in favor of independence, showing that it plays a distinctive role in considerations of fairness. Finally, we discuss how to balance different fairness considerations.",['Tim Räz'],"['Institute of Philosophy, University of Bern, Switzerland Institute of Biomedical Ethics and History of Medicine, University of Zürich, Switzerland']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Hiding Images Within Images,"We present a system to hide a full color image inside another of the
same size with minimal quality loss to either image.  Deep neural
networks are simultaneously trained to create the hiding and revealing
processes and are designed to specifically work as a pair.  The system
is trained on images drawn randomly from the ImageNet database, and
works well on natural images from a wide variety of sources.  Beyond
demonstrating the successful application of deep learning to hiding
images, we examine how the result is achieved and apply numerous
transformations to analyze if image quality in the host and hidden
image can be maintained.  These transformation range from simple image
manipulations to sophisticated machine learning-based adversaries.
Two extensions to the basic system are presented that mitigate the
possibility of discovering the content of the hidden image.  With
these extensions, not only can the hidden information be kept secure,
but the system can be used to hide even more than a single image.
Applications for this technology include image authentication,
digital watermarks, finding exact regions of image manipulation, and
storing meta-information about image rendering and content.",['Shumeet Baluja'],Google,IEEE Transactions on Pattern Analysis and Machine Intelligence (2019),2019,TRUE
High Dimensional Model Explanations: An Axiomatic Approach,"Complex black-box machine learning models are regularly used in critical decision-making domains. This has given rise to several calls for algorithmic explainability. Many explanation algorithms proposed in literature assign importance to each feature individually. However, such explanations fail to capture the joint effects of sets of features. Indeed, few works so far formally analyze high dimensional model explanations. In this paper, we propose a novel high dimension model explanation method that captures the joint effect of feature subsets. We propose a new axiomatization for a generalization of the Banzhaf index; our method can also be thought of as an approximation of a black-box model by a higher-order polynomial. In other words, this work justifies the use of the generalized Banzhaf index as a model explanation by showing that it uniquely satisfies a set of natural desiderata and that it is the optimal local approximation of a black-box model. Our empirical evaluation of our measure highlights how it manages to capture desirable behavior, whereas other measures that do not satisfy our axioms behave in an unpredictable manner.","['Neel Patel', 'Martin Strobel', 'Yair Zick']","['University of Southern California', 'National University of Singapore', 'University of Massachusetts, Amherst']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
How can I choose an explainer?: An Application-grounded Evaluation of Post-hoc Explanations,"There have been several research works proposing new Explainable AI (XAI) methods designed to generate model explanations having specific properties, or desiderata, such as fidelity, robustness, or human-interpretability. However, explanations are seldom evaluated based on their true practical impact on decision-making tasks. Without that assessment, explanations might be chosen that, in fact, hurt the overall performance of the combined system of ML model + end-users. This study aims to bridge this gap by proposing XAI Test, an application-grounded evaluation methodology tailored to isolate the impact of providing the end-user with different levels of information. We conducted an experiment following XAI Test to evaluate three popular XAI methods - LIME, SHAP, and TreeInterpreter - on a real-world fraud detection task, with real data, a deployed ML model, and fraud analysts. During the experiment, we gradually increased the information provided to the fraud analysts in three stages: Data Only, i.e., just transaction data without access to model score nor explanations, Data + ML Model Score, and Data + ML Model Score + Explanations. Using strong statistical analysis, we show that, in general, these popular explainers have a worse impact than desired. Some of the conclusion highlights include: i) showing Data Only results in the highest decision accuracy and the slowest decision time among all variants tested, ii) all the explainers improve accuracy over the Data + ML Model Score variant but still result in lower accuracy when compared with Data Only; iii) LIME was the least preferred by users, probably due to its substantially lower variability of explanations from case to case.","['Sérgio Jesus', 'Catarina Belém', 'Vladimir Balayan', 'João Bento', 'Pedro Saleiro', 'Pedro Bizarro', 'João Gama']","['Feedzai, DCC-FCUP, Universidade do Porto', 'Feedzai', 'Feedzai', 'Feedzai', 'Feedzai', 'Feedzai', 'LIAAD, INESCTEC, Universidade do Porto']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Human-Centered Tools for Coping with Imperfect Algorithms during Medical Decision-Making,"Machine learning (ML) is increasingly being used in image retrieval systems for medical decision making. One application of ML is to retrieve visually similar medical images from past patients (e.g. tissue from biopsies) to reference when making a medical decision with a new patient. However, no algorithm can perfectly capture an expert's ideal notion of similarity for every case: an image that is algorithmically determined to be similar may not be medically relevant to a doctor's specific diagnostic needs. In this paper, we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm, and developed tools that empower users to cope with the search algorithm on-the-fly, communicating what types of similarity are most important at different moments in time. In two evaluations with pathologists, we found that these refinement tools increased the diagnostic utility of images found and increased user trust in the algorithm. The tools were preferred over a traditional interface, without a loss in diagnostic accuracy. We also observed that users adopted new strategies when using refinement tools, re-purposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors. Taken together, these findings inform future human-ML collaborative systems for expert decision-making.","['Carrie Jun Cai', 'Emily Reif', 'Narayan G Hegde', 'Been Kim', 'Daniel Smilkov', 'Martin Wattenberg', 'Fernanda Viégas', 'Greg Corrado']",Google,Conference on Human Factors in Computing Systems (2019),2019,TRUE
Human-in-the-Loop Interpretability Prior,"We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.",['Been Kim'],Google,NeurIPS (Spotlight) (2018),2018,TRUE
"I agree with the decision, but they didn't deserve this: Future Developers' Perception of Fairness in Algorithmic Decisions","While professionals are increasingly relying on algorithmic systems for making a decision, on some occasions, algorithmic decisions may be perceived as biased or not just. Prior work has looked into the perception of algorithmic decision-making from the user's point of view. In this work, we investigate how students in fields adjacent to algorithm development perceive algorithmic decisionmaking. Participants (N=99) were asked to rate their agreement with statements regarding six constructs that are related to facets of fairness and justice in algorithmic decision-making in three separate scenarios. Two of the three scenarios were independent of each other, while the third scenario presented three different outcomes of the same algorithmic system, demonstrating perception changes triggered by different outputs. Quantitative analysis indicates that a) 'agreeing' with a decision does not mean the person 'deserves the outcome', b) perceiving the factors used in the decision-making as 'appropriate' does not make the decision of the system 'fair' and c) perceiving a system's decision as 'not fair' is affecting the participants' 'trust' in the system. In addition, participants found proportional distribution of benefits more fair than other approaches. Qualitative analysis provides further insights into that information the participants find essential to judge and understand an algorithmic decision-making system's fairness. Finally, the level of academic education has a role to play in the perception of fairness and justice in algorithmic decision-making.","['Maria Kasinidou', 'Styliani Kleanthous', 'Pınar Barlas', 'Jahna Otterbacher']","['Cyprus Center for Algorithmic Transparency, Open University of Cyprus, Nicosia, Cyprus', 'Cyprus Center for Algorithmic Transparency, Open University of Cyprus Nicosia, Cyprus', 'Research Centre on Interactive Media, Smart Systems and Emerging Technologies, Nicosia, Cyprus', 'Cyprus Center for Algorithmic Transparency, Open University of Cyprus, Nicosia, Cyprus']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Identifying and Correcting Label Bias in Machine Learning,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained
on such datasets can inherit these biases. In this
paper, we provide a mathematical formulation of
how this bias can arise. We do so by assuming
the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who
intends to provide accurate labels but may have
biases against certain groups. Despite the fact that
we only observe the biased labels, we are able to
show that the bias may nevertheless be corrected
by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset
corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine
learning classifier. Our procedure is fast and robust and can be used with virtually any learning
algorithm. We evaluate on a number of standard
machine learning fairness datasets and a variety
of fairness notions, finding that our method outperforms standard approaches in achieving fair
classification.",['Ofir Nachum'],Google,arxiv (2019),2019,TRUE
Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases,"Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition. But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects? We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images. We find that state-of-the-art unsupervised models trained on ImageNet, a popular benchmark image dataset curated from internet images, automatically learn racial, gender, and intersectional biases. We replicate 8 previously documented human biases from social psychology, from the innocuous, as with insects and flowers, to the potentially harmful, as with race and gender. Our results closely match three hypotheses about intersectional bias from social psychology. For the first time in unsupervised computer vision, we also quantify implicit human biases about weight, disabilities, and several ethnicities. When compared with statistical patterns in online image datasets, our findings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web.","['Ryan Steed', 'Aylin Caliskan']","['Carnegie Mellon University, Pittsburgh, Pennsylvania, USA', 'George Washington University, Washington, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
"Implications of AI (un-)fairness in higher education admissions: the effects of perceived AI (un-)fairness on exit, voice and organizational reputation","Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.","['Frank Marcinkowski', 'Kimon Kieslich', 'Christopher Starke', 'Marco Lünich']","['University of Düsseldorf, Germany', 'University of Düsseldorf, Germany', 'University of Düsseldorf, Germany', 'University of Düsseldorf, Germany']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Impossible Explanations?: Beyond explainable AI in the GDPR from a COVID-19 use case scenario,"Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI. We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR). Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment. Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.","['Ronan Hamon', 'Henrik Junklewitz', 'Gianclaudio Malgieri', 'Paul De Hert', 'Laurent Beslay', 'Ignacio Sanchez']","['European Commission, Joint Research Centre, Ispra, Italy', 'European Commission, Joint Research Centre, Ispra, Italy', 'Augmented Law Institute, EDHEC Business School, Lille, France', 'Law Science Technology & Society, Vrije Universiteit Brussel, Brussels, Belgium', 'European Commission, Joint Research Centre, Ispra, Italy', 'European Commission, Joint Research Centre, Ispra, Italy']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Improving fairness in machine learning systems: What do industry practitioners need?,"The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams’ challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by industry practitioners and solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address industry practitioners’ needs.","['Ken Holstein', 'Jennifer Wortman Vaughan', 'Hal Daumé III', 'Miro Dudík', 'Hanna Wallach']",Microsoft,2019 ACM CHI Conference on Human Factors in Computing Systems,2019-06-01,TRUE
Integrating FATE/critical data studies into data science curricula: where are we going and how do we get there?,"There have been multiple calls for integrating topics related to fairness, accountability, transparency, ethics (FATE) and social justice into Data Science curricula, but little exploration of how this might work in practice. This paper presents the findings of a collaborative auto-ethnography (CAE) engaged in by a MSc Data Science teaching team based at University of Sheffield (UK) Information School where FATE/Critical Data Studies (CDS) topics have been a core part of the curriculum since 2015/16. In this paper, we adopt the CAE approach to reflect on our experiences of working at the intersection of disciplines, and our progress and future plans for integrating FATE/CDS into the curriculum. We identify a series of challenges for deeper FATE/CDS integration related to our own competencies and the wider socio-material context of Higher Education in the UK. We conclude with recommendations for ourselves and the wider FATE/CDS orientated Data Science community.","['Jo Bates', 'David Cameron', 'Alessandro Checco', 'Paul Clough', 'Frank Hopfgartner', 'Suvodeep Mazumdar', 'Laura Sbaffi', 'Peter Stordy', 'Antonio de la Vega de León']","['University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK and Peak Indicators, Chesterfield, UK', 'University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK', 'University of Sheffield, Sheffield, UK']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV),"The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of âzebraâ is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.","['Been Kim', 'Martin Wattenberg', 'Carrie Jun Cai', 'James Wexler', 'Fernanda Viegas', 'Rory Abbott Sayres']",Google,ICML (2018),2018,TRUE
Interpreting Social Respect:  A Normative Lens for ML Models,"Machine learning is often viewed as an inherently value-neutral process:
statistical tendencies in the training inputs are simply''
used to generalize to new examples. However when models impact social
systems such as interactions between humans, these patterns learned by models
have normative implications. It is important that we ask not onlywhat
patterns exist in the data?'', but also ``how do we want our system 
to impact people?'' In particular, because minority and marginalized
members of society are often statistically underrepresented in data sets, models
may have undesirable disparate impact on such groups. As such, objectives of
social equity and distributive justice require that we develop tools for both
identifying and interpreting harms introduced by models.",['Ben Hutchinson'],Google,(2019) (2019),2019,TRUE
Interventions for ranking in the presence of implicit bias,"Implicit bias is the unconscious attribution of particular qualities (or lack thereof) to a member from a particular social group (e.g., defined by gender or race). Studies on implicit bias have shown that these unconscious stereotypes can have adverse outcomes in various social contexts, such as job screening, teaching, or policing. Recently, [34] considered a mathematical model for implicit bias and showed the effectiveness of the Rooney Rule as a constraint to improve the utility of the outcome for certain cases of the subset selection problem. Here we study the problem of designing interventions for the generalization of subset selection - ranking - that requires to output an ordered set and is a central primitive in various social and computational contexts. We present a family of simple and interpretable constraints and show that they can optimally mitigate implicit bias for a generalization of the model studied in [34]. Subsequently, we prove that under natural distributional assumptions on the utilities of items, simple, Rooney Rule-like, constraints can also surprisingly recover almost all the utility lost due to implicit biases. Finally, we augment our theoretical results with empirical findings on real-world distributions from the IIT-JEE (2009) dataset and the Semantic Scholar Research corpus.","['L. Elisa Celis', 'Anay Mehrotra', 'Nisheeth K. Vishnoi']","['Yale University', 'IIT Kanpur', 'Yale University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Intriguing Properties of Adversarial Examples,"It is becoming increasingly clear that many machine learning classifiers are vulnerable
to adversarial examples. In attempting to explain the origin of adversarial
examples, previous studies have typically focused on the fact that neural networks
operate on high dimensional data, they overfit, or they are too linear. Here we
argue that the origin of adversarial examples is primarily due to an inherent uncertainty
that neural networks have about their predictions. We show that the functional
form of this uncertainty is independent of architecture, dataset, and training
protocol; and depends only on the statistics of the logit differences of the network,
which do not change significantly during training. This leads to adversarial error
having a universal scaling, as a power-law, with respect to the size of the adversarial
perturbation. We show that this universality holds for a broad range of datasets
(MNIST, CIFAR10, ImageNet, and random data), models (including state-of-theart
deep networks, linear models, adversarially trained networks, and networks
trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated
by these results, we study the effects of reducing prediction entropy on
adversarial robustness. Finally, we study the effect of network architectures on
adversarial sensitivity. To do this, we use neural architecture search with reinforcement
learning to find adversarially robust architectures on CIFAR10. Our
resulting architecture is more robust to white and black box attacks compared to
previous attempts.","['Ekin Dogus Cubuk', 'Quoc V. Le']",Google,ICLR (2018),2018,TRUE
Investigating the Effects of Gender Bias on GitHub,"Diversity, including gender diversity, is valued by many software development organizations, yet the field remains dominated by men. One reason for this lack of diversity is gender bias. In this paper, we study the effects of that bias by using an existing framework derived from the gender studies literature. We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub, then evaluate those hypotheses quantitatively. While our results show that effects of gender bias are largely invisible on the GitHub platform itself, there are still signals of women concentrating their work in fewer places and being more restrained in communication than men.",['Emerson Murphy-Hill'],Google,Proceedings of the 2019 International Conference on Software Engineering,2019,TRUE
Language (Technology) is Power: A Critical Survey of “Bias” in NLP,"We survey 146 papers analyzing “bias” in NLP systems, 
finding that their motivations are often vague, inconsistent, and 
lacking in normative reasoning, despite the fact that analyzing “bias” 
is an inherently normative process. We further find that these papers’ 
proposed quantitative techniques for measuring or mitigating “bias” are 
poorly matched to their motivations and do not engage with the relevant 
literature outside of NLP. Based on these findings, we describe the 
beginnings of a path forward by proposing three recommendations that 
should guide work analyzing “bias” in NLP systems. These recommendations
 rest on a greater recognition of the relationships between language and
 social hierarchies, encouraging researchers and practitioners to 
articulate their conceptualizations of “bias”—i.e., what kinds of system
 behaviors are harmful, in what ways, to whom, and why, as well as the 
normative reasoning underlying these statements—and to center work 
around the lived experiences of members of communities affected by NLP 
systems, while interrogating and reimagining the power relations between
 technologists and such communities.","['Su Lin Blodgett', 'Solon Barocas', 'Hal Daumé III', 'Hanna Wallach']",Microsoft,ACL,2020-06-01,TRUE
Learning Differentially Private Recurrent Language Models,"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes ""large step"" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","['Brendan McMahan', 'Daniel Ramage', 'Li Zhang']",Google,International Conference on Learning Representations (ICLR) (2018),2018,TRUE
Learning how to explain neural networks: PatternNet and PatternAttribution,"DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of  simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.","['Pieter-jan Kindermans', 'Dumitru Erhan', 'Been Kim']",Google,ICLR (2018),2018,TRUE
Learning to Attack: Adversarial Transformation Networks,"With the rapidly increasing popularity of deep neural networks
for image recognition tasks, a parallel interest in generating
adversarial examples to attack the trained models has
arisen. To date, these approaches have involved either directly
computing gradients with respect to the image pixels or directly
solving an optimization on the image pixels. We generalize
this pursuit in a novel direction: can a separate network
be trained to efficiently attack another fully trained network?
We demonstrate that it is possible, and that the generated
attacks yield startling insights into the weaknesses of
the target network. We call such a network an Adversarial
Transformation Network (ATN). ATNs transform any input
into an adversarial attack on the target network, while being
minimally perturbing to the original inputs and the target networkâs
outputs. Further, we show that ATNs are capable of
not only causing the target network to make an error, but can
be constructed to explicitly control the type of misclassification
made. We demonstrate ATNs on both simple MNIST digit
classifiers and state-of-the-art ImageNet classifiers deployed
by Google, Inc.: Inception ResNet-v2.","['Shumeet Baluja', 'Ian Fischer']",Google,Proceedings of AAAI-2018,2018,TRUE
Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning,"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states.","['Shane Gu', 'Julian Ibarz', 'Sergey Levine']",Google,ICLR (2018),2018,TRUE
Leave-one-out Unfairness,"We introduce leave-one-out unfairness, which characterizes how likely a model's prediction for an individual will change due to the inclusion or removal of a single other person in the model's training data. Leave-one-out unfairness appeals to the idea that fair decisions are not arbitrary: they should not be based on the chance event of any one person's inclusion in the training data. Leave-one-out unfairness is closely related to algorithmic stability, but it focuses on the consistency of an individual point's prediction outcome over unit changes to the training data, rather than the error of the model in aggregate. Beyond formalizing leave-one-out unfairness, we characterize the extent to which deep models behave leave-one-out unfairly on real data, including in cases where the generalization error is small. Further, we demonstrate that adversarial training and randomized smoothing techniques have opposite effects on leave-one-out fairness, which sheds light on the relationships between robustness, memorization, individual fairness, and leave-one-out fairness in deep models. Finally, we discuss salient practical applications that may be negatively affected by leave-one-out unfairness.","['Emily Black', 'Matt Fredrikson']","['Carnegie Mellon University', 'Carnegie Mellon University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Lessons from archives: strategies for collecting sociocultural data in machine learning,"A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics & privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.","['Eun Seo Jo', 'Timnit Gebru']","['Stanford University', 'Google']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy,"Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID-19 response strategies such as the targeting of public health resources. Yet little attention has been paid to measurement validity and demographic bias, due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics. We illustrate how linking large-scale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels. More precisely, we show that linking voter roll data---containing individual-level voter turnout for specific voting locations along with race and age---can facilitate the construction of rigorous bias and reliability tests. Using data from North Carolina's 2018 general election, these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context: older and non-white voters are less likely to be captured by mobility data. We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups.","['Amanda Coston', 'Neel Guha', 'Derek Ouyang', 'Lisa Lu', 'Alexandra Chouldechova', 'Daniel E. Ho']","['Carnegie Mellon University', 'Stanford University', 'Stanford University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Manipulating and Measuring Model Interpretability,"Despite a growing literature on creating interpretable machine learning methods, there have been few experimental studies of their effects on end users. We present a series of large-scale, randomized, pre-registered experiments in which participants were shown functionally identical models that varied only in two factors thought to influence interpretability: the number of input features and the model transparency (clear or black-box). Participants who were shown a clear model with a small number of features were better able to simulate the model’s predictions. However, contrary to what one might expect when manipulating interpretability, we found no significant difference in multiple measures of trust across conditions. Even more surprisingly, increased transparency hampered people’s ability to detect when a model has made a sizeable mistake. These findings emphasize the importance of studying how models are presented to people and empirically verifying that interpretable models achieve their intended effects on end users.","['Forough Poursabzi-Sangdeh', 'Dan Goldstein', 'Jake Hofman', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,CHI 2021,2021-05-20,TRUE
"Matroids, Matchings, and Fairness","The desire to use machine learning to assist in human decision making has spawned a large area of research in understanding the impact of such systems not only on the society as a whole, but also the specific impact on different subpopulations. Recent work has shown that  while there are several natural ways to quantify the fairness of a particular system, none of them are universal, and except for trivial cases, satisfying one means violating another~\citet&lbrace;Kleinberg, Goel, Kleinberg2&rbrace;.  ","['Ravi Kumar', 'Silvio Lattanzi', 'Sergei Vassilvitskii']",Google,AISTATS 2019 (2019),2019,TRUE
Measurement and Fairness,"We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.","['Abigail Z. Jacobs', 'Hanna Wallach']","['University of Michigan', 'Microsoft Research']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Measuring and Mitigating Unintended Bias in Text Classification,"We introduce and illustrate a new approach to measuring and
mitigating unintended bias in machine learning models. Our
definition of unintended bias is parameterized by a test set
and a subset of input features. We illustrate how this can
be used to evaluate text classifiers using a synthetic test set
and a public corpus of comments annotated for toxicity from
Wikipedia Talk pages. We also demonstrate how imbalances
in training data can lead to unintended bias in the resulting
models, and therefore potentially unfair applications. We use
a set of common demographic identity terms as the subset of
input features on which we measure bias. This technique permits
analysis in the common scenario where demographic information
on authors and readers is unavailable, so that bias
mitigation must focus on the content of the text itself. The
mitigation method we introduce is an unsupervised approach
based on balancing the training dataset. We demonstrate that
this approach reduces the unintended bias without compromising
overall model quality","['Lucas Dixon', 'Jeffrey Sorensen', 'Nithum Thain', 'Lucy Vasserman']",Google,"AAAI/ACM Conference on AI, Ethics, and Society (2018)",2018,TRUE
Measuring justice in machine learning,"How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?",['Alan Lundgard'],"['Massachusetts Institute of Technology Cambridge, Massachusetts']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Measuring the Biases that Matter: The Ethical and Casual Foundations for Measures of Fairness in Algorithms,"Measures of algorithmic bias can be roughly classified into four categories, distinguished by the conditional probabilistic dependencies to which they are sensitive. First, measures of ""procedural bias"" diagnose bias when the score returned by an algorithm is probabilistically dependent on a sensitive class variable (e.g. race or sex). Second, measures of ""outcome bias"" capture probabilistic dependence between class variables and the outcome for each subject (e.g. parole granted or loan denied). Third, measures of ""behavior-relative error bias"" capture probabilistic dependence between class variables and the algorithmic score, conditional on target behaviors (e.g. recidivism or loan default). Fourth, measures of ""score-relative error bias"" capture probabilistic dependence between class variables and behavior, conditional on score. Several recent discussions have demonstrated a tradeoff between these different measures of algorithmic bias, and at least one recent paper has suggested conditions under which tradeoffs may be minimized. In this paper we use the machinery of causal graphical models to show that, under standard assumptions, the underlying causal relations among variables forces some tradeoffs. We delineate a number of normative considerations that are encoded in different measures of bias, with reference to the philosophical literature on the wrongfulness of disparate treatment and disparate impact. While both kinds of error bias are nominally motivated by concern to avoid disparate impact, we argue that consideration of causal structures shows that these measures are better understood as complicated and unreliable measures of procedural biases (i.e. disparate treatment). Moreover, while procedural bias is indicative of disparate treatment, we show that the measure of procedural bias one ought to adopt is dependent on the account of the wrongfulness of disparate treatment one endorses. Finally, given that neither score-relative nor behavior-relative measures of error bias capture the relevant normative considerations, we suggest that error bias proper is best measured by score-based measures of accuracy, such as the Brier score.","['Bruce Glymour', 'Jonathan Herington']","['Department of Philosophy, Kansas State University, Manhattan, KS, USA', 'Department of Philosophy, Kansas State University, Manhattan, KS, USA']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Metric-Optimized Example Weights,"Real-world machine learning applications often have complex test metrics, and may have training and test data that are not identically distributed. Motivated by known connections between complex test metrics and cost-weighted learning, we propose addressing these issues by using a weighted loss function with a standard loss, where the weights on the training examples are learned to optimize the test metric on a validation set. These metric-optimized example weights can be learned for any test metric, including black box and customized ones for specific applications. We illustrate the performance of the proposed method on diverse public benchmark datasets and real-world applications. We also provide a generalization bound for the method.","['Sen Zhao', 'Harikrishna Narasimhan']",Google,Proceedings of the 36th International Conference on Machine Learning (2019),2019,TRUE
Mitigating bias in algorithmic hiring: evaluating claims and practices,"There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.","['Manish Raghavan', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy']","['Cornell University', 'Microsoft Research and Cornell University', 'Cornell University', 'Cornell University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Mitigating Bias in Set Selection with Noisy Protected Attributes,"Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result! Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a ""denoised"" selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness.","['Anay Mehrotra', 'L. Elisa Celis']","['Yale University', 'Yale University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Mitigating Unwanted Biases with Adversarial Learning,"Machine learning can be used to train a model that accurately represents
the data on which it is trained. The most common loss functions
minimized by gradient descent involve accuracy. However,
modeling the training data optimally requires accurately modeling
any undesirable biases present in that training data. One task
which easily demonstrates this phenomenon is word embeddings
learned from standard corpora. When such word embeddings are
used to perform tasks like analogy completion, the bias in the word
embeddings propagates to the predicted analogy completions. Ideally
we would like to remove the biased information which might
impact task performance while retaining as much other semantic
information as possible. We present here a method for debiasing
networks using an adversary. First we formalize this problem by
describing the nature of the input to our network X, describing
the prediction which is desired Y and the protected variable Z. The
objective then becomes to maximize the primary networkâs ability
to predict Y while minimizing the adversaryâs ability to use that
prediction to predict Z. When applied to analogy completion this
method results in embeddings which are still quite useful for performing
analogy completion but without producing predictions
impacted by bias prediction. When applied to a categorization task
such as the one in the UCI Adult Dataset it results in a predictive
model that maintains accuracy while ensuring equality of odds.
This method is quite flexible and is applicable to any problem set
which is expressible as a model which predicts a label Y using an
input X while trying to be fair with respect to a protected variable
Z.",['Blake Lemoine'],Google,(2018) (2018),2018,TRUE
Model agnostic interpretability of rankers via intent modelling,"A key problem in information retrieval is understanding the latent intention of a user's under-specified query. Retrieval models that are able to correctly uncover the query intent often perform well on the document ranking task. In this paper we study the problem of interpretability for text based ranking models by trying to unearth the query intent as understood by complex retrieval models. We propose a model-agnostic approach that attempts to locally approximate a complex ranker by using a simple ranking model in the term space. Given a query and a blackbox ranking model, we propose an approach that systematically exploits preference pairs extracted from the target ranking and document perturbations to identify a set of intent terms and a simple term based ranker that can faithfully and accurately mimic the complex blackbox ranker in that locality. Our results indicate that we can indeed interpret more complex models with high fidelity. We also present a case study on how our approach can be used to interpret recently proposed neural rankers.","['Jaspreet Singh', 'Avishek Anand']","['L3S Research Center, Hannover, Germany', 'L3S Research Center, Hannover, Germany']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Model Cards for Model Reporting,"Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.","['Margaret Mitchell', 'Simone Wu', 'Andrew Zaldivar', 'Parker Barnes', 'Lucy Vasserman', 'Ben Hutchinson', 'Elena Spitzer', 'Inioluwa Deborah Raji', 'Timnit Gebru']","['Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'University of Toronto', 'Google']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Model Reconstruction from Model Explanations,"We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations. On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive. Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.","['Smitha Milli', 'Ludwig Schmidt', 'Anca D. Dragan', 'Moritz Hardt']","['University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Motivating the Rules of the Game for Adversarial Example Research,"Advances in machine learning have led to broad deployment of systems with impressive
performance on important problems. Nonetheless, these systems can be induced
to make errors on data that are surprisingly similar to examples the learned system
handles correctly. The existence of these errors raises a variety of questions about
out-of-sample generalization and whether bad actors might use such examples to abuse
deployed systems. As a result of these security concerns, there has been a flurry of
recent papers proposing algorithms to defend against such malicious perturbations of
correctly handled examples. It is unclear how such misclassifications represent a different
kind of security problem than other errors, or even other attacker-produced
examples that have no specific relationship to an uncorrupted input. In this paper,
we argue that adversarial example defense papers have, to date, mostly considered
abstract, toy games that do not relate to any specific security concern. Furthermore,
defense papers have not yet precisely described all the abilities and limitations of attackers
that would be relevant in practical security. Towards this end, we establish a
taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally,
we provide a series of recommendations outlining a path forward for future work
to more clearly articulate the threat model and perform more meaningful evaluation.",['George E. Dahl'],Google,arxiv (2018),2018,TRUE
Multi-category fairness in sponsored search auctions,"Fairness in advertising is a topic of particular concern motivated by theoretical and empirical observations in both the computer science and economics literature. We examine the problem of fairness in advertising for general purpose platforms that service advertisers from many different categories. First, we propose inter-category and intra-category fairness desiderata that take inspiration from individual fairness and envy-freeness. Second, we investigate the ""platform utility"" (a proxy for the quality of allocation) achievable by mechanisms satisfying these desiderata. More specifically, we compare the utility of fair mechanisms against the unfair optimum, and show by construction that our fairness desiderata are compatible with utility. Our mechanisms also enjoy nice implementation properties including metric-obliviousness, which allows the platform to produce fair allocations without needing to know the specifics of the fairness requirements.","['Christina Ilvento', 'Meena Jagadeesan', 'Shuchi Chawla']","['Harvard University', 'Harvard University', 'University of Wisconsin-Madison']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Multi-layered explanations from algorithmic impact assessments in the GDPR,"Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability. The aim of this paper is to address how Data Protection Impact Assessments (DPIAs) (Art. 35) in the European Union (EU)'s General Data Protection Regulation (GDPR) link the GDPR's two approaches to algorithmic accountability---individual rights and systemic governance--- and potentially lead to more accountable and explainable algorithms. We argue that algorithmic explanation should not be understood as a static statement, but as a circular and multi-layered transparency process based on several layers (general information about an algorithm, group-based explanations, and legal justification of individual decisions taken). We argue that the impact assessment process plays a crucial role in connecting internal company heuristics and risk mitigation to outward-facing rights, and in forming the substance of several kinds of explanations.","['Margot E. Kaminski', 'Gianclaudio Malgieri']","['University of Colorado', 'Vrije Universiteit Brussels, Brussels, Belgium']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Multi-Task Learning for Personal Search Ranking with Query Clustering,"User needs vary significantly across different tasks, and therefore
their queries will also vary significantly in their expressiveness
and semantics. Many studies have been proposed
to model such query diversity by obtaining query types and
building query-dependent ranking models. To obtain query
types, these studies typically require either a labeled query
dataset or clicks from multiple users aggregated over the
same document. These techniques, however, are not applicable
when manual query labeling is not viable, and aggregated
clicks are unavailable due to the private nature of the document
collection, e.g., in personal search scenarios. Therefore,
in this paper, we study the problem of how to obtain query
type in an unsupervised fashion and how to leverage this information
using query-dependent ranking models in personal
search. We first develop a hierarchical clustering algorithm
based on truncated SVD and varimax rotation to obtain
coarse-to-fine query types. Then, we propose three query-dependent
ranking models, including two neural models that
leverage query type information as additional features, and
one novel multi-task neural model that is trained to simultaneously
rank documents and predict query types. We evaluate
our ranking models using the click data collected from one of
the worldâs largest personal search engines. The experiments
demonstrate that the proposed multi-task model can significantly
outperform the baseline neural models, which either
do not incorporate query type information or just simply
feed query type as an additional feature. To the best of our
knowledge, this is the first successful application of query-dependent
multi-task learning in personal search ranking.","['Maryam Karimzadehgan', 'Michael Bendersky', 'Zhen Qin', 'Don Metzler']",Google,Proceedings of ACM Conference on Information and Knowledge Management (CIKM) (2018),2018,TRUE
Narratives and Counternarratives on Data Sharing in Africa,"As machine learning and data science applications grow ever more prevalent, there is an increased focus on data sharing and open data initiatives, particularly in the context of the African continent. Many argue that data sharing can support research and policy design to alleviate poverty, inequality, and derivative effects in Africa. Despite the fact that the datasets in question are often extracted from African communities, conversations around the challenges of accessing and sharing African data are too often driven by non-African stakeholders. These perspectives frequently employ a deficit narratives, often focusing on lack of education, training, and technological resources in the continent as the leading causes of friction in the data ecosystem. We argue that these narratives obfuscate and distort the full complexity of the African data sharing landscape. In particular, we use storytelling via fictional personas built from a series of interviews with African data experts to complicate dominant narratives and to provide counternarratives. Coupling these personas with research on data practices within the continent, we identify recurring barriers to data sharing as well as inequities in the distribution of data sharing benefits. In particular, we discuss issues arising from power imbalances resulting from the legacies of colonialism, ethno-centrism, and slavery, disinvestment in building trust, lack of acknowledgement of historical and present-day extractive practices, and Western-centric policies that are ill-suited to the African context. After outlining these problems, we discuss avenues for addressing them when sharing data generated in the continent.","['Rediet Abebe', 'Kehinde Aruleba', 'Abeba Birhane', 'Sara Kingsley', 'George Obaido', 'Sekou L. Remy', 'Swathi Sadagopan']","['University of California, Berkeley', 'University of the Witwatersrand', 'University College Dublin & Lero', 'Carnegie Mellon University', 'University of the Witwatersrand', 'IBM Research - Africa', 'Deloitte']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
On Completeness-aware Concept-Based Explanations in Deep Neural Networks,"Concept-based explanations can be a key direction to understand how DNNs make decisions. In this paper, we study concept-based explainability in a systematic framework. First, we define the notion of completeness, which quantifies how sufficient a particular set of concepts is in explaining the model's behavior. Based on performance and variability motivations, we propose two definitions to quantify completeness. We show that they yield the commonly-used PCA method under certain assumptions. Next, we study two additional constraints to ensure the interpretability of discovered concept, based on sparsity principles. Through systematic experiments, on specifically-designed synthetic dataset and real-world text and image datasets, we demonstrate the superiority of our framework in finding concepts that are complete (in explaining the decision) and that are interpretable.","['Been Kim', 'Sercan Arik', 'Chun-Liang Li', 'Tomas Pfister']",Google,NeurIPS (2020),2020,TRUE
On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection,"Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency. In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels (>20% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.","['Vivian Lai', 'Chenhao Tan']","['University of Colorado Boulder', 'University of Colorado Boulder']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
On Microtargeting Socially Divisive Ads: A Case Study of Russia-Linked Ad Campaigns on Facebook,"Targeted advertising is meant to improve the efficiency of matching advertisers to their customers. However, targeted advertising can also be abused by malicious advertisers to efficiently reach people susceptible to false stories, stoke grievances, and incite social conflict. Since targeted ads are not seen by non-targeted and non-vulnerable people, malicious ads are likely to go unreported and their effects undetected. This work examines a specific case of malicious advertising, exploring the extent to which political ads1 from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S. elections exploited Facebook's targeted advertising infrastructure to efficiently target ads on divisive or polarizing topics (e.g., immigration, race-based policing) at vulnerable sub-populations. In particular, we do the following: (a) We conduct U.S. census-representative surveys to characterize how users with different political ideologies report, approve, and perceive truth in the content of the IRA ads. Our surveys show that many ads are ""divisive"": they elicit very different reactions from people belonging to different socially salient groups. (b) We characterize how these divisive ads are targeted to sub-populations that feel particularly aggrieved by the status quo. Our findings support existing calls for greater transparency of content and targeting of political ads. (c) We particularly focus on how the Facebook ad API facilitates such targeting. We show how the enormous amount of personal data Facebook aggregates about users and makes available to advertisers enables such malicious targeting.","['Filipe N. Ribeiro', 'Koustuv Saha', 'Mahmoudreza Babaei', 'Lucas Henrique', 'Johnnatan Messias', 'Fabricio Benevenuto', 'Oana Goga', 'Krishna P. Gummadi', 'Elissa M. Redmiles']","['UFOP/UFMG, Brazil', 'Georgia Tech, US', 'MPI-SWS, Germany', 'UFMG, Brazil', 'MPI-SWS, Germany', 'UFMG, Brazil', 'Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, France', 'MPI-SWS, Germany', 'University of Maryland, US']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
On the apparent conflict between individual and group fairness,"A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.",['Reuben Binns'],['University of Oxford'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜,"The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.","['Emily M. Bender', 'Timnit Gebru', 'Angelina McMillan-Major', 'Shmargaret Shmitchell']","['University of Washington, Seattle, WA, USA', 'Black in AI, Palo Alto, CA, USA', 'University of Washington, Seattle, WA, USA', 'The Aether']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
On the Moral Justification of Statistical Parity,"A crucial but often neglected aspect of algorithmic fairness is the question of how we justify enforcing a certain fairness metric from a moral perspective. When fairness metrics are proposed, they are typically argued for by highlighting their mathematical properties. Rarely are the moral assumptions beneath the metric explained. Our aim in this paper is to consider the moral aspects associated with the statistical fairness criterion of independence (statistical parity). To this end, we consider previous work, which discusses the two worldviews ""What You See Is What You Get"" (WYSIWYG) and ""We're All Equal"" (WAE) and by doing so provides some guidance for clarifying the possible assumptions in the design of algorithms. We present an extension of this work, which centers on morality. The most natural moral extension is that independence needs to be fulfilled if and only if differences in predictive features (e.g. high school grades and standardized test scores are predictive of performance at university) between socio-demographic groups are caused by unjust social disparities or measurement errors. Through two counterexamples, we demonstrate that this extension is not universally true. This means that the question of whether independence should be used or not cannot be satisfactorily answered by only considering the justness of differences in the predictive features.","['Corinna Hertweck', 'Christoph Heitz', 'Michele Loi']","['Zurich University of Applied Sciences, University of Zurich', 'Zurich University of Applied Sciences', 'University of Zurich']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
On the Social and Technical Challenges of Web Search Autosuggestion Moderation,"Past research shows that users benefit from systems that support them in their writing and exploration tasks. The autosuggestion feature of Web search engines is an example of such a system: It helps users in formulating their queries by offering a list of suggestions as they type. Autosuggestions are typically generated by machine learning (ML) systems trained on a corpus of search logs and document representations. Such automated methods can become prone to issues that result in problematic suggestions that are biased, racist, sexist or in other ways inappropriate. While current search engines have become increasingly proficient at suppressing such problematic suggestions, there are still persistent issues that remain. In this paper, we reflect on past efforts and on why certain issues still linger by covering explored solutions along a prototypical pipeline for identifying, detecting, and addressing problematic autosuggestions. To showcase their complexity, we discuss several dimensions of problematic suggestions, difficult issues along the pipeline, and why our discussion applies to the increasing number of applications beyond web search that implement similar textual suggestion features. By outlining persistent social and technical challenges in moderating web search suggestions, we provide a renewed call for action.","['T. J. Hazen', 'Alexandra Olteanu', 'Gabriella Kazai', 'Fernando Diaz', 'Michael Golebiewski']",Microsoft,UNKNOWN,2020-07-01,TRUE
"One Label, One Billion Faces: Usage and Consistency of Racial Categories in Computer Vision","Computer vision is widely deployed, has highly visible, society-altering applications, and documented problems with bias and representation. Datasets are critical for benchmarking progress in fair computer vision, and often employ broad racial categories as population groups for measuring group fairness. Similarly, diversity is often measured in computer vision datasets by ascribing and counting categorical race labels. However, racial categories are ill-defined, unstable temporally and geographically, and have a problematic history of scientific use. Although the racial categories used across datasets are superficially similar, the complexity of human race perception suggests the racial system encoded by one dataset may be substantially inconsistent with another. Using the insight that a classifier can learn the racial system encoded by a dataset, we conduct an empirical study of computer vision datasets supplying categorical race labels for face images to determine the cross-dataset consistency and generalization of racial categories. We find that each dataset encodes a substantially unique racial system, despite nominally equivalent racial categories, and some racial categories are systemically less consistent than others across datasets. We find evidence that racial categories encode stereotypes, and exclude ethnic groups from categories on the basis of nonconformity to stereotypes. Representing a billion humans under one racial category may obscure disparities and create new ones by encoding stereotypes of racial systems. The difficulty of adequately converting the abstract concept of race into a tool for measuring fairness underscores the need for a method more flexible and culturally aware than racial categories.","['Zaid Khan', 'Yun Fu']","['Northeastern University', 'Northeastern University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Onward for the freedom of others: marching beyond the AI ethics,"The debate on the ethics of Artificial Intelligence brought together different stakeholders including but not limited to academics, policymakers, CEOs, activists, workers' representatives, lobbyists, journalists, and 'moral machines'. Prominent political institutions crafted principles for the 'ethical being' of the AI companies while tech giants were documenting ethics in a series of self-written guidelines. In parallel, a large community started to flourish, focusing on how to technically embed ethical parameters into algorithmic systems. Founded upon the philosophical work of Simone de Beauvoir and Jean-Paul Sartre, this paper explores the philosophical antinomies of the 'AI Ethics' debate as well as the conceptual disorientation of the 'fairness discussion'. By bringing the philosophy of existentialism to the dialogue, this paper attempts to challenge the dialectical monopoly of utilitarianism and sheds fresh light on the -already glaring- AI arena. Why is 'the AI Ethics guidelines' a futile battle doomed to dangerous abstraction? How this battle can harm our sense of collective freedom? Which is the uncomfortable reality that remains obscured by the smoke-gas of the 'AI Ethics' discussion? And eventually, what's the alternative? There seems to be a different pathway for discussing and implementing ethics; A pathway that sets the freedom of others at the epicenter of the battle for a sustainable and open to all future.",['Petros Terzis'],"['University of Winchester, Winchester, UK']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Operationalizing Framing to Support Multiperspective Recommendations of Opinion Pieces,"Diversity in personalized news recommender systems is often defined as dissimilarity, and operationalized based on topic diversity (e.g., corona versus farmers strike). Diversity in news media, however, is understood as multiperspectivity (e.g., different opinions on corona measures), and arguably a key responsibility of the press in a democratic society. While viewpoint diversity is often considered synonymous with source diversity in communication science domain, in this paper, we take a computational view. We operationalize the notion of framing, adopted from communication science. We apply this notion to a re-ranking of topic-relevant recommended lists, to form the basis of a novel viewpoint diversification method. Our offline evaluation indicates that the proposed method is capable of enhancing the viewpoint diversity of recommendation lists according to a diversity metric from literature. In an online study, on the Blendle platform, a Dutch news aggregator, with more than 2000 users, we found that users are willing to consume viewpoint diverse news recommendations. We also found that presentation characteristics significantly influence the reading behaviour of diverse recommendations. These results suggest that future research on presentation aspects of recommendations can be just as important as novel viewpoint diversification methods to truly achieve multiperspectivity in online news environments.","['Mats Mulder', 'Oana Inel', 'Jasper Oosterman', 'Nava Tintarev']","['Delft University of Technology, Delft, The Netherlands', 'Delft University of Technology, Delft, The Netherlands', 'Blendle Utrecht, The Netherlands', 'Maastricht University Maastricht, The Netherlands']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Operationalizing the Legal Principle of Data Minimization for Personalization,"Article 5(1)(c) of the European Union’s General Data Protection Regulation (GDPR) requires that “personal data shall be […] adequate, relevant, and limited to what is necessary in relation to the purposes for which they are processed (‘data minimisation’)”. To date, the legal and computational definitions of ‘purpose limitation’ and ‘data minimization’ remain largely unclear. In particular, the interpretation of these principles is an open issue for information access systems that optimize for user experience through personalization and do not strictly require personal data collection for the delivery of basic service.","['Asia J. Biega', 'Peter Potash', 'Hal Daumé III', 'Fernando Diaz', 'Michèle Finck']",Microsoft,SIGIR 2020,2020-07-28,TRUE
Optimal Noise-Adding Mechanism in Additive Differential Privacy,"We derive the optimal $(0, \delta)$-differentially private query-output independent noise-adding mechanism for single real-valued query function under a general cost-minimization framework. Under a mild technical condition, we show that the optimal noise probability distribution is a uniform distribution with a probability mass at the origin. We explicitly derive the optimal noise distribution for general $\ell^p$ cost functions, including $\ell^1$ (for noise magnitude) and $\ell^2$ (for noise power) cost functions, and show that the probability concentration on the origin occurs when $\delta > \frac&lbrace;p&rbrace;&lbrace;p+1&rbrace;$. Our result demonstrates an improvement over the existing Gaussian mechanisms  by a factor of two and three for $(0,\delta)$-differential privacy in the high privacy regime in the context of minimizing the noise magnitude and noise power, and the gain is more pronounced in the low privacy regime. Our result is consistent with the existing result for $(0,\delta)$-differential privacy in the discrete setting, and identifies a probability concentration phenomenon in the continuous setting.","['Wei Ding', 'Ruiqi Guo', 'Sanjiv Kumar']",Google,Proceedings of the 22th International Conference on Artificial Intelligence and Statistics (AISTATS) (2019),2019,TRUE
"Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals","Machine learning models often need to satisfy many real-world policy goals and capture some
kinds of side information. We show that many such goals can be mathematically expressed as
constraints on the modelâs predictions on the data, which we call rate constraints. In this paper, we
study the specific problem of training non-convex models subject to these rate constraints (which
are non-convex and non-differentiable), and the general problem of constrained optimization of
possibly non-convex objectives with possibly non-convex and non-differentiable constraints. In the
non-convex setting, the Lagrangian may not have an equilibrium to converge to, and thus using the
standard approach of Lagrange multipliers may fail as a deterministic solution may not even exist.
Furthermore, if the constraints are non-differentiable, then one cannot optimize the Lagrangian
with gradient-based methods by definition. To solve these issues, we present the proxy-Lagrangian,
which leads to an algorithm that produces a stochastic classifier with theoretical guarantees by
playing a two-player non-zero-sum game. The first player minimizes external regret in terms of a
differentiable relaxation of the constraints and the second player enforces the original constraints
by minimizing swap-regret: this leads to finding a solution concept which we call a semi-coarse
correlated equilibrium which we interestingly show corresponds to an approximately optimal and
feasible solution to the constrained optimization problem. We then give a procedure which shrinks
the randomized solution down to one that is a mixture of at most m + 1 deterministic solutions.
This culminates into end-to-end procedures which can provably solve non-convex constrained
optimization problems with possibly non-differentiable and non-convex constraints. We provide
extensive experimental results on benchmark and real-world problems, enforcing a broad range of
policy goals including different fairness metrics, and other goals on accuracy, coverage, recall, and
churn.","['Andrew Cotter', 'Serena Wang']",Google,Journal of Machine Learning Research (2019),2019,TRUE
Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems,"Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.",['Joshua A. Kroll'],"['Naval Postgraduate School, Monterey, CA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Overcoming Failures of Imagination in AI Infused System Development and Deployment,"NeurIPS 2020 requested that research paper submissions include impact statements on ‘potential nefarious uses and the consequences of failure.’ When researching, designing, and implementing systems, a key challenge to anticipating risks, however, is to overcome what Clarke (1962) called ‘failures of imagination.’ The growing research on bias, fairness, and transparency in computational systems aims to illuminate and mitigate harms, and could thus help inform reflections on possible negative impacts of particular pieces of technical work. The prevalent notion of computational harms — narrowly construed as either allocational or representational harms — does not fully capture the context dependent and unobservable nature of harms across the wide range of AI infused systems. The current literature primarily addresses only a small range of examples of harms to motivate algorithmic fixes, overlooking the wider scope of probable harms and the way these harms may affect different stakeholders. The system affordances and possible usage scenarios may also exacerbate harms in unpredictable ways, as they determine stakeholders’ control (including non-users) over how they interact with a system output. To effectively assist in anticipating and identifying harmful uses, we argue that frameworks of harms must be context-aware and consider a wider range of potential stakeholders, system affordances, uses, and outputs, as well as viable proxies for assessing harms in the widest sense.","['Margarita Boyarskaya', 'Alexandra Olteanu', 'Kate Crawford']",Microsoft,In the Navigating the Broader Impacts of AI Research Workshop at NeurIPS 2020,2020-12-01,TRUE
Overview of the TREC 2019 Fair Ranking Track,"


The goal of the TREC Fair Ranking track was to develop a benchmark for evaluating retrieval systems in terms of fairness to different content providers in addition to classic notions of relevance. As part of the benchmark, we defined standardized fairness metrics with evaluation protocols and released a dataset for the fair ranking problem. The 2019 task focused on reranking academic paper abstracts given a query. The objective was to fairly represent relevant authors from several groups that were unknown at the system submission time. Thus, the track emphasized the development of systems which have robust performance across a variety of group definitions. Participants were provided with querylog data (queries, documents, and relevance) from Semantic Scholar. This paper presents an overview of the track, including the task definition, descriptions of the data and the annotation process, as well as a comparison of the performance of submitted systems.


","['Asia J. Biega', 'Fernando Diaz', 'Michael D. Ekstrand', 'Sebastian Kohlmeier']",Microsoft,TREC 2019,2019-11-01,TRUE
Pairwise Fairness for Ranking and Regression,"Improving fairness for ranking and regression models has less mature algorithmic tooling than classifiers. Here, we present pairwise formulations of fairness for ranking and regression models that can express analogues of statistical fairness notions like equal opportunity or equal accuracy, as well as statistical parity. The proposed framework supports both discrete protected groups, and continuous protected attributes. We show that the resulting training problems can be efficiently and effectively solved using constrained optimization or robust optimization algorithms. Experiments illustrate the broad applicability and trade-offs of these methods.","['Harikrishna Narasimhan', 'Andy Cotter', 'Serena Lutong Wang']",Google,33rd AAAI Conference on Artificial Intelligence (2020),2020,TRUE
Pareto-Efficient Fairness for Skewed Subgroup Data,"As awareness of the potential for learned models to amplify existing societal biases increases,
the field of ML fairness has developed mitigation techniques. A prevalent method applies constraints, including equality of performance, with respect to subgroups defined over the intersection of sensitive attributes such as race and gender. Enforcing such constraints when the subgroup populations are considerably skewed with respect to a target can lead to unintentional degradation in performance, without benefiting any individual subgroup, counter to the United Nations Sustainable Development goals of reducing inequalities and promoting growth. In order to avoid such performance degradation while ensuring equitable treatment to all groups, we propose Pareto-Efficient Fairness (PEF), which identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane. Specifically, PEF finds a Pareto Optimal point which maximizes multiple subgroup
accuracy measures. The algorithm scalarizes using the adaptive weighted metric norm by iteratively searching the Pareto region of all models enforcing the fairness constraint. PEF is backed
by strong theoretical results on discoverability and provides domain practitioners finer control in
navigating both convex and non-convex accuracyfairness trade-offs. Empirically, we show that PEF
increases performance of all subgroups in skewed synthetic data and UCI datasets.","['Alyssa Whitlock Lees', 'Chris Welty']",Google,AISG (2019),2019,TRUE
Perturbation Sensitivity Analysis to Detect Unintended Model Biases,"Data-driven statistical Natural Language Processing (NLP) techniques leverage large amounts of language data to build models that can understand language. However, most language data reflect the public discourse at the time the data was produced, and hence NLP models are susceptible to learning incidental associations around named referents at a particular point in time, in addition to general linguistic meaning. An NLP system designed to model notions such as sentiment and toxicity should ideally produce scores that are independent of the identity of such entities mentioned in text and their social associations. For example, in a general purpose sentiment analysis system, a phrase such as I hate Katy Perry should be interpreted as having the same sentiment as I hate Taylor Swift. Based on this idea, we propose a generic evaluation framework, Perturbation Sensitivity Analysis, which detects unintended model biases related to named entities, and requires no new annotations or corpora. We demonstrate the utility of this analysis by employing it on two different NLP models --- a sentiment model and a toxicity model --- applied on online comments in English language from four different genres.","['Vinodkumar Prabhakaran', 'Ben Hutchinson']",Google,EMNLP 2019 (2019),2019,TRUE
Playing the Game of Universal Adversarial Perturbations,"We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.
By observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely \fp,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.","['Mateusz Malinowski', 'Olivier Pietquin']",Google,(2018) (2018),2018,TRUE
POTs: protective optimization technologies,"Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks' ability to capture a variety of harms caused by systems. We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms' inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial. We propose Protective Optimization Technologies (POTs). POTs, provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic beating applications, and recalibrating credit scoring for loan applicants.","['Bogdan Kulynych', 'Rebekah Overdorf', 'Carmela Troncoso', 'Seda Gürses']","['EPFL', 'EPFL', 'EPFL', 'TU Delft / KU Leuven']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Practical Compositional Fairness: Understanding Fairness in Multi-Component Recommender Systems,"Most literature in fairness has focused on improving fairness with respect to one single model or one single objective. However, real-world machine learning systems are usually composed of many different components. Unfortunately, recent research has shown that even if each component is ""fair"", the overall system can still be ""unfair"".  In this paper, we focus on how well fairness composes over multiple components in real systems. We consider two recently proposed fairness metrics for rankings: exposure and pairwise ranking accuracy gap. We provide theory that demonstrates a set of conditions under which fairness of individual models does compose. We then present an analytical framework for both understanding whether a system's signals can achieve compositional fairness, and diagnosing which of these signals lowers the overall system's end-to-end fairness the most. Despite previously bleak theoretical results, on multiple data-sets -- including a large-scale real-world recommender system -- we find that the overall system's end-to-end fairness is largely achievable by improving fairness in individual components.","['Xuezhi Wang', 'Nithum Thain', 'Flavien Prost', 'Ed H. Chi', 'Alex Beutel']",Google,WSDM 2021,2021,TRUE
Preference-informed fairness,"In this work, we study notions of fairness in decision-making systems when individuals have diverse preferences over the possible outcomes of the decisions. Our starting point is the seminal work of Dwork et al. [ITCS 2012] which introduced a notion of individual fairness (IF): given a task-specific similarity metric, every pair of individuals who are similarly qualified according to the metric should receive similar outcomes. We show that when individuals have diverse preferences over outcomes, requiring IF may unintentionally lead to less-preferred outcomes for the very individuals that IF aims to protect (e.g. a protected minority group). A natural alternative to IF is the classic notion of fair division, envy-freeness (EF): no individual should prefer another individual's outcome over their own. Although EF allows for solutions where all individuals receive a highly-preferred outcome, EF may also be overly-restrictive for the decision-maker. For instance, if many individuals agree on the best outcome, then if any individual receives this outcome, they all must receive it, regardless of each individual's underlying qualifications for the outcome. We introduce and study a new notion of preference-informed individual fairness (PIIF) that is a relaxation of both individual fairness and envy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style constraints, but allows for deviations provided they are in line with individuals' preferences. We show that PIIF can permit outcomes that are more favorable to individuals than any IF solution, while providing considerably more flexibility to the decision-maker than EF. In addition, we show how to efficiently optimize any convex objective over the outcomes subject to PIIF for a rich class of individual preferences. Finally, we demonstrate the broad applicability of the PIIF framework by extending our definitions and algorithms to the multiple-task targeted advertising setting introduced by Dwork and Ilvento [ITCS 2019].","['Michael P. Kim', 'Aleksandra Korolova', 'Guy N. Rothblum', 'Gal Yona']","['Stanford University', 'University of Southern California', 'Weizmann Institute of Science', 'Weizmann Institute of Science']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Price Discrimination with Fairness Constraints,"Price discrimination - offering different prices to different customers - has become common practice. While it allows sellers to increase their profits, it also raises several concerns in terms of fairness. This topic has received extensive attention from media, industry, and regulatory agencies. In this paper, we consider the problem of setting prices for different groups under fairness constraints. In this paper, we propose a formal framework for pricing with fairness, including several definitions of fairness and their potential impact on consumers, sellers, and society at large. In a first step towards the ambitious agenda of designing pricing strategies that are fair, we consider the simplest scenario of a single-product seller facing consumers who can be partitioned into two groups based on a single, binary feature observable to the seller. For each group, we assume that the seller knows the valuation distribution and the population size. The seller's goal is to maximize profit by optimally selecting a price for each group, subject to a fairness constraint which may be self-imposed or explicitly enforced by laws and regulations. We first propose four definitions: fairness in price, demand, consumer surplus, and no-purchase valuation. With our model and definitions in place, we first show that satisfying all four fairness goals simultaneously is impossible unless the mean valuations are the same for both groups. In fact, even achieving two fairness measures simultaneously cannot be done in basic settings. We then consider the impact of imposing each fairness criterion separately, and identify conditions under which the consumer surplus and the social welfare increase or decrease. Under linear or exponential demand, we show that imposing a small amount of fairness in price or no-purchase valuation increases social welfare, whereas fairness in demand or surplus reduces social welfare. We fully characterize the impact of imposing different types of fairness for linear demand. We discover that imposing too much price fairness may result in a lower social welfare relative to imposing no price fairness. Imposing demand and surplus fairness always decreases social welfare. However, imposing no-purchase valuation fairness always increases social welfare. We also extend our results to the cases when there are multiple groups or there is an unprotected feature. Finally, we computationally show that most of our findings continue to hold for three common nonlinear demand models. Our results and insights provide a first step in understanding the impact of imposing fairness in the context of pricing.","['Maxime C. Cohen', 'Adam N. Elmachtoub', 'Xiao Lei']","['Desautels Faculty of Management, McGill University, Montreal, Quebec, Canada', 'Department of Industrial Engineering and Operations Research & Data Science Institute, Columbia University, New York, New York, USA', 'Department of Industrial Engineering and Operations Research, Columbia University, New York, New York, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Privacy Amplification by Iteration,"Most commonly used learning algorithms work by iteratively updating an intermediate solution using one or a few data points in each iteration.  Analysis of differential privacy for such algorithms often involves ensuring privacy of each step and then reasoning about the cumulative privacy cost of the algorithm. This is enabled by composition theorems for differential privacy that allow releasing of all the intermediate results.  In this work, we demonstrate that for contractive iterations, not releasing the intermediate results strongly amplifies the privacy guarantees.
",[],Google,2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS),2018,TRUE
Privacy Amplification via Random Check-Ins,"Differentially Private Stochastic Gradient Descent (DP-SGD) forms a fundamental building block in many applications for learning over sensitive data. Two standard approaches, privacy amplification by subsampling, and privacy amplification by shuffling, permit adding lower noise in DP-SGD than via na\""&lbrace;\i&rbrace;ve schemes. A key assumption in both these approaches is that the elements in the data set can be uniformly sampled, or be uniformly permuted ---  constraints that may become prohibitive when the data is processed in a decentralized or distributed fashion. In this paper, we focus on conducting iterative methods like DP-SGD in the setting of federated learning (FL) wherein the data is distributed among many devices (clients). Our main contribution is the random check-in distributed protocol, which crucially relies only on randomized participation decisions made locally and independently by each client. It has privacy/accuracy trade-offs similar to privacy amplification by subsampling/shuffling. However, our method does not require server-initiated communication, or even knowledge of the population size. To our knowledge, this is the first privacy amplification tailored for a distributed learning framework, and it may have broader applicability beyond FL. Along the way, we extend privacy amplification by shuffling to incorporate $(\epsilon,\delta)$-DP local randomizers, and exponentially improve its guarantees. In practical regimes, this improvement allows for similar privacy and utility using data from an order of magnitude fewer users.","['Peter Kairouz', 'H. Brendan McMahan', 'Om Thakkar']",Google,"Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020",2020,TRUE
Privacy Dependencies,"This Article offers a comprehensive survey of privacy dependencies—the many ways that our privacy depends on the decisions and disclosures of other people. What
we do and what we say can reveal as much about others as it does about ourselves, even when we don’t realize it or when we think we’re sharing information about ourselves alone.
We identify three bases upon which our privacy can depend: our social ties, our similarities to others, and our differences from others. In a tie-based dependency, an observer learns
about one person by virtue of her social relationships with others—family, friends, or other associates. In a similarity-based dependency, inferences about our unrevealed attributes are
drawn from our similarities to others for whom that attribute is known. And in difference-based dependencies, revelations about ourselves demonstrate how we are different from
others—by showing, for example, how we “break the mold” of normal behavior or establishing how we rank relative to others with respect to some desirable attribute. We
elaborate how these dependencies operate, isolating the relevant mechanisms and providing concrete examples of each mechanism in practice, the values they implicate, and the legal
and technical interventions that may be brought to bear on them. Our work adds to a growing chorus demonstrating that privacy is neither an individual choice nor an individual value—
but it is the first to systematically demonstrate how different types of dependencies can raise very different normative concerns, implicate different areas of law, and create different
challenges for regulation.","['Solon Barocas', 'Karen Levy']",Microsoft,Washington Law Review,2020-06-01,TRUE
Privacy in Geospatial Applications and Location-Based Social Networks,"The use of location data has greatly benefited from the availability of location-based services, the popularity of social networks, and the accessibility of public location data sets. However, in addition to providing users with the ability to obtain accurate driving directions or the convenience of geo-tagging friends and pictures, location is also a very sensitive type of data, as attested by more than a decade of research on different aspects of privacy related to location data.",['Igor Bilogrevic'],Google,Handbook of Mobile Data Privacy (2018),2018,TRUE
Privacy-preserving Prediction,"Ensuring differential privacy of models learned from sensitive user data is an important goal that has been studied extensively in recent years.
It is now known that for some basic learning problems, especially those involving high-dimensional data, producing an accurate private model requires much more data than learning without privacy. At the same time, in many applications it is not necessary to expose the model itself. Instead users may be allowed to query the prediction model on their inputs only through an appropriate interface. Here we formulate the problem of ensuring privacy of individual predictions and investigate the overheads required to achieve it in several standard models of classification and regression.",[],Google,Conference on Learning Theory (COLT) 2018,2018,TRUE
Problem Formulation and Fairness,"Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team---and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases---we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways---and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.","['Samir Passi', 'Solon Barocas']","['Information Science, Cornell University', 'Information Science, Cornell University']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
"Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements","As more researchers have become aware of and passionate about algorithmic fairness, there has been an explosion in papers laying out new metrics, suggesting algorithms to address issues, and calling attention to issues in existing applications of machine learning. This research has greatly expanded our understanding of the concerns and challenges in deploying machine learning, but there has been much less work in seeing how the rubber meets the road. ","['Alex Beutel', 'Ed H. Chi']",Google,"AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) (2019)",2019,TRUE
Racial categories in machine learning,"Controversies around race and machine learning have sparked debate among computer scientists over how to design machine learning systems that guarantee fairness. These debates rarely engage with how racial identity is embedded in our social experience, making for sociological and psychological complexity. This complexity challenges the paradigm of considering fairness to be a formal property of supervised learning with respect to protected personal attributes. Racial identity is not simply a personal subjective quality. For people labeled ""Black"" it is an ascribed political category that has consequences for social differentiation embedded in systemic patterns of social inequality achieved through both social and spatial segregation. In the United States, racial classification can best be understood as a system of inherently unequal status categories that places whites as the most privileged category while signifying the Negro/black category as stigmatized. Social stigma is reinforced through the unequal distribution of societal rewards and goods along racial lines that is reinforced by state, corporate, and civic institutions and practices. This creates a dilemma for society and designers: be blind to racial group disparities and thereby reify racialized social inequality by no longer measuring systemic inequality, or be conscious of racial categories in a way that itself reifies race. We propose a third option. By preceding group fairness interventions with unsupervised learning to dynamically detect patterns of segregation, machine learning systems can mitigate the root cause of social disparities, social segregation and stratification, without further anchoring status categories of disadvantage.","['Sebastian Benthall', 'Bruce D. Haynes']","['New York University', 'University of California, Davis']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Re-imagining Algorithmic Fairness in India and Beyond,"Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.","['Nithya Sambasivan', 'Erin Arnesen', 'Ben Hutchinson', 'Tulsee Doshi', 'Vinodkumar Prabhakaran']","['Google Research Mountain View, CA', 'Google Research Mountain View, CA', 'Google Research Mountain View, CA', 'Google Research Mountain View, CA', 'Google Research Mountain View, CA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
"Reasons, Values, Stakeholders: A Philosophical Framework for Explainable Artificial Intelligence","The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholders, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate nearly prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions, recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations, and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.",['Atoosa Kasirzadeh'],['University of Toronto Australian National University'],"FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Recommendations and user agency: the reachability of collaboratively-filtered information,"Recommender systems often rely on models which are trained to maximize accuracy in predicting user preferences. When the systems are deployed, these models determine the availability of content and information to different users. The gap between these objectives gives rise to a potential for unintended consequences, contributing to phenomena such as filter bubbles and polarization. In this work, we consider directly the information availability problem through the lens of user recourse. Using ideas of reachability, we propose a computationally efficient audit for top-N linear recommender models. Furthermore, we describe the relationship between model complexity and the effort necessary for users to exert control over their recommendations. We use this insight to provide a novel perspective on the user cold-start problem. Finally, we demonstrate these concepts with an empirical investigation of a state-of-the-art model trained on a widely used movie ratings dataset.","['Sarah Dean', 'Sarah Rich', 'Benjamin Recht']","['UC Berkeley', 'Canopy Crest', 'UC Berkeley']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Reducing sentiment polarity for demographic attributes in word embeddings using adversarial learning,"The use of word embedding models in sentiment analysis has gained a lot of traction in the Natural Language Processing (NLP) community. However, many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment, resulting in unfair downstream machine learning algorithms. We leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment, and re-embed them into the word embeddings. We show that our method effectively minimizes unfair positive/negative sentiment polarity while retaining the semantic accuracy of the word embeddings. Furthermore, we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks.","['Chris Sweeney', 'Maryam Najafian']","['MIT', 'MIT']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
"Regulating transparency?: Facebook, Twitter and the German Network Enforcement Act","Regulatory regimes designed to ensure transparency often struggle to ensure that transparency is meaningful in practice. This challenge is particularly great when coupled with the widespread usage of dark patterns --- design techniques used to manipulate users. The following article analyses the implementation of the transparency provisions of the German Network Enforcement Act (NetzDG) by Facebook and Twitter, as well as the consequences of these implementations for the effective regulation of online platforms. This question of effective regulation is particularly salient, due to an enforcement action in 2019 by Germany's Federal Office of Justice (BfJ) against Facebook for what the BfJ claim were insufficient compliance with transparency requirements, under NetzDG. This article provides an overview of the transparency requirements of NetzDG and contrasts these with the transparency requirements of other relevant regulations. It will then discuss how transparency concerns not only providing data, but also how the visibility of the data that is made transparent is managed, by deciding how the data is provided and is framed. We will then provide an empirical analysis of the design choices made by Facebook and Twitter, to assess the ways in which their implementations differ. The consequences of these two divergent implementations on interface design and user behaviour are then discussed, through a comparison of the transparency reports and reporting mechanisms used by Facebook and Twitter. As a next step, we will discuss the BfJ's consideration of the design of Facebook's content reporting mechanisms, and what this reveals about their respective interpretations of NetzDG's scope. Finally, in recognising that this situation is one in which a regulator is considering design as part of their action - we develop a wider argument on the potential for regulatory enforcement around dark patterns, and design practices more generally, for which this case is an early, indicative example.","['Ben Wagner', 'Krisztina Rozgonyi', 'Marie-Therese Sekwenz', 'Jennifer Cobbe', 'Jatinder Singh']","['Vienna University of Economics and Business', 'University of Vienna', 'Vienna University of Economics and Business', 'University of Cambridge', 'University of Cambridge']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring,"We improve the recently-proposed ``MixMatch'' semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring. Distribution alignment encourages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of groundtruth labels. Augmentation anchoring feeds multiple strongly augmented versions of an input into the model and encourages each output to be close to the prediction for a weakly-augmented version of the same input. To produce strong augmentations, we propose a variant of AutoAugment which learns the augmentation policy while the model is being trained. Our new algorithm, dubbed ReMixMatch, is significantly more data-efficient than prior work, requiring between 5x and 16x less data to reach the same accuracy. For example, on CIFAR10 with 250 labeled examples we reach 93.73% accuracy (compared to MixMatchâs accuracy of 93.58% with 4,000 examples) and a median accuracy of 84.92% with just four labels per class. We make our code and data open-source at https://github.com/google-research/remixmatch.","['Alex Kurakin', 'Ekin Dogus Cubuk', 'Kihyuk Sohn', 'Nicholas Carlini']",Google,ICLR (2020),2020,TRUE
Removing Spurious Features can Hurt Accuracy and Affect Groups Disproportionately,"Spurious features interfere with the goal of obtaining robust models that perform well across many groups within the population. A natural remedy is to remove such features from the model. However, in this work, we show that removing spurious features can surprisingly decrease accuracy due to the inductive biases of overparameterized models. In noiseless overparameterized linear regression, we completely characterize how the removal of spurious features affects accuracy across different groups (more generally, test distributions). In addition, we show that removal of spurious features can decrease the accuracy even on balanced datasets (where each target co-occurs equally with each spurious feature); and it can inadvertently make the model more susceptible to other spurious features. Finally, we show that robust self-training produces models that no longer depend on spurious features without affecting their overall accuracy. The empirical results on the Toxic-Comment-Detection and CelebA datasets show that our results hold in non-linear models.","['Fereshte Khani', 'Percy Liang']","['Stanford University', 'Stanford University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
"Representativeness in Statistics, Politics, and Machine Learning","Representativeness is a foundational yet slippery concept. Though familiar at first blush, it lacks a single precise meaning. Instead, meanings range from typical or characteristic, to a proportionate match between sample and population, to a more general sense of accuracy, generalizability, coverage, or inclusiveness. Moreover, the concept has long been contested. In statistics, debates about the merits and methods of selecting a representative sample date back to the late 19th century; in politics, debates about the value of likeness as a logic of political representation are older still. Today, as the concept crops up in the study of fairness and accountability in machine learning, we need to carefully consider the term's meanings in order to communicate clearly and account for their normative implications. In this paper, we ask what representativeness means, how it is mobilized socially, and what values and ideals it communicates or confronts. We trace the concept's history in statistics and discuss normative tensions concerning its relationship to likeness, exclusion, authority, and aspiration. We draw on these analyses to think through how representativeness is used in FAccT debates, with emphasis on data, shift, participation, and power.","['Kyla Chasalow', 'Karen Levy']","['Cornell University', 'Cornell University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Reviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems,"This paper introduces reviewability as a framework for improving the accountability of automated and algorithmic decisionmaking (ADM) involving machine learning. We draw on an understanding of ADM as a socio-technical process involving both human and technical elements, beginning before a decision is made and extending beyond the decision itself. While explanations and other model-centric mechanisms may assist some accountability concerns, they often provide insufficient information of these broader ADM processes for regulatory oversight and assessments of legal compliance. Reviewability involves breaking down the ADM process into technical and organisational elements to provide a systematic framework for determining the contextually appropriate record-keeping mechanisms to facilitate meaningful review - both of individual decisions and of the process as a whole. We argue that a reviewability framework, drawing on administrative law's approach to reviewing human decision-making, offers a practical way forward towards more a more holistic and legally-relevant form of accountability for ADM.","['Jennifer Cobbe', 'Michelle Seng Ah Lee', 'Jatinder Singh']","['Compliant and Accountable Systems Research Group, University of Cambridge, UK', 'Compliant and Accountable Systems Research Group, University of Cambridge, UK', 'Compliant and Accountable Systems Research Group, University of Cambridge, UK']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Risk-Sensitive Generative Adversarial Imitation Learning,"We study risk-sensitive imitation learning where the agent's goal is to perform at least as well as the expert in terms of a risk profile. We first formulate our risk-sensitive imitation learning setting. We consider the generative adversarial approach to imitation learning (GAIL) and derive an optimization problem for our formulation, which we call it risk-sensitive GAIL (RS-GAIL). We then derive two different versions of our RS-GAIL optimization problem that aim at matching the risk profiles of the agent and the expert w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop risk-sensitive generative adversarial imitation learning algorithms based on these optimization problems. We evaluate the performance of our algorithms and compare them with GAIL and the risk-averse imitation learning (RAIL) algorithms in two MuJoCo and two OpenAI classical control tasks.",['Yinlam Chow'],Google,AISTATS (2018),2018,TRUE
Robot Eyes Wide Shut: Understanding Dishonest Anthropomorphism,"The goal of this paper is to advance design, policy, and ethics scholarship on how engineers and regulators can protect consumers from deceptive robots and artificial intelligences that exhibit the problem of dishonest anthropomorphism. The analysis expands upon ideas surrounding the principle of honest anthropomorphism originally formulated by Margot Kaminsky, Mathew Ruben, William D. Smart, and Cindy M. Grimm in their groundbreaking Maryland Law Review article, ""Averting Robot Eyes."" Applying boundary management theory and philosophical insights into prediction and perception, we create a new taxonomy that identifies fundamental types of dishonest anthropomorphism and pinpoints harms that they can cause. To demonstrate how the taxonomy can be applied as well as clarify the scope of the problems that it can cover, we critically consider a representative series of ethical issues, proposals, and questions concerning whether the principle of honest anthropomorphism has been violated.","['Brenda Leong', 'Evan Selinger']","['Future of Privacy Forum, Washington, D.C, USA', 'Department of Philosophy, Rochester Institute of Technology, Rochester, NY, USA']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
Robustness in machine learning explanations: does it matter?,"The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.",['Leif Hancox-Li'],['Capital One'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Roles for computing in social change,"A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems --- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.","['Rediet Abebe', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy', 'Manish Raghavan', 'David G. Robinson']","['Harvard University', 'Microsoft Research and Cornell University', 'Cornell University', 'Cornell University', 'Cornell University', 'Cornell University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing,"Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of five ethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.",['Joonseok Lee'],Google,"Proceedings of the 3rd AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) (2020)",2020,TRUE
Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints,"Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation. We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against \textit&lbrace;any&rbrace; number of adversarial deletions. We extensively evaluate the performance of our algorithms against prior state-of-the-art on real-world applications, including (i) Uber-pick up locations with location privacy constraints; (ii) feature selection with fairness constraints for income prediction and crime rate prediction; and (iii) robust to deletion summarization of census data, consisting of 2,458,285 feature vectors.",['Morteza Zadimoghaddam'],Google,"Thirty-fifth International Conference on Machine Learning, ICML 2018",2018,TRUE
Scalable Private Learning with PATE,"The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a ""student"" model the knowledge of an ensemble of ""teacher"" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachersâ answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets.","['Nicolas Papernot', 'Shuang Song']",Google,International Conference on Learning Representations (ICLR) (2018),2018,TRUE
Shape Constraints for Set Functions,"Set functions predict a label from a permutation-invariant variable-size collection of feature vectors. We propose making set functions more understandable and regularized by capturing domain knowledge through shape constraints. We show how prior work in monotonic constraints can be adapted to set functions. Then we propose two new shape constraints designed to generalize the conditioning role of weights in a weighted mean. We show how one can train standard functions and set functions that satisfy these shape constraints with a deep lattice network. We propose a non-linear estimation strategy we call the semantic feature engine that uses set functions with the proposed shape constraints to estimate labels for compound sparse categorical features. Experiments on real-world data show the achieved accuracy is similar to deep sets or deep neural networks, but provides guarantees of the model behavior and is thus easier to explain and debug.","['Andrew Cotter', 'Serena Wang']",Google,International Conference on Machine Learning (2019),2019,TRUE
SIREN: A Simulation Framework for Understanding the Effects of Recommender Systems in Online News Environments,"The growing volume of digital data stimulates the adoption of recommender systems in different socioeconomic domains, including news industries. While news recommenders help consumers deal with information overload and increase their engagement, their use also raises an increasing number of societal concerns, such as ""Matthew effects"", ""filter bubbles"", and the overall lack of transparency. We argue that focusing on transparency for content-providers is an under-explored avenue. As such, we designed a simulation framework called SIREN1 (SImulating Recommender Effects in online News environments), that allows content providers to (i) select and parameterize different recommenders and (ii) analyze and visualize their effects with respect to two diversity metrics. Taking the U.S. news media as a case study, we present an analysis on the recommender effects with respect to long-tail novelty and unexpectedness using SIREN. Our analysis offers a number of interesting findings, such as the similar potential of certain algorithmically simple (item-based k-Nearest Neighbour) and sophisticated strategies (based on Bayesian Personalized Ranking) to increase diversity over time. Overall, we argue that simulating the effects of recommender systems can help content providers to make more informed decisions when choosing algorithmic recommenders, and as such can help mitigate the aforementioned societal concerns.","['Dimitrios Bountouridis', 'Jaron Harambam', 'Mykola Makhortykh', 'Mónica Marrero', 'Nava Tintarev', 'Claudia Hauff']","['Delft University of Technology, The Netherlands', 'Institute for Information Law, University of Amsterdam, The Netherlands', 'Amsterdam School of Communication Research, University of Amsterdam, The Netherlands', 'Delft University of Technology, The Netherlands', 'Delft University of Technology, The Netherlands', 'Delft University of Technology, The Netherlands']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
Social Biases in NLP Models as Barriers for Persons with Disabilities,"Building equitable and inclusive technologies
demands paying attention to how social attitudes towards persons with disabilities are
represented within technology. Representations perpetuated by NLP models often inadvertently encode undesirable social biases
from the data on which they are trained. In this
paper, first we present evidence of such undesirable biases towards mentions of disability in
two different NLP models: toxicity prediction
and sentiment analysis. Next, we demonstrate
that neural embeddings that are critical first
steps in most NLP pipelines also contain undesirable biases towards mentions of disabilities.
We then expose the topical biases in the social
discourse about some disabilities which may
explain such biases in the models; for instance,
terms related to gun violence, homelessness,
and drug addiction are over-represented in discussions about mental illness.","['Ben Hutchinson', 'Vinodkumar Prabhakaran', 'Kellie Webster', 'Yu Zhong']",Google,Proceedings of ACL 2020,2020,TRUE
"Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries","Social data in digital form, including user-generated content, expressed or implicit relations between people, and behavioral traces, are at the core of popular applications and platforms, driving the research agenda of many researchers. The promises of social data are many, including understanding “what the world thinks” about a social issue, brand, celebrity, or other entity, as well as enabling better decision-making in a variety of fields including public policy, healthcare, and economics. Many academics and practitioners have warned against the naive usage of social data. There are biases and inaccuracies occurring at the source of the data, but also introduced during processing. There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked. This paper recognizes the rigor with which these issues are addressed by different researchers varies across a wide range. We identify a variety of menaces in the practices around social data use, and organize them in a framework that helps to identify them.","['Alexandra Olteanu', 'Carlos Castillo', 'Fernando Diaz', 'Emre Kiciman']",Microsoft,Frontiers in Big Data,2019-07-11,TRUE
Socially Fair k-Means Clustering,"We show that the popular k-means clustering algorithm (Lloyd's heuristic), used for a variety of scientific data, can result in outcomes that are unfavorable to subgroups of data (e.g., demographic groups). Such biased clusterings can have deleterious implications for human-centric applications such as resource allocation. We present a fair k-means objective and algorithm to choose cluster centers that provide equitable costs for different groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for k-means, inheriting its simplicity, efficiency, and stability. In comparison with standard Lloyd's, we find that on benchmark datasets, Fair-Lloyd exhibits unbiased performance by ensuring that all groups have equal costs in the output k-clustering, while incurring a negligible increase in running time, thus making it a viable fair option wherever k-means is currently used.","['Mehrdad Ghadiri', 'Samira Samadi', 'Santosh Vempala']","['Georgia Tech, USA', 'MPI for Intelligent Systems, Germany', 'Georgia Tech, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
"Spoken Corpora Data, Automatic Speech Recognition, and Bias Against African American Language: The case of Habitual 'Be'","Recent work has revealed that major automatic speech recognition (ASR) systems such as Apple, Amazon, Google, IBM, and Microsoft perform much more poorly for Black U.S. speakers than for white U.S. speakers. Researchers postulate that this may be a result of biased datasets which are largely racially homogeneous. However, while the study of ASR performance with regards to the intersection of racial identity and language use is slowly gaining traction within AI, machine learning, and algorithmic bias research, little to nothing has been done to examine the data drawn from the spoken corpora which are commonly used in the training and evaluation of ASRs in order to understand whether or not they are actually biased, this study seeks to begin addressing this gap in the research by investigating spoken corpora used for ASR training and evaluation for a grammatical linguistic feature of what the field of linguistics terms African American Language (AAL), a systematic, rule-governed, and legitimate linguistic variety spoken by many (but not all) African Americans in the U.S. This grammatical feature, habitual 'be', is an uninflected form of 'be' that encodes the characteristic of habituality, as in ""I be in my office by 7:30am"", paraphrasable as ""I am usually in my office by 7:30"" in Standardized American English. This study utilizes established corpus linguistics methods on the transcribed data of four major spoken corpora -- Switchboard, Fisher, TIMIT, and LibriSpeech -- to understand the frequency, distribution, and usage of habitual 'be' within each corpus as compared to a reference corpus of spoken AAL -- the Corpus of Regional African American Language (CORAAL). The results find that habitual 'be' appears far less frequently, is dispersed in far fewer transcribed texts, and is surrounded by a much less diverse set of word types and parts of speech in the four ASR corpora as compared with CORAAL. This work provides foundational evidence that spoken corpora used in the training and evaluation of widely used ASR systems are, in fact, biased against AAL and likely contribute to poorer ASR performance for Black users.",['Joshua L Martin'],"['University of Florida Gainesville, Florida']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Standardized Tests and Affirmative Action: The Role of Bias and Variance,"The University of California suspended through 2024 the requirement that applicants from California submit SAT scores, upending the major role standardized testing has played in college admissions. We study the impact of such decisions and its interplay with other policies---such as affirmative action---on admitted class composition. This paper considers a theoretical framework to study the effect of requiring test scores on academic merit and diversity in college admissions. The model has a college and set of potential students. Each student has observed application components and group membership, as well as an unobserved noisy skill level generated from an observed distribution. The college is Bayesian and maximizes an objective that depends on both diversity and merit. It estimates each applicant's true skill level using the observed features and potentially their group membership, and then admits students with or without affirmative action. We characterize the trade-off between the (potentially positive) informational role of standardized testing in college admissions and its (negative) exclusionary nature. Dropping test scores may exacerbate disparities by decreasing the amount of information available for each applicant, especially those from non-traditional backgrounds. However, if there are substantial barriers to testing, removing the test improves both academic merit and diversity by increasing the size of the applicant pool. Finally, using application and transcript data from the University of Texas at Austin, we demonstrate how an admissions committee could measure the trade-off in practice to better decide whether to drop their test scores requirement. The full paper can be found at https://arxiv.org/abs/2010.04396.","['Nikhil Garg', 'Hannah Li', 'Faidra Monachou']","['UC Berkeley, Berkeley, USA', 'Stanford University, Stanford, USA', 'Stanford University, Stanford, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Stretching Human Laws to Apply to Machines: The Dangers of a 'Colorblind' Computer,"Automated decision making has become widespread in recent years, largely due to advances in machine learning. As a result of this trend, machine learning systems are increasingly used to make decisions in high-stakes domains, such as employment or university admissions. The weightiness of these decisions has prompted the realization that, like humans, machines must also comply with the law. But human decision-making processes are quite different from automated decision-making processes, which creates a mismatch between laws and the decision makers to which they are intended to apply. In turn, this mismatch can lead to counterproductive outcomes.","['Zach Harned', 'Hanna Wallach']",Microsoft,"Florida State University Law Review, Forthcoming",2019-12-01,TRUE
Studying up: reorienting the study of algorithmic fairness around issues of power,"Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of ""studying up"". We reflect on the contributions that the call to ""study up"" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation ""upward"". A case study from our own work illustrates what it looks like to reorient one's research questions ""up"" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that ""study up"". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.","['Chelsea Barabas', 'Colin Doyle', 'JB Rubinovitz', 'Karthik Dinakar']","['Massachusetts Institute of Technology', 'Harvard Law School', 'Massachusetts Institute of Technology', 'Harvard Law School']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Text Embeddings Contain Bias. Here's Why That Matters.,"With the public release of embedding models, itâs important to understand the various biases that they contain. Developers who use them should be aware of the biases inherent in the models as well as how biases can manifest in downstream applications that use these models. In this post, we examine a few specific forms of bias and suggest tools for evaluating as well as mitigating bias.",['Mario Guajardo-Céspedes'],Google,(2018) (2018),2018,TRUE
"The Algorithmic Leviathan: Arbitrariness, Fairness, and Opportunity in Algorithmic Decision Making Systems","Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern? We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are ""fair"" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.","['Kathleen Creel', 'Deborah Hellman']","['Stanford University, Palo Alto, California, USA', 'University of Virginia, Charlottesville, Virginia, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
The case for voter-centered audits of search engines during political elections,"Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's ""related searches"" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.","['Eni Mustafaraj', 'Emma Lurie', 'Claire Devine']","['Wellesley College', 'University of California', 'Wellesley College']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
The concept of fairness in the GDPR: a linguistic and contextual interpretation,"There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal. This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation. In terms of linguistic comparison, the paper analyses all translations of the world ""fair"" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law. The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive). In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter) In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version). The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of ""Treu und Glaube"") and equitability (French, Spanish and Portuguese). Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of ""bona fide"". Taking into account both the value of ""bona fide"" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects. The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of ""vulnerability"". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR. In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.",['Gianclaudio Malgieri'],"['Vrije Universiteit Brussel, Brissels, Belgium']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
The Disparate Effects of Strategic Manipulation,"When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system's approval. Models of agent responsiveness, termed ""strategic manipulation,"" analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to ""trick"" a published classifier. In cases of real world classification, however, an agent's ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group's costs are higher than the other's, the learner's equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner's utility while actually making both candidate groups worse-off---even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual's ""quality"" when agents' capacities to adaptively respond differ.","['Lily Hu', 'Nicole Immorlica', 'Jennifer Wortman Vaughan']","['Harvard University, Cambridge, MA', 'Microsoft Research, Cambridge, MA', 'Microsoft Research, New York, NY']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,TRUE
The disparate equilibria of algorithmic decision making when individuals invest rationally,"The long-term impact of algorithmic decision making is shaped by the dynamics between the deployed decision rule and individuals' response. Focusing on settings where each individual desires a positive classification---including many important applications such as hiring and school admissions, we study a dynamic learning setting where individuals invest in a positive outcome based on their group's expected gain and the decision rule is updated to maximize institutional benefit. By characterizing the equilibria of these dynamics, we show that natural challenges to desirable long-term outcomes arise due to heterogeneity across groups and the lack of realizability. We consider two interventions, decoupling the decision rule by group and subsidizing the cost of investment. We show that decoupling achieves optimal outcomes in the realizable case but has discrepant effects that may depend on the initial conditions otherwise. In contrast, subsidizing the cost of investment is shown to create better equilibria for the disadvantaged group even in the absence of realizability.","['Lydia T. Liu', 'Ashia Wilson', 'Nika Haghtalab', 'Adam Tauman Kalai', 'Christian Borgs', 'Jennifer Chayes']","['University of California', 'Microsoft Research', 'Cornell University', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
"The Distributive Effects of Risk Prediction in Environmental Compliance: Algorithmic Design, Environmental Justice, and Public Policy","Government agencies are embracing machine learning to support a variety of resource allocation decisions. The U.S. Environmental Protection Agency (EPA), for example, has engaged academic research labs to test the use of machine learning in support of an important national initiative to reduce Clean Water Act violations. We evaluate prototypical risk prediction models that can support compliance interventions and demonstrate how critical algorithmic design choices can generate or mitigate disparate impact in environmental enforcement. First, we show that the definition of which facilities to focus on through this national compliance initiative hinges on arbitrary differences in state-level permitting schemes, causing a shift in environmental protection away from areas with more minority populations. Second, the policy objective to reduce the noncompliance rate is encoded in a classification model, which does not account for the extent of pollution beyond the permitted limit. We hence compare allocation schemes between regression and classification, and show that the latter directs attention towards facilities in more rural and white areas. Overall, our study illustrates that as machine learning enters government, algorithmic design can both embed and elucidate sources of administrative policy discretion with discernable distributional consequences.","['Elinor Benami', 'Reid Whitaker', 'Vincent La', 'Hongjin Lin', 'Brandon R. Anderson', 'Daniel E. Ho']","['Virginia Tech', 'University of California Berkeley', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
The effect of differential victim crime reporting on predictive policing systems,"Police departments around the world have been experimenting with forms of place-based data-driven proactive policing for over two decades. Modern incarnations of such systems are commonly known as hot spot predictive policing. These systems predict where future crime is likely to concentrate such that police can allocate patrols to these areas and deter crime before it occurs. Previous research on fairness in predictive policing has concentrated on the feedback loops which occur when models are trained on discovered crime data, but has limited implications for models trained on victim crime reporting data. We demonstrate how differential victim crime reporting rates across geographical areas can lead to outcome disparities in common crime hot spot prediction models. Our analysis is based on a simulation1 patterned after district-level victimization and crime reporting survey data for Bogotá, Colombia. Our results suggest that differential crime reporting rates can lead to a displacement of predicted hotspots from high crime but low reporting areas to high or medium crime and high reporting areas. This may lead to misallocations both in the form of over-policing and under-policing.","['Nil-Jana Akpinar', 'Maria De-Arteaga', 'Alexandra Chouldechova']","['Department of Statistics and Data Science & Machine Learning Department, Carnegie Mellon University', 'Information, Risk, and Operations, Management Department, McCombs School of Business, University of Texas at Austin', 'Heinz College & Department of Statistics and Data Science, Carnegie Mellon University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
The Effect of the Rooney Rule on Implicit Bias in the Long Term,"The Rooney Rule, originally proposed to counter implicit bias in hiring, has been implemented in the private and public sector in various settings. This rule requires that a decision-maker include at least one candidate from an underrepresented group in their shortlist of candidates. Recently, [42] proposed a mathematical model of implicit bias and studied the effectiveness of the Rooney Rule when applied to a single selection decision. However, selection decisions often occur repeatedly over time; e.g., a software firm is continuously hiring employees or a university makes admissions decisions every year. Further, it has been observed that, given consistent counterstereotypical feedback, implicit biases against underrepresented candidates can change. In this paper, we propose a model of how a decision-maker's implicit bias changes over time given their hiring decisions either with or without the Rooney Rule in place. Our model draws from the work of [42] and the literature on opinion dynamics. Our main result is that, for this model, when the decision-maker is constrained by the Rooney Rule, their implicit bias roughly reduces at a rate that is inverse of the size of the shortlist---independent of the total number of candidates, whereas without the Rooney Rule, the rate is inversely proportional to the number of candidates. Thus, our model predicts that when the number of candidates is much larger than the size of the shortlist, the Rooney Rule enables a significantly faster reduction in implicit bias, providing additional reason in favor of instating it as a strategy to mitigate implicit bias. Towards empirically evaluating the long-term effect of the Rooney Rule in repeated selection decisions, we conduct an iterative candidate selection experiment on Amazon Mechanical Turk. We observe that, indeed, decision-makers subject to the Rooney Rule select more minority candidates in addition to those required by the rule itself than they would if no rule is in effect, and in fact are able to do so without considerably decreasing the utility of candidates selected.","['L. Elisa Celis', 'Chris Hays', 'Anay Mehrotra', 'Nisheeth K. Vishnoi']","['Yale University', 'Yale University', 'Yale University', 'Yale University']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
The effects of competition and regulation on error inequality in data-driven markets,"Recent work has documented instances of unfairness in deployed machine learning models, and significant researcher effort has been dedicated to creating algorithms that intrinsically consider fairness. In this work, we highlight another source of unfairness: market forces that drive differential investment in the data pipeline for differing groups. We develop a high-level model to study this question. First, we show that our model predicts unfairness in a monopoly setting. Then, we show that under all but the most extreme models, competition does not eliminate this tendency, and may even exacerbate it. Finally, we consider two avenues for regulating a machine-learning driven monopolist - relative error inequality and absolute error-bounds - and quantify the price of fairness (and who pays it). These models imply that mitigating fairness concerns may require policy-driven solutions, not only technological ones.","['Hadi Elzayn', 'Benjamin Fish']","['University of Pennsylvania', 'Microsoft Research']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?,"There is a recent surge of papers that focus on attention as explanation of model predictions, giving mixed evidence on whether attention can be used as such. This has led some to try and `improve' attention so as to make it more interpretable. We argue that we should pay attention no heed.
While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear towards what goal it is used as explanation. We argue that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction. When that is the case, input saliency methods better suit our needs, and there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal for their explanations.","['Jasmijn Bastings', 'Katja Filippova']",Google,Proceedings of the 2020 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,2020,TRUE
The Ethics of Emotion in Artificial Intelligence Systems,"In this paper, we develop a taxonomy of conceptual models and proxy data used for digital analysis of human emotional expression and outline how the combinations and permutations of these models and data impact their incorporation into artificial intelligence (AI) systems. We argue we should not take computer scientists at their word that the paradigms for human emotions they have developed internally and adapted from other disciplines can produce ground truth about human emotions; instead, we ask how different conceptualizations of what emotions are, and how they can be sensed, measured and transformed into data, shape the ethical and social implications of these AI systems.","['Luke Stark', 'Jesse Hoey']","['Faculty of Information and Media Studies, University of Western Ontario, London ON Canada', 'David R. Cheriton School of Computer Science, University of Waterloo, Waterloo ON Canada']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
The Externalities of Exploration and How Data Diversity Helps Exploitation,"Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users for information that will lead to better decisions in the future. Recently, concerns have been raised about whether the process of exploration could be viewed as unfair, placing too much burden on certain individuals or groups. Motivated by these concerns, we initiate the study of the externalities of exploration – the undesirable side effects that the presence of one party may impose on another – under the linear contextual bandits model. We introduce the notion of a group externality, measuring the extent to which the presence of one population of users impacts the rewards of another. We show that this impact can in some cases be negative, and that, in a certain sense, no algorithm can avoid it. We then study externalities at the individual level, interpreting the act of exploration as an externality imposed on the current user of a system by future users. This drives us to ask under what conditions inherent diversity in the data makes explicit exploration unnecessary. We build on a recent line of work on the smoothed analysis of the greedy algorithm that always chooses the action that currently looks optimal, improving on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most O~(T1/3)","['Manish Raghavan', 'Alex Slivkins', 'Jennifer Wortman Vaughan', 'Zhiwei Steven Wu']",Microsoft,31st Annual Conference on Learning Theory,2018-02-01,TRUE
The false promise of risk assessments: epistemic reform and the limits of fairness,"Risk assessments have proliferated in the United States criminal justice system. The theory of change motivating their adoption involves two key assumptions: first, that risk assessments will reduce human biases by making objective decisions, and second, that risk assessments will promote criminal justice reform. In this paper I interrogate both of these assumptions, concluding that risk assessments are an ill-advised tool for challenging the centrality and legitimacy of incarceration within the criminal justice system. First, risk assessments fail to provide objectivity, as their use creates numerous sites of discretion. Second, risk assessments provide no guarantee of reducing incarceration; instead, they risk legitimizing the criminal justice system's structural racism. I then consider, via an ""epistemic reform,"" the path forward for criminal justice reform. I reinterpret recent results regarding the ""impossibility of fairness"" as not simply a tension between mathematical metrics but as evidence of a deeper tension between notions of equality. This expanded frame challenges the formalist, colorblind proceduralism at the heart of the criminal justice system and suggests a more structural approach to reform. Together, this analysis highlights how algorithmic fairness narrows the scope of judgments about justice and how ""fair"" algorithms can reinforce discrimination.",['Ben Green'],['Harvard University'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
The hidden assumptions behind counterfactual explanations and principal reasons,"Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established ""principal reason"" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant---and withholding others. These ""feature-highlighting explanations"" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear. In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes. We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden. While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world---and the subjective choices necessary to compensate for this---must be understood before these techniques can be usefully implemented.","['Solon Barocas', 'Andrew D. Selbst', 'Manish Raghavan']","['Microsoft Research and Cornell University', 'University of California Los Angeles', 'Cornell University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
The impact of overbooking on a pre-trial risk assessment tool,"Pre-trial risk assessment tools are used to make recommendations to judges about appropriate conditions of pre-trial supervision for people who have been arrested. Increasingly, there is concern about whether these models are operating fairly, including concerns about whether the models' input factors are fair measures of one's criminal activity. In this paper, we assess the impact of booking charges that do not result in a conviction on a popular risk assessment tool, the Arnold Public Safety Assessment. Using data from a pilot run of the tool in San Francisco, CA, we find that booking charges that do not result in a conviction (i.e. charges that are dropped or end in an acquittal) increased the recommended level of pre-trial supervision in around 27% of cases evaluated by the tool.","['Kristian Lum', 'Chesa Boudin', 'Megan Price']","['Human Rights Data Analysis Group', ""San Francisco Public Defender's Office"", 'Human Rights Data Analysis Group']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models","We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models--including classification, seq2seq, and structured prediction--and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.","['Ian Tenney', 'James Wexler', 'Jasmijn Bastings', 'Tolga Bolukbasi', 'Sebastian Gehrmann', 'Mahima Pushkarna', 'Emily Reif']",Google,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,2020,TRUE
The philosophical basis of algorithmic recourse,"Philosophers have established that certain ethically important values are modally robust in the sense that they systematically deliver correlative benefits across a range of counterfactual scenarios. In this paper, we contend that recourse - the systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenarios - is such a modally robust good. In particular, we argue that two essential components of a good life - temporally extended agency and trust - are underwritten by recourse. We critique existing approaches to the conceptualization, operationalization and implementation of recourse. Based on these criticisms, we suggest a revised approach to recourse and give examples of how it might be implemented - especially for those who are least well off1.","['Suresh Venkatasubramanian', 'Mark Alfano']","['University of Utah', 'Macquarie University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
The Profiling Potential of Computer Vision and the Challenge of Computational Empiricism,"Computer vision and other biometrics data science applications have commenced a new project of profiling people. Rather than using 'transaction generated information', these systems measure the 'real world' and produce an assessment of the 'world state' - in this case an assessment of some individual trait. Instead of using proxies or scores to evaluate people, they increasingly deploy a logic of revealing the truth about reality and the people within it. While these profiling knowledge claims are sometimes tentative, they increasingly suggest that only through computation can these excesses of reality be captured and understood. This article explores the bases of those claims in the systems of measurement, representation, and classification deployed in computer vision. It asks if there is something new in this type of knowledge claim, sketches an account of a new form of computational empiricism being operationalised, and questions what kind of human subject is being constructed by these technological systems and practices. Finally, the article explores legal mechanisms for contesting the emergence of computational empiricism as the dominant knowledge platform for understanding the world and the people within it.",['Jake Goldenfein'],"['Cornell Tech, New York, New York and Cornell Tech, Cornell University and Swinburne University of Technology']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
The relationship between trust in AI and trustworthy machine learning technologies,"To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.","['Ehsan Toreini', 'Mhairi Aitken', 'Kovila Coopamootoo', 'Karen Elliott', 'Carlos Gonzalez Zelaya', 'Aad van Moorsel']","['Newcastle University, United Kingdom', 'Newcastle University, United Kingdom', 'Newcastle University, United Kingdom', 'Newcastle University, United Kingdom', 'Newcastle University, United Kingdom', 'Newcastle University, United Kingdom']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
The Sanction of Authority: Promoting Public Trust in AI,"Trusted AI literature to date has focused on the trust needs of users who knowingly interact with discrete AIs. Conspicuously absent from the literature is a rigorous treatment of public trust in AI. We argue that public distrust of AI originates from the underdevelopment of a regulatory ecosystem that would guarantee the trustworthiness of the AIs that pervade society. Drawing from structuration theory and literature on institutional trust, we offer a model of public trust in AI that differs starkly from models driving Trusted AI efforts. This model provides a theoretical scaffolding for Trusted AI research which underscores the need to develop nothing less than a comprehensive and visibly functioning regulatory ecosystem. We elaborate the pivotal role of externally auditable AI documentation within this model and the work to be done to ensure it is effective, and outline a number of actions that would promote public trust in AI. We discuss how existing efforts to develop AI documentation within organizations---both to inform potential adopters of AI components and support the deliberations of risk and ethics review boards---is necessary but insufficient assurance of the trustworthiness of AI. We argue that being accountable to the public in ways that earn their trust, through elaborating rules for AI and developing resources for enforcing these rules, is what will ultimately make AI trustworthy enough to be woven into the fabric of our society.","['Bran Knowles', 'John T. Richards']","['Lancaster University, Lancaster, UK', 'TJ Watson Research Center, IBM Yorktown Heights, New York, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks,"This paper describes a testing methodology for quantitatively assessing the risk of \emph&lbrace;unintended memorization&rbrace; of rare or unique sequences in generative sequence models---a common type of neural network. Such models are sometimes trained on sensitive data (e.g., the text of users' private messages); our methodology allows deep-learning to choose configurations that minimize memorization during training, thereby  benefiting privacy.",['Nicholas Carlini'],Google,USENIX Security (2019),2019,TRUE
The Social Cost of Strategic Classification,"Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift. We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population. Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.","['Smitha Milli', 'John Miller', 'Anca D. Dragan', 'Moritz Hardt']","['University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
The social lives of generative adversarial networks,"Generative adversarial networks (GANs) are a genre of deep learning model of significant practical and theoretical interest for their facility in producing photorealistic 'fake' images which are plausibly similar, but not identical, to a corpus of training data. But from the perspective of a sociologist, the distinctive architecture of GANs is highly suggestive. First, a convolutional neural network for classification, on its own, is (at present) popularly considered to be an 'AI'; and a generative neural network is a kind of inversion of such a classification network (i.e. a layered transformation from a vector of numbers to an image, as opposed to a transformation from an image to a vector of numbers). If, then, in the training of GANs, these two 'AIs' interact with each other in a dyadic fashion, shouldn't we consider that form of learning... social? This observation can lead to some surprising associations as we compare and contrast GANs with the theories of the sociologist Pierre Bourdieu, whose concept of the so-called habitus is one which is simultaneously cognitive and social: a productive perception in which classification practices and practical action cannot be fully disentangled. Bourdieu had long been concerned with the reproduction of social stratification: his early works studied formal public schooling in France not as an egalitarian system but instead as one which unintentionally maintained the persistence of class distinctions. It was, he argued, through the cultural inculcation of an embodied and partially unconscious habitus---a ""durably installed generative principle of regulated improvisations""---that, he argued, students from the upper classes are given an advantage which is only further reinforced throughout their educational trajectories. For Bourdieu, institutions of schooling instill ""deeply interiorized master patterns"" of behavior and thought (and classification) which in turn direct the acquisition of subsequent patterns, whose character is determined not simply by this cognitive layering but by their actual use in lived practice, especially early in childhood development. In this work I develop a productive analogy between the GAN architecture and Bourdieu's habitus, in three ways. First, I call attention to the fact that connectionist approaches and Bourdieu's theories were both conceived as revolts against rule-bound paradigms. In the 1980s, Rumelhart and McClelland used a multilayer neural network to learn the phonology of English past-tense verbs because ""sometimes we don't follow the rules... language is full of exceptions to the rules""; and in the case of Bourdieu, the habitus was an answer to a long-standing question: ""how can behaviour be regulated without being the product of obedience to rules?"" Bourdieu strove to transgress what was then seen in the social sciences as a conceptual opposition between structure-based theories of social life and those which emphasized an embodied agency. Second, I suggest that concerns about bias and discrimination in machine learning in recent years can in part be attributed due to the increased use of ML models not just for static classification but for practical action. Similarly, the habitus for Bourdieu is simultaneously durable and transposable: its judgments may be relatively stable, but are capable of being deployed dynamically in novel and varying social situations---or what ML practitioners might call generalizability. We can thus theorize generative models (including GANs) as biased not just in their stereotyped classifications, but through their potential for actively generating new biased data. These generated actions then recursively become part of the social arena Bourdieu called the field, into which new agents are 'born' and for which they may know few alternatives. Finally, it is intriguing that GAN researchers and Bourdieu both extensively use metaphors from game theory. Goodfellow described the GAN architecture as a ""two-player minimax game with value function V(G,D)"", meaning that there is a single abstract function whose output value the discriminator is trying to maximize and which the generator is trying to minimize; but the dynamic nature of the GAN training process means that convergence to Nash equilibrium is nontrivial. But for Bourdieu, such a utility-based approach to artistic creation could not be more crude when compared to the social reality of art worlds: utilitarianism is, for him, ""the degree zero of sociology"", by which he means an isolated, inert, and amodal---and therefore not particularly sociological---starting point. Moreover, 19th-century bohemian culture was characterized primarily by its inversion of financial incentives, in which failure is a kind of success, and ""selling out"" (i.e. maximizing profit) worst of all; and thus the relentless optimization of neural networks may be fundamentally at odds with the ""value functions"" of many human artists. I conclude that deep learning, while primarily understood as a scientific and technical achievement, may also intentionally or unintentionally constitute a nascent, independent reinvention of social theory.",['Michael Castelle'],['University of Warwick'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
The Use and Misuse of Counterfactuals in Ethical Machine Learning,"The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.","['Atoosa Kasirzadeh', 'Andrew Smart']","['University of Toronto, Australian National University', 'Google']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Thieves of Sesame Street: Model Extraction on BERT-based APIs,"We study the problem of model extraction in natural language processing, where an adversary with query access to a victim model attempts to reconstruct a local copy of the model. We show that when both the adversary and victim model fine-tune existing pretrained models such as BERT, the adversary does not need to have access to any training data to mount the attack. Indeed, we show that randomly sampled sequences of words, which do not satisfy grammar structures, make effective queries to extract textual models. This is true even for complex tasks such as natural language inference or question answering. ","['Gaurav Singh Tomar', 'Ankur Parikh', 'Nicolas Papernot']",Google,ICLR 2020 (2020),2020,TRUE
This Whole Thing Smacks of Gender: Algorithmic Exclusion in Bioimpedance-based Body Composition Analysis,"Smart weight scales offer bioimpedance-based body composition analysis as a supplement to pure body weight measurement. Companies such as Withings and Fitbit tout composition analysis as providing self-knowledge and the ability to make more informed decisions. However, these aspirational statements elide the reality that these numbers are a product of proprietary regression equations that require a binary sex/gender as their input. Our paper combines transgender studies-influenced personal narrative with an analysis of the scientific basis of bioimpedance technology used as part of the Withings smart scale. Attempting to include nonbinary people reveals that bioelectrical impedance analysis has always rested on physiologically shaky ground. White nonbinary people are merely the tip of the iceberg of those who may find that their smart scale is not so intelligent when it comes to their bodies. Using body composition analysis as an example, we explore how the problem of trans and nonbinary inclusion in personal health tech goes beyond the issues of adding a third ""gender"" box or slapping a rainbow flag on the packaging. We also provide recommendations as to how to approach creating more inclusive technologies even while still relying on exclusionary data.","['Kendra Albert', 'Maggie Delano']","['Harvard Law School Cambridge, MA', 'Swarthmore College, Swarthmore, PA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
TILT: A GDPR-Aligned Transparency Information Language and Toolkit for Practical Privacy Engineering,"In this paper, we present TILT, a transparency information language and toolkit explicitly designed to represent and process transparency information in line with the requirements of the GDPR and allowing for a more automated and adaptive use of such information than established, legalese data protection policies do. We provide a detailed analysis of transparency obligations from the GDPR to identify the expressiveness required for a formal transparency language intended to meet respective legal requirements. In addition, we identify a set of further, non-functional requirements that need to be met to foster practical adoption in real-world (web) information systems engineering. On this basis, we specify our formal language and present a respective, fully implemented toolkit around it. We then evaluate the practical applicability of our language and toolkit and demonstrate the additional prospects it unlocks through two different use cases: a) the inter-organizational analysis of personal data-related practices allowing, for instance, to uncover data sharing networks based on explicitly announced transparency information and b) the presentation of formally represented transparency information to users through novel, more comprehensible, and potentially adaptive user interfaces, heightening data subjects' actual informedness about data-related practices and, thus, their sovereignty. Altogether, our transparency information language and toolkit allow - differently from previous work - to express transparency information in line with actual legal requirements and practices of modern (web) information systems engineering and thereby pave the way for a multitude of novel possibilities to heighten transparency and user sovereignty in practice.","['Elias Grünewald', 'Frank Pallas']","['Technische Universität Berlin, Information Systems Engineering Research Group, Berlin, Germany', 'Technische Universität Berlin, Information Systems Engineering Research Group, Berlin, Germany']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
To Trust Or Not To Trust A Classifier,"Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the &lbrace;\it trust score&rbrace;, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis.",['Been Kim'],Google,NeurIPS (2018),2018,TRUE
Tough Times at Transitional Homeless Shelters: Considering the Impact of Financial Insecurity on Digital Security and Privacy,"Addressing digital security and privacy issues can be particularly difficult for users who face challenging circumstances. We performed semi-structured interviews with residents and staff at 4 transitional homeless shelters in the U.S. San Francisco Bay Area (n=15 residents, 3 staff) to explore their digital security and privacy challenges. Based on these interviews, we outline four tough times themes -- challenges experienced by our financially insecure participants that impacted their digital security and privacy -- which included: (1) limited financial resources, (2) limited access to reliable devices and Internet, (3) untrusted relationships, and (4) ongoing stress. We provide examples of how each theme impacts digital security and privacy practices and needs. We then use these themes to provide a framework outlining opportunities for technology creators to better support users facing security and privacy challenges related to financial insecurity.","['Manya Sleeper', ""Kathleen O'Leary"", 'Anna Turner', 'Jill Palzkill Woelfer', 'Sunny Consolvo']",Google,Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,2019,TRUE
Toward a better trade-off between performance and fairness with kernel-based distribution matching,"As recent literature has demonstrated how classifiers often carry unintended biases toward some subgroups, deploying machine learned models to users demands careful consideration of the social consequences. How should we address this problem in a real-world system? How should we balance core performance and fairness metrics? In this paper, we introduce a MinDiff framework for regularizing classifiers toward different fairness metrics and analyze a technique with kernel-based statistical dependency tests. We run a thorough study on an academic dataset to compare the Pareto frontier achieved by different regularization approaches, and apply our kernel-based method to two large-scale industrial systems demonstrating real-world improvements.","['Flavien Prost', 'Ed H. Chi', 'Alex Beutel']",Google,Neurips (2019),2019,TRUE
Toward Fairness in AI for People with Disabilities: A Research Roadmap,"AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.","['Anhong Guo', 'Ece Kamar', 'Jennifer Wortman Vaughan', 'Hanna Wallach', 'Meredith Ringel Morris']",Microsoft,ASSETS 2019 Workshop on AI Fairness for People with Disabilities,2019-10-27,TRUE
Toward situated interventions for algorithmic equity: lessons from the field,"Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is ""scalable"" beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.","['Michael Katell', 'Meg Young', 'Dharma Dailey', 'Bernease Herman', 'Vivian Guetler', 'Aaron Tam', 'Corinne Bintz', 'Daniella Raz', 'P. M. Krafft']","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'West Virginia University', 'University of Washington', 'Middlebury College', 'University of Michigan', 'University of Oxford and University of Washington']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Towards a critical race methodology in algorithmic fairness,"We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.","['Alex Hanna', 'Emily Denton', 'Andrew Smart', 'Jamila Smith-Loud']","['Google', 'Google', 'Google', 'Google']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Towards a more representative politics in the ethics of computer science,"Ethics curricula in computer science departments should include a focus on the political action of students. While 'ethics' holds significant sway over current discourse in computer science, recent work, particularly in data science, has shown that this discourse elides the underlying political nature of the problems that it aims to solve. In order to avoid these pitfalls---such as co-option, whitewashing, and assumed universal values---we should recognize and teach the political nature of computing technologies, largely through science and technology studies. Education is an essential focus not just intrinsically, but also because computing students end up joining the companies which have outsize impacts on our lives. At those companies, students both have a responsibility to society and agency beyond just engineering decisions, albeit not uniformly. I propose that we move away from strict ethics curricula and include examples of and calls for political action of students and future engineers. Through such examples---calls to action, practitioner reflections, legislative engagement, direct action---we might allow engineers to better recognize both their diverse agencies and responsibilities.",['Jared Moore'],['University of Washington'],"FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Towards A Rigorous Science of Interpretable Machine Learning,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",['Been Kim'],Google,arXiv (2017),2017,TRUE
Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure,"Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.","['Ben Hutchinson', 'Andrew Smart', 'Alex Hanna', 'Emily Denton', 'Christina Greer', 'Oddur Kjartansson', 'Parker Barnes', 'Margaret Mitchell']","['Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
Towards Automatic Concept-based Explanations,"Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are salient for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \emph&lbrace;concept&rbrace; based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent and salient for the neural network's predictions.","['James Wexler', 'Been Kim']",Google,NeurIPS (2019),2019,TRUE
Towards Cross-Lingual Generalization of Translation Gender Bias,"Cross-lingual generalization issues for less explored languages have been broadly tackled in recent NLP studies. In this study, we apply the philosophy on the problem of translation gender bias, which necessarily involves multilingualism and socio-cultural diversity. Beyond the conventional evaluation criteria for the social bias, we aim to put together various aspects of linguistic viewpoints into the measuring process, to create a template that makes evaluation less tilted to specific types of language pairs. With a manually constructed set of content words and template, we check both the accuracy of gender inference and the fluency of translation, for German, Korean, Portuguese, and Tagalog. Inference accuracy and disparate impact, namely the biasedness factors associated with each other, show that the failure of bias mitigation threatens the delicacy of translation. Furthermore, our analyses on each system and language indicate that the translation fluency and inference accuracy are not necessarily correlated. The results implicitly suggest that the amount of available language resources that boost up the performance might amplify the bias cross-linguistically.","['Won Ik Cho', 'Jiwon Kim', 'Jaeyeong Yang', 'Nam Soo Kim']","['Dept. of ECE and INMC, Seoul National University, Seoul, Korea', 'Independent Researcher, Daegu, Korea', 'Dept. of Linguistics, Seoul National University, Seoul, Korea', 'Dept. of ECE and INMC, Seoul National University, Seoul, Korea']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Towards Equitable AI for the Next Billion Users,"In this article, we present research provocations for AI in the Global South, to spur a conversation on the implicit beliefs, biases, and issues that may be normalized in AI. As much of AIâs functioning is still not well understood or fully developed, we believe these critical areas for research are crucial to shaping inclusive AI as it becomes more complex and powerful. We bring our perspectives as HCI researchers and social scientists that work closely with AI researchers. We have started to address some of these areas in our research and invite further explorations from the research community.","['Nithya Sambasivan', 'Jess Scon Holbrook']",Google,ACM interactions (2019),2019,TRUE
Towards Fair Deep Anomaly Detection,"Anomaly detection aims to find instances that are considered unusual and is a fundamental problem of data science. Recently, deep anomaly detection methods were shown to achieve superior results particularly in complex data such as images. Our work focuses on deep one-class classification for anomaly detection which learns a mapping only from the normal samples. However, the non-linear transformation performed by deep learning can potentially find patterns associated with social bias. The challenge with adding fairness to deep anomaly detection is to ensure both making fair and correct anomaly predictions simultaneously. In this paper, we propose a new architecture for the fair anomaly detection approach (Deep Fair SVDD) and train it using an adversarial network to de-correlate the relationships between the sensitive attributes and the learned representations. This differs from how fairness is typically added namely as a regularizer or a constraint. Further, we propose two effective fairness measures and empirically demonstrate that existing deep anomaly detection methods are unfair. We show that our proposed approach can remove the unfairness largely with minimal loss on the anomaly detection performance. Lastly, we conduct an in-depth analysis to show the strength and limitations of our proposed model, including parameter analysis, feature visualization, and run-time analysis.","['Hongjing Zhang', 'Ian Davidson']","['University of California, Davis', 'University of California, Davis']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy,"Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.","['Kaiyu Yang', 'Klint Qinami', 'Li Fei-Fei', 'Jia Deng', 'Olga Russakovsky']","['Princeton University', 'Princeton University', 'Stanford University', 'Princeton University', 'Princeton University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Towards Federated Learning at Scale: System Design,"Federated Learning is a distributed machine learning approach which enables training on a large corpus of data which never needs to leave user devices. We have spent some effort over the last two years building a scalable production system for FL. In this paper, we report about the resulting high-level design, sketching the challenges and the solutions, as well as touching the open problems and future directions.","['K. A. Bonawitz', 'Alex Ingerman', 'Jakub Konečný', 'Stefano Mazzocchi', 'Brendan McMahan', 'Daniel Ramage']",Google,SysML 2019,2019,TRUE
Towards measuring fairness in AI: the Casual Conversations dataset,"This paper introduces a novel dataset to help researchers evaluate their computer vision and audio models for accuracy across a diverse set of age, genders, apparent skin tones and ambient lighting conditions. Our dataset is composed of 3,011 subjects and contains over 45,000 videos, with an average of 15 videos per person. The videos were recorded in multiple U.S. states with a diverse set of adults in various age, gender and apparent skin tone groups. A key feature is that each subject agreed to participate for their likenesses to be used. Additionally, our age and gender annotations are provided by the subjects themselves. A group of trained annotators labeled the subjects’ apparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations for videos recorded in low ambient lighting are also provided. As an application to measure robustness of predictions across certain attributes, we provide a comprehensive study on the top five winners of the DeepFake Detection Challenge (DFDC). Experimental evaluation shows that the winning models are less performant on some specific groups of people, such as subjects with darker skin tones and thus may not generalize to all people. In addition, we also evaluate the state-of-the-art apparent age and gender classification methods. Our experiments provides a through analysis on these models in terms of fair treatment of people from various backgrounds.","['Caner Hazirbas', 'Joanna Bitton', 'Brian Dolhansky', 'Jacqueline Pan', 'Albert Gordo', 'Cristian Canton Ferrer']",Facebook,UNKNOWN,2021-04-08,TRUE
Towards Query Logs For Privacy Studies: On Deriving Search Queries From Questions,"Translating verbose information needs into crisp search queries is a phenomenon that is ubiquitous but hardly understood. Insights into this process could be valuable in several applications, including synthesizing large privacy-friendly query logs from public Web sources which are readily available to the academic research community. In this work, we take a step towards understanding query formulation by tapping into the rich potential of community question answering (CQA) forums. Specifically, we sample natural language (NL) questions spanning diverse themes from the Stack Exchange platform, and conduct a large-scale conversion experiment where crowdworkers submit search queries they would use when looking for equivalent information. We provide a careful analysis of this data, accounting for possible sources of bias during conversion, along with insights into user-specific linguistic patterns and search behaviors. We release a dataset of 7,000 question-query pairs from this study to facilitate further research on query understanding.","['Asia J. Biega', 'Jana Schmidt', 'Rishiraj Saha Roy']",Microsoft,ECIR 2020,2020-04-14,TRUE
Training On-Device Ranking Models from Cross-User Interactions in a Privacy-Preserving Fashion,(See the attached PDF -- a one-page abstract for the upcoming DESIRES 2018 workshop),['Marc Najork'],Google,Proc. of the First Biennial Conference on Design of Experimental Search & Information Retrieval Systems (DESIRES) (2018),2018,TRUE
Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints,"Classifiers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other policy goals. We study the generalization performance for such constrained optimization problems, in terms of how well the constraints are satisfied at evaluation time, given that they are satisfied at training time. To improve generalization, we frame the problem as a two-player game where one player optimizes the model parameters on a training dataset, and the other player enforces the constraints on an independent validation dataset. We build on recent work in two-player constrained optimization to show that if one uses this two-dataset approach, then constraint generalization can be significantly improved. As we illustrate experimentally, this approach works not only in theory, but also in practice.","['Andrew Cotter', 'Serena Wang']",Google,International Conference on Machine Learning (2019),2019,TRUE
Transfer of Machine Learning Fairness across Domains,"If our models are used in new or unexpected cases, do we know if they will make fair predictions? Previously, researchers developed ways to debias a model for a single problem domain. However, this is often not how models are trained and used in practice. For example, labels and demographics (sensitive attributes) are often hard to observe, resulting in auxiliary or synthetic data to be used for training, and proxies of the sensitive attribute to be used for evaluation of fairness. A model trained for one setting may be picked up and used in many others, particularly as is common with pre-training and cloud APIs. Despite the pervasiveness of these complexities, remarkably little work in the fairness literature has theoretically examined these issues. We frame all of these settings as domain adaptation problems: how can we use what we have learned in a source domain to debias in a new target domain, without directly debiasing on the target domain as if it is a completely new problem? We offer new theoretical guarantees of improving fairness across domains, and offer a modeling approach to transfer to data-sparse target domains. We give empirical results validating the theory and showing that these modeling approaches can improve fairness metrics with less data.","['Xuezhi Wang', 'Alex Beutel', 'Ed H. Chi']",Google,(2019) (2019),2019,TRUE
"Transparent, Scrutable and Explainable User Models for Personalized Recommendation","Most recommender systems base their recommendations on implicit or explicit item-level feedback provided by users. These item ratings are combined into a complex user model, which then predicts the suitability of other items. While effective, such methods have limited scrutability and transparency. For instance, if a user's interests change, then many item ratings would usually need to be modified to significantly shift the user's recommendations. Similarly, explaining how the system characterizes the user is impossible, short of presenting the entire list of known item ratings. In this paper, we present a new set-based recommendation technique that permits the user model to be explicitly presented to users in natural language, empowering users to understand recommendations made and improve the recommendations dynamically. While performing comparably to traditional collaborative filtering techniques in a standard static setting, our approach allows users to efficiently improve recommendations. Further, it makes it easier for the model to be validated and adjusted, building user trust and understanding.",['Filip Radlinski'],Google,Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19) (2019),2019,TRUE
Two-Sided Fairness for Repeated Matchings in Two-Sided Markets: A Case Study of a Ride-Hailing Platform,"Ride hailing platforms, such as Uber, Lyft, Ola or DiDi, have traditionally focused on the satisfaction of the passengers, or on boosting successful business transactions. However, recent studies provide a multitude of reasons to worry about the drivers in the ride hailing ecosystem. The concerns range from bad working conditions and worker manipulation to discrimination against minorities. With the sharing economy ecosystem growing, more and more drivers financially depend on online platforms and their algorithms to secure a living. It is pertinent to ask what a fair distribution of income on such platforms is and what power and means the platform has in shaping these distributions.","['Tom Sühr', 'Asia J. Biega', 'Meike Zehlike', 'Krishna P. Gummadi', 'Abhijnan Chakraborty']",Microsoft,KDD 2019,2019-08-04,TRUE
Understanding and correcting pathologies in the training of learned optimizers,"Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned optimizers may similarly outperform current hand-designed optimizers, especially for specific problems. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process. The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance. This allows us to train neural networks to perform optimization of a specific task faster than tuned first-order methods. Moreover, by training the optimizer against validation loss (as opposed to training loss), we are able to learn optimizers that train networks to generalize better than first order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks faster in wall-clock time compared to tuned first-order methods and with an improvement in test loss.","['Luke Metz', 'Niru Maheswaranathan', 'Daniel Freeman', 'Jascha Sohl-dickstein']",Google,ICML (2019),2019,TRUE
Understanding the Effect of Accuracy on Trust in Machine Learning Models,"


We address a relatively under-explored aspect of human–computer interaction: people’s abilities to understand the relationship between a machine learning model’s stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople’s trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model’s stated accuracy on held-out data and on its observed accuracy in practice. We find that people’s trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to re- cent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.


","['Ming Ying', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",Microsoft,2019 ACM CHI Conference on Human Factors in Computing Systems,2019-05-01,TRUE
Upward Max Min Fairness,"Often one would like to allocate shared resources in a fair way. A common and well studied notion of fairness is Max-Min Fairness, where we first maximize the smallest allocation, and subject to that the second smallest, and so on. We consider a networking application where multiple commodities compete over the capacity of a network. In our setting each commodity has multiple possible paths to route its demand (for example, a network using MPLS tunneling). In this setting, the only known way of finding a max-min fair allocation requires an iterative solution of multiple linear programs. Such an approach, although polynomial time, scales badly with the size of the network, the number of demands, and the number of paths. More importantly, a network operator has limited control and understanding of the inner working of the algorithm. Finally, this approach is inherently centralized and cannot be implemented via a distributed protocol. In this paper we introduce Upward Max-Min Fairness, a novel relaxation of Max-Min Fairness and present a family of simple dynamics that converge to it. These dynamics can be implemented in a distributed manner. Moreover, we present an efficient combinatorial algorithm for finding an upward max-min fair allocation, which is a natural extension of the well known Water Filling Algorithm for a multiple path setting. We test the expected behavior of this new algorithm and show that on realistic networks upward max-min fair allocations are comparable to the max-min fair allocations both in fairness and in network utilization.","['Avinatan Hassidim', 'Haim Kaplan', 'Alok Kumar', 'Danny Raz', 'Michal Segalov']",Google,INFOCOM (2012),2012,TRUE
Value Cards: An Educational Toolkit for Teaching Social Impacts of Machine Learning through Deliberation,"Recently, there have been increasing calls for computer science curricula to complement existing technical training with topics related to Fairness, Accountability, Transparency and Ethics (FATE). In this paper, we present Value Cards, an educational toolkit to inform students and practitioners the social impacts of different machine learning models via deliberation. This paper presents an early use of our approach in a college-level computer science course. Through an in-class activity, we report empirical data for the initial effectiveness of our approach. Our results suggest that the use of the Value Cards toolkit can improve students' understanding of both the technical definitions and trade-offs of performance metrics and apply them in real-world contexts, help them recognize the significance of considering diverse social values in the development and deployment of algorithmic systems, and enable them to communicate, negotiate and synthesize the perspectives of diverse stakeholders. Our study also demonstrates a number of caveats we need to consider when using the different variants of the Value Cards toolkit. Finally, we discuss the challenges as well as future applications of our approach.","['Hong Shen', 'Wesley H. Deng', 'Aditi Chattopadhyay', 'Zhiwei Steven Wu', 'Xu Wang', 'Haiyi Zhu']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'University of California, Berkeley Berkeley, CA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'University of Michigan Ann Arbor, MI, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Value-laden disciplinary shifts in machine learning,"As machine learning models are increasingly used for high-stakes decision making, scholars have sought to intervene to ensure that such models do not encode undesirable social and political values. However, little attention thus far has been given to how values influence the machine learning discipline as a whole. How do values influence what the discipline focuses on and the way it develops? If undesirable values are at play at the level of the discipline, then intervening on particular models will not suffice to address the problem. Instead, interventions at the disciplinary-level are required. This paper analyzes the discipline of machine learning through the lens of philosophy of science. We develop a conceptual framework to evaluate the process through which types of machine learning models (e.g. neural networks, support vector machines, graphical models) become predominant. The rise and fall of model-types is often framed as objective progress. However, such disciplinary shifts are more nuanced. First, we argue that the rise of a model-type is self-reinforcing-it influences the way model-types are evaluated. For example, the rise of deep learning was entangled with a greater focus on evaluations in compute-rich and data-rich environments. Second, the way model-types are evaluated encodes loaded social and political values. For example, a greater focus on evaluations in compute-rich and data-rich environments encodes values about centralization of power, privacy, and environmental concerns.","['Ravit Dotan', 'Smitha Milli']","['University of California', 'University of California']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
WCMP: Weighted Cost Multipathing for Improved Fairness in Data Centers,"Data Center topologies employ multiple paths among servers to deliver scalable, cost-effective network capacity. The simplest and the most widely deployed approach for load balancing among these paths, Equal Cost Multipath (ECMP), hashes flows among the shortest paths toward a destination. ECMP leverages uniform hashing of balanced flow sizes to achieve fairness and good load balancing in data centers. However, we show that ECMP further assumes a balanced, regular, and fault-free topology, which are invalid assumptions in practice that can lead to substantial performance degradation and, worse, variation in flow bandwidths even for same size flows.","['Junlan Zhou', 'Leon Poutievski', 'Amin Vahdat']",Google,EuroSys '14: Proceedings of the Ninth European Conference on Computer Systems (2014),2014,TRUE
"What does it mean to 'solve' the problem of discrimination in hiring?: social, technical and legal perspectives from the UK on automated hiring systems","Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective. In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.","['Javier Sánchez-Monedero', 'Lina Dencik', 'Lilian Edwards']","['Cardiff University, Cardiff, Wales, United Kingdom', 'Cardiff University, Cardiff, Wales, United Kingdom', 'University of Newcastle, Newcastle upon Tyne, England, United Kingdom']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
What is Fair?  Exploring Pareto-Efficiency for Fairness Constraint Classifiers,"The potential for learned models to amplify existing societal biases has been broadly recognized. Fairness-aware classifier constraints, which apply equality metrics of performance across subgroups defined on sensitive attributes such as race and gender, seek to rectify inequity but can yield non-uniform degradation in performance for skewed datasets.  In certain domains, imbalanced degradation of performance can yield another form of unintentional bias. In the spirit of constructing fairness-aware algorithms as societal imperative, we explore an alternative: Pareto-Efficient Fairness (PEF).  PEF identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane, maximizing multiple subgroup accuracies. Empirically we demonstrate that PEF increases performance of all subgroups in several UCI datasets.","['Alyssa Whitlock Lees', 'Chris Welty']",Google,arxiv (2019),2019,TRUE
What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability,"As research on algorithms and their impact proliferates, so do calls for scrutiny/accountability of algorithms. A systematic review of the work that has been done in the field of 'algorithmic accountability' has so far been lacking. This contribution puts forth such a systematic review, following the PRISMA statement. 242 English articles from the period 2008 up to and including 2018 were collected and extracted from Web of Science and SCOPUS, using a recursive query design coupled with computational methods. The 242 articles were prioritized and ordered using affinity mapping, resulting in 93 'core articles' which are presented in this contribution. The recursive search strategy made it possible to look beyond the term 'algorithmic accountability'. That is, the query also included terms closely connected to the theme (e.g. ethics and AI, regulation of algorithms). This approach allows for a perspective not just from critical algorithm studies, but an interdisciplinary overview drawing on material from data studies to law, and from computer science to governance studies. To structure the material, Bovens's widely accepted definition of accountability serves as a focal point. The material is analyzed on the five points Bovens identified as integral to accountability: its arguments on (1) the actor, (2) the forum, (3) the relationship between the two, (3) the content and criteria of the account, and finally (5) the consequences which may result from the account. The review makes three contributions. First, an integration of accountability theory in the algorithmic accountability discussion. Second, a cross-sectoral overview of the that same discussion viewed in light of accountability theory which pays extra attention to accountability risks in algorithmic systems. Lastly, it provides a definition of algorithmic accountability based on accountability theory and algorithmic accountability literature.",['Maranke Wieringa'],"['Utrecht University, Utrecht, The Netherlands']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
"What We Can't Measure, We Can't Understand: Challenges to Demographic Data Procurement in the Pursuit of Fairness","As calls for fair and unbiased algorithmic systems increase, so too does the number of individuals working on algorithmic fairness in industry. However, these practitioners often do not have access to the demographic data they feel they need to detect bias in practice. Even with the growing variety of toolkits and strategies for working towards algorithmic fairness, they almost invariably require access to demographic attributes or proxies. We investigated this dilemma through semi-structured interviews with 38 practitioners and professionals either working in or adjacent to algorithmic fairness. Participants painted a complex picture of what demographic data availability and use look like on the ground, ranging from not having access to personal data of any kind to being legally required to collect and use demographic data for discrimination assessments. In many domains, demographic data collection raises a host of difficult questions, including how to balance privacy and fairness, how to define relevant social categories, how to ensure meaningful consent, and whether it is appropriate for private companies to infer someone's demographics. Our research suggests challenges that must be considered by businesses, regulators, researchers, and community groups in order to enable practitioners to address algorithmic bias in practice. Critically, we do not propose that the overall goal of future work should be to simply lower the barriers to collecting demographic data. Rather, our study surfaces a swath of normative questions about how, when, and whether this data should be procured, and, in cases where it is not, what should still be done to mitigate bias.","['McKane Andrus', 'Elena Spitzer', 'Jeffrey Brown', 'Alice Xiang']","['Partnership on AI', 'Partnership on AI', 'Partnership on AI, Minnesota State University, Mankato', 'Sony AI, Partnership on AI']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,TRUE
What's sex got to do with machine learning?,"The debate about fairness in machine learning has largely centered around competing substantive definitions of what fairness or nondiscrimination between groups requires. However, very little attention has been paid to what precisely a group is. Many recent approaches have abandoned observational, or purely statistical, definitions of fairness in favor of definitions that require one to specify a causal model of the data generating process. The implicit ontological assumption of these exercises is that a racial or sex group is a collection of individuals who share a trait or attribute, for example: the group ""female"" simply consists in grouping individuals who share female-coded sex features. We show this by exploring the formal assumption of modularity in causal models using directed acyclic graphs (DAGs), which hold that the dependencies captured by one causal pathway are invariant to interventions on any other causal pathways. Modeling sex, for example, as a node in a causal model aimed at elucidating fairness questions proposes two substantive claims: 1) There exists a feature, sex-on-its-own, that is an inherent trait of an individual that then (causally) brings about social phenomena external to it in the world; and 2) the relations between sex and its downstream effects can be modified in whichever ways and the former node would still retain the meaning that sex has in our world. Together, these claims suggest sex to be a category that could be different in its (causal) relations with other features of our social world via hypothetical interventions yet still mean what it means in our world. This fundamental stability of categories and causes (unless explicitly intervened on) is essential in the methodology of causal inference, because without it, causal operations can alter the meaning of a category, fundamentally change how it is situated within a causal diagram, and undermine the validity of any inferences drawn on the diagram as corresponding to any real phenomena in the world. We argue that these methods' ontological assumptions about social groups such as sex are conceptual errors. Many of the ""effects"" that sex purportedly ""causes"" are in fact constitutive features of sex as a social status. They constitute what it means to be sexed. In other words, together, they give the social meaning of sex features. These social meanings are precisely, we argue, what makes sex discrimination a distinctively morally problematic type of act that differs from mere irrationality or meanness on the basis of a physical feature. Correcting this conceptual error has a number of important implications for how analytical models can be used to detect discrimination. If what makes something discrimination on the basis of a particular social grouping is that the practice acts on what it means to be in that group in a way that we deem wrongful, then what we need from analytical diagrams is a model of what constitutes the social grouping. Such a model would allow us to explain the special moral (and legal) reasons we have to be concerned with the treatment of this category by reference to the empirical social relations and meanings that establish the category as what it is. Only then can we have the normative debate about what is fair or nondiscriminatory vis-à-vis that group. We suggest that formal diagrams of constitutive relations would present an entirely different path toward reasoning about discrimination (and relatedly, counterfactuals) because they proffer a model of how the meaning of a social group emerges from its constitutive features. Whereas the value of causal diagrams is to guide the construction and testing of sophisticated modular counterfactuals, the value of constitutive diagrams would be to identify a different kind of counterfactual as central to our inquiry into discrimination: one that asks how the social meaning of a group would be changed if its non-modular features were altered.","['Lily Hu', 'Issa Kohler-Hausmann']","['Harvard University', 'Yale University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
What’s in a Name? Reducing Bias in Bios without Access to Protected Attributes,"There is a growing body of work that proposes methods for mitigating bias in machine learning systems. These methods typically rely on access to protected attributes such as race, gender, or age. However, this raises two significant challenges: (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classiﬁcation, we propose a method for discouraging correlation between the predicted probability of an individual’s true occupation and a word embedding of their name. This method leverages the societal biases that are encoded in word embeddings, eliminating the need for access to protected attributes. Crucially, it only requires access to individuals’ names at training time and not at deployment time. We evaluate two variations of our proposed method using a large-scale data set of online biographies. We ﬁnd that both variations simultaneously reduce race and gender biases, with almost no reduction in the classiﬁer’s overall true positive rate.","['Alexey Romanov', 'Maria De-Arteaga', 'Hanna Wallach', 'Jennifer Chayes', 'Christian Borgs', 'Alexandra Chouldechova', 'Sahin Geyik', 'krishnaram Kenthapadi', 'Anna Rumshisky', 'Adam Tauman Kalai']",Microsoft,NAACL 2019,2019-04-10,TRUE
When Are Search Completion Suggestions Problematic?,"Problematic web search query completion suggestions—perceived as biased, offensive, or in some other way harmful—can reinforce existing stereotypes and misbeliefs, and even nudge users towards undesirable patterns of behavior.  Locating such suggestions is difficult, not only due to the long-tailed nature of web search, but also due to differences in how people assess potential harms.  Grounding our study in web search query logs, we explore when system-provided suggestions might be perceived as problematic through a series of crowd-experiments where we systematically manipulate: the search query fragments provided by users,  possible user search intents, and the list of query completion suggestions.  To examine why query suggestions might be perceived as problematic, we contrast them to an inventory of known types of problematic suggestions. We report our observations around differences in the prevalence of  a) suggestions that are problematic on their own versus  b) suggestions that are problematic for the query fragment provided by a user, for both common informational needs and in the presence of web search voids—topics searched by few to no users.  Our experiments surface a rich array of scenarios where suggestions are considered problematic, including due to the context in which they were surfaced. Compounded by the elusive nature of many such scenarios, the prevalence of suggestions perceived as problematic only for certain user inputs, raises concerns about blind spots due to data annotation practices that may lead to some types of problematic suggestions being overlooked.","['Alexandra Olteanu', 'Fernando Diaz', 'Gabriella Kazai']",Microsoft,Computer Supported Collaborative Work and Social Computing (CSCW),2020-08-01,TRUE
"When not to design, build, or deploy","Recent debate within the FAT* community has focused on how the field conceptualizes the problems it seeks to address, what approach the field should take in attempting to address these problems, and whether the field should even pursue some of the proposed remedies. Questions regarding when not to design, build, or deploy a technology are perhaps the most common expression of this trend. Identifying the problems to address is inextricably linked to the broader question of how to collectively make decisions about what technologies our societies need and want.","['Solon Barocas', 'Asia J. Biega', 'Benjamin Fish', 'Jędrzej Niklas', 'Luke Stark']",Microsoft,"2020 Conference on Fairness, Accountability, and Transparency (FAT* '20)",2020-01-01,TRUE
When the Umpire is also a Player: Bias in Private Label Product Recommendations on E-commerce Marketplaces,"Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about the fairness of black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own 'private label' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace 'organic' recommendations with ad-driven 'sponsored' recommendations, which include their own private labels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any public audit of these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematic audit of related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazon private label products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.","['Abhisek Dash', 'Abhijnan Chakraborty', 'Saptarshi Ghosh', 'Animesh Mukherjee', 'Krishna P. Gummadi']","['Indian Institute of Technology, Kharagpur, India', 'Indian Institute of Technology, Delhi, India', 'Indian Institute of Technology, Kharagpur, India', 'Indian Institute of Technology, Kharagpur, India', 'Max Planck Institute for Software, Systems, Germany']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE
Who's the Guinea Pig?: Investigating Online A/B/n Tests in-the-Wild,"A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization. These tests are often run on their websites without explicit consent from users. In this paper, we investigate such online A/B/n tests by using Optimizely as a lens. First, we provide measurement results of 575 websites that use Optimizely drawn from the Alexa Top-1M, and analyze the distributions of their audiences and experiments. Then, we use three case studies to discuss potential ethical pitfalls of such experiments, including involvement of political content, price discrimination, and advertising campaigns. We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators.","['Shan Jiang', 'John Martin', 'Christo Wilson']","['Northeastern University', 'Northeastern University', 'Northeastern University']","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",January 2019,FALSE
"Whose side are ethics codes on?: power, responsibility and the social good","The moral authority of ethics codes stems from an assumption that they serve a unified society, yet this ignores the political aspects of any shared resource. The sociologist Howard S. Becker challenged researchers to clarify their power and responsibility in the classic essay: Whose Side Are We On. Building on Becker's hierarchy of credibility, we report on a critical discourse analysis of data ethics codes and emerging conceptualizations of beneficence, or the ""social good"", of data technology. The analysis revealed that ethics codes from corporations and professional associations conflated consumers with society and were largely silent on agency. Interviews with community organizers about social change in the digital era supplement the analysis, surfacing the limits of technical solutions to concerns of marginalized communities. Given evidence that highlights the gulf between the documents and lived experiences, we argue that ethics codes that elevate consumers may simultaneously subordinate the needs of vulnerable populations. Understanding contested digital resources is central to the emerging field of public interest technology. We introduce the concept of digital differential vulnerability to explain disproportionate exposures to harm within data technology and suggest recommendations for future ethics codes..","['Anne L. Washington', 'Rachel Kuo']","['New York University', 'New York University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Whose tweets are surveilled for the police: an audit of a social-media monitoring tool via log files,"Social media monitoring by law enforcement is becoming commonplace, but little is known about what software packages for it do. Through public records requests, we obtained log files from the Corvallis (Oregon) Police Department's use of social media monitoring software called DigitalStakeout. These log files include the results of proprietary searches by DigitalStakeout that were running over a period of 13 months and include 7240 social media posts. In this paper, we focus on the Tweets logged in this data and consider the racial and ethnic identity (through manual coding) of the users that are therein flagged by DigitalStakeout. We observe differences in the demographics of the users whose Tweets are flagged by DigitalStakeout compared to the demographics of the Twitter users in the region, however, our sample size is too small to determine significance. Further, the demographics of the Twitter users in the region do not seem to reflect that of the residents of the region, with an apparent higher representation of Black and Hispanic people. We also reconstruct the keywords related to a Narcotics report set up by DigitalStakeout for the Corvallis Police Department and find that these keywords flag Tweets unrelated to narcotics or flag Tweets related to marijuana, a drug that is legal for recreational use in Oregon. Almost all of the keywords have a common meaning unrelated to narcotics (e.g. broken, snow, hop, high) that call into question the utility that such a keyword based search could have to law enforcement. As social media monitoring is increasingly used for law enforcement purposes, racial biases in surveillance may contribute to existing racial disparities in law enforcement practices. We are hopeful that log files obtainable through public records request will shed light on the operation of these surveillance tools. There are challenges in auditing these tools: public records requests may go unfulfilled even if the data is available, social media platforms may not provide comparable data for comparison with surveillance data, demographics can be difficult to ascertain from social media and Institutional Review Boards may not understand how to weigh the ethical considerations involved in this type of research. We include in this paper a discussion of our experience in navigating these issues.","['Glencora Borradaile', 'Brett Burkhardt', 'Alexandria LeClerc']","['Oregon State University', 'Oregon State University', 'Oregon State University']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,FALSE
Why does my model fail?: contrastive local explanations for retail forecasting,"In various business settings, there is an interest in using more complex machine learning techniques for sales forecasting. It is difficult to convince analysts, along with their superiors, to adopt these techniques since the models are considered to be ""black boxes,"" even if they perform better than current models in use. We examine the impact of contrastive explanations about large errors on users' attitudes towards a ""black-box"" model. We propose an algorithm, Monte Carlo Bounds for Reasonable Predictions. Given a large error, MC-BRP determines (1) feature values that would result in a reasonable prediction, and (2) general trends between each feature and the target, both based on Monte Carlo simulations. We evaluate on a real dataset with real users by conducting a user study with 75 participants to determine if explanations generated by MC-BRP help users understand why a prediction results in a large error, and if this promotes trust in an automatically-learned model. Our study shows that users are able to answer objective questions about the model's predictions with overall 81.1% accuracy when provided with these contrastive explanations. We show that users who saw MC-BRP explanations understand why the model makes large errors in predictions significantly more than users in the control group. We also conduct an in-depth analysis of the difference in attitudes between Practitioners and Researchers, and confirm that our results hold when conditioning on the users' background.","['Ana Lucic', 'Hinda Haned', 'Maarten de Rijke']","['University of Amsterdam, Amsterdam, Netherlands', 'Ahold Delhaize, Zaandam, Netherlands', 'University of Amsterdam, Amsterdam, Netherlands']","FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",January 2020,TRUE
Why Reliabilism Is Not Enough:Epistemic and Moral Justification in Machine Learning,"In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed widespread adoption of machine learning? We argue that, in general, people implicitly adopt reliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method. We argue that, in cases where model deployments require moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral âwrapperâ around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justificationâmoral justification. Finally, we offer cautions relevant to the (implicit or explicit)adoption of the reliabilist interpretation of machine learning.",['Ben Hutchinson'],Google,AIES 2020 (2020),2020,TRUE
You Can't Sit With Us: Exclusionary Pedagogy in AI Ethics Education,"Given a growing concern about the lack of ethical consideration in the Artificial Intelligence (AI) field, many have begun to question how dominant approaches to the disciplinary education of computer science (CS)---and its implications for AI---has led to the current ""ethics crisis"". However, we claim that the current AI ethics education space relies on a form of ""exclusionary pedagogy,"" where ethics is distilled for computational approaches, but there is no deeper epistemological engagement with other ways of knowing that would benefit ethical thinking or an acknowledgement of the limitations of uni-vocal computational thinking. This results in indifference, devaluation, and a lack of mutual support between CS and humanistic social science (HSS), elevating the myth of technologists as ""ethical unicorns"" that can do it all, though their disciplinary tools are ultimately limited. Through an analysis of computer science education literature and a review of college-level course syllabi in AI ethics, we discuss the limitations of the epistemological assumptions and hierarchies of knowledge which dictate current attempts at including ethics education in CS training and explore evidence for the practical mechanisms through which this exclusion occurs. We then propose a shift towards a substantively collaborative, holistic, and ethically generative pedagogy in AI education.","['Inioluwa Deborah Raji', 'Morgan Klaus Scheuerman', 'Razvan Amironesei']","['Mozilla Foundation', 'Information Science, University of Colorado Boulder', 'CADE, University of San Francisco']","FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",March 2021,FALSE