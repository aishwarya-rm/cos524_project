{'abstract': 'Despite a growing literature on creating interpretable machine learning methods, there have been few experimental studies of their effects on end users. We present a series of large-scale, randomized, pre-registered experiments in which participants were shown functionally identical models that varied only in two factors thought to influence interpretability: the number of input features and the model transparency (clear or black-box). Participants who were shown a clear model with a small number of features were better able to simulate the model’s predictions. However, contrary to what one might expect when manipulating interpretability, we found no significant difference in multiple measures of trust across conditions. Even more surprisingly, increased transparency hampered people’s ability to detect when a model has made a sizeable mistake. These findings emphasize the importance of studying how models are presented to people and empirically verifying that interpretable models achieve their intended effects on end users.', 'title': 'Manipulating and Measuring Model Interpretability - Microsoft Research', 'authors': ['Forough Poursabzi-Sangdeh', 'Dan Goldstein', 'Jake Hofman', 'Jennifer Wortman Vaughan', 'Hanna Wallach'], 'date': '2021-05-20', 'conference': 'CHI 2021'}
{'abstract': "In email interfaces, providing users with reply suggestions may simplify or accelerate correspondence.\xa0 While the “success'” of such systems is typically quantified using the number of suggestions selected by users, this ignores the impact of social context, which can change how suggestions are perceived.\xa0 To address this, we developed a mixed-methods framework involving qualitative interviews and crowdsourced experiments to characterize problematic email reply suggestions.\xa0 Our interviews revealed issues with over-positive, dissonant, cultural, and gender-assuming replies, as well as contextual politeness.\xa0 In our experiments, crowdworkers assessed email scenarios that we generated and systematically controlled, showing that contextual factors like social ties and the presence of salutations impacts users’ perceptions of email correspondence.\xa0 These assessments created a novel dataset of human-authored corrections for problematic email replies. Our study highlights the social complexity of providing suggestions for email correspondence, raising issues that may apply to all social messaging systems.", 'title': '"I Can’t Reply with That": Characterizing Problematic Email Reply Suggestions - Microsoft Research', 'authors': ['Ronald Robertson', 'Alexandra Olteanu', 'Fernando Diaz', 'Milad Shokouhi', 'Peter Bailey'], 'date': '2021-05-01', 'conference': 'CHI Conference on Human Factors in Computing Systems (CHI ’21)'}
{'abstract': 'NeurIPS 2020 requested that research paper submissions include impact statements on ‘potential nefarious uses and the consequences of failure.’ When researching, designing, and implementing systems, a key challenge to anticipating risks, however, is to overcome what Clarke (1962) called ‘failures of imagination.’ The growing research on bias, fairness, and transparency in computational systems aims to illuminate and mitigate harms, and could thus help inform reflections on possible negative impacts of particular pieces of technical work. The prevalent notion of computational harms — narrowly construed as either allocational or representational harms — does not fully capture the context dependent and unobservable nature of harms across the wide range of AI infused systems. The current literature primarily addresses only a small range of examples of harms to motivate algorithmic fixes, overlooking the wider scope of probable harms and the way these harms may affect different stakeholders. The system affordances and possible usage scenarios may also exacerbate harms in unpredictable ways, as they determine stakeholders’ control (including non-users) over how they interact with a system output. To effectively assist in anticipating and identifying harmful uses, we argue that frameworks of harms must be context-aware and consider a wider range of potential stakeholders, system affordances, uses, and outputs, as well as viable proxies for assessing harms in the widest sense.', 'title': 'Overcoming Failures of Imagination in AI Infused System Development and Deployment - Microsoft Research', 'authors': ['Margarita Boyarskaya', 'Alexandra Olteanu', 'Kate Crawford'], 'date': '2020-12-01', 'conference': 'In the Navigating the Broader Impacts of AI Research Workshop at NeurIPS 2020'}
{'abstract': 'Problematic web search query completion suggestions—perceived as biased, offensive, or in some other way harmful—can reinforce existing stereotypes and misbeliefs, and even nudge users towards undesirable patterns of behavior.\xa0 Locating such suggestions is difficult, not only due to the long-tailed nature of web search, but also due to differences in how people assess potential harms.\xa0 Grounding our study in web search query logs, we explore when system-provided suggestions might be perceived as problematic through a series of crowd-experiments where we systematically manipulate: the search query fragments provided by users,\xa0 possible user search intents, and the list of query completion suggestions.\xa0 To examine why query suggestions might be perceived as problematic, we contrast them to an inventory of known types of problematic suggestions. We report our observations around differences in the prevalence of\xa0 a) suggestions that are problematic on their own versus\xa0 b) suggestions that are problematic for the query fragment provided by a user, for both common informational needs and in the presence of web search voids—topics searched by few to no users.\xa0 Our experiments surface a rich array of scenarios where suggestions are considered problematic, including due to the context in which they were surfaced. Compounded by the elusive nature of many such scenarios, the prevalence of suggestions perceived as problematic only for certain user inputs, raises concerns about blind spots due to data annotation practices that may lead to some types of problematic suggestions being overlooked.', 'title': 'When Are Search Completion Suggestions Problematic? - Microsoft Research', 'authors': ['Alexandra Olteanu', 'Fernando Diaz', 'Gabriella Kazai'], 'date': '2020-08-01', 'conference': 'Computer Supported Collaborative Work and Social Computing (CSCW)'}
{'abstract': 'Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established “principal reason” explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant—and withholding others.', 'title': 'The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons - Microsoft Research', 'authors': ['Solon Barocas', 'Andrew Selbst', 'Manish Raghavan'], 'date': '2020-08-01', 'conference': 'FAT* 2020'}
{'abstract': 'There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.', 'title': 'Mitigating bias in algorithmic hiring: evaluating claims and practices - Microsoft Research', 'authors': ['Manish Raghavan', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy'], 'date': '2020-08-01', 'conference': 'FAT* 2020'}
{'abstract': 'Article 5(1)(c) of the European Union’s General Data Protection Regulation (GDPR) requires that “personal data shall be […] adequate, relevant, and limited to what is necessary in relation to the purposes for which they are processed (‘data minimisation’)”. To date, the legal and computational definitions of ‘purpose limitation’ and ‘data minimization’ remain largely unclear. In particular, the interpretation of these principles is an open issue for information access systems that optimize for user experience through personalization and do not strictly require personal data collection for the delivery of basic service.', 'title': 'Operationalizing the Legal Principle of Data Minimization for Personalization - Microsoft Research', 'authors': ['Asia J. Biega', 'Peter Potash', 'Hal Daumé III', 'Fernando Diaz', 'Michèle Finck'], 'date': '2020-07-28', 'conference': 'SIGIR 2020'}
{'abstract': 'We introduce the concept of expected exposure as the average attention ranked items receive from users over repeated samples of the same query. Furthermore, we advocate for the adoption of the principle of equal expected exposure: given a fixed information need, no item receive more or less expected exposure compared to any other item of the same relevance grade. We argue that this principle is desirable for many retrieval objectives and scenarios, including topical diversity and fair ranking. Leveraging user models from existing retrieval metrics, we propose a general evaluation methodology based on expected exposure and draw connections to related metrics in information retrieval evaluation. Importantly, this methodology relaxes classic information retrieval assumptions, allowing a system, in response to a query, to produce a distribution over rankings instead of a single fixed ranking. We study the behavior of the expected exposure metric and stochastic rankers across a variety of information access conditions, including ad hoc retrieval and recommendation. We believe that measuring and optimizing expected exposure metrics using randomization opens a new area for retrieval algorithm development and progress.', 'title': 'Evaluating Stochastic Rankings with Expected Exposure - Microsoft Research', 'authors': ['Fernando Diaz', 'Bhaskar Mitra', 'Michael D. Ekstrand', 'Asia J. Biega', 'Ben Carterette'], 'date': '2020-07-01', 'conference': 'Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM)'}
{'abstract': 'Abstract', 'title': 'Language (Technology) is Power: A Critical Survey of “Bias” in NLP - Microsoft Research', 'authors': ['Su Lin Blodgett', 'Solon Barocas', 'Hal Daumé III', 'Hanna Wallach'], 'date': '2020-06-01', 'conference': 'ACL'}
{'abstract': 'This Article offers a comprehensive survey of privacy dependencies—the many ways that our privacy depends on the decisions and disclosures of other people. What\nwe do and what we say can reveal as much about others as it does about ourselves, even when we don’t realize it or when we think we’re sharing information about ourselves alone.\nWe identify three bases upon which our privacy can depend: our social ties, our similarities to others, and our differences from others. In a tie-based dependency, an observer learns\nabout one person by virtue of her social relationships with others—family, friends, or other associates. In a similarity-based dependency, inferences about our unrevealed attributes are\ndrawn from our similarities to others for whom that attribute is known. And in difference-based dependencies, revelations about ourselves demonstrate how we are different from\nothers—by showing, for example, how we “break the mold” of normal behavior or establishing how we rank relative to others with respect to some desirable attribute. We\nelaborate how these dependencies operate, isolating the relevant mechanisms and providing concrete examples of each mechanism in practice, the values they implicate, and the legal\nand technical interventions that may be brought to bear on them. Our work adds to a growing chorus demonstrating that privacy is neither an individual choice nor an individual value—\nbut it is the first to systematically demonstrate how different types of dependencies can raise very different normative concerns, implicate different areas of law, and create different\nchallenges for regulation.', 'title': 'Privacy Dependencies - Microsoft Research', 'authors': ['Solon Barocas', 'Karen Levy'], 'date': '2020-06-01', 'conference': 'Washington Law Review'}
{'abstract': 'Translating verbose information needs into crisp search queries is a phenomenon that is ubiquitous but hardly understood. Insights into this process could be valuable in several applications, including synthesizing large privacy-friendly query logs from public Web sources which are readily available to the academic research community. In this work, we take a step towards understanding query formulation by tapping into the rich potential of community question answering (CQA) forums. Specifically, we sample natural language (NL) questions spanning diverse themes from the Stack Exchange platform, and conduct a large-scale conversion experiment where crowdworkers submit search queries they would use when looking for equivalent information. We provide a careful analysis of this data, accounting for possible sources of bias during conversion, along with insights into user-specific linguistic patterns and search behaviors. We release a dataset of 7,000 question-query pairs from this study to facilitate further research on query understanding.', 'title': 'Towards Query Logs For Privacy Studies: On Deriving Search Queries From Questions - Microsoft Research', 'authors': ['Asia J. Biega', 'Jana Schmidt', 'Rishiraj Saha Roy'], 'date': '2020-04-14', 'conference': 'ECIR 2020'}
{'abstract': 'Employers are increasingly using information and communication technologies to monitor employees. Such workplace surveillance is extensive in the United States, but its experience and potential consequences differ across groups based on gender. We thus sought to identify whether self‐reported male and female employees differ in the extent to which they find the use of workplace cameras equipped with facial recognition technology (FRT) acceptable, and examine the role of privacy attitudes more generally in mediating views on workplace surveillance. Using data from a nationally representative survey conducted by the Pew Research Center, we find that women are much less likely than men to approve of the use of cameras using FRT in the workplace. We then further explore whether men and women think differently about privacy, and if perceptions of privacy moderate the relationship between gender and approval of workplace surveillance. Finally, we consider the implications of these findings for privacy and surveillance via embedded technologies, and how the consequences of surveillance and technologies like FRT may be gendered. Note: We recognize evaluations based on a binary definition of gender are invariably partial and exclusionary. As we note in our discussion of the study’s limitations, we were constrained by the survey categories provided by Pew.', 'title': "“I Don't Want Someone to Watch Me While I'm Working”: Gendered Views of Facial Recognition Technology in Workplace Surveillance - Microsoft Research", 'authors': ['Luke Stark', 'Amanda Stanhaus', 'Denise L. Anthony'], 'date': '2020-03-10', 'conference': 'Journal of the Association for Information Science and Technology'}
{'abstract': 'Many organizations have published principles intended to guide the ethical development and deployment of AI systems; however, their abstract nature makes them difficult to operationalize. Some organizations have therefore produced AI ethics checklists, as well as checklists for more specific concepts, such as fairness, as applied to AI systems. But unless checklists are grounded in practitioners’ needs, they may be misused. To understand the role of checklists in AI ethics, we conducted an iterative co-design process with 48 practitioners, focusing on fairness. We co-designed an AI fairness checklist and identified desiderata and concerns for AI fairness checklists in general. We found that AI fairness checklists could provide organizational infrastructure for formalizing ad-hoc processes and empowering individual advocates. We discuss aspects of organizational culture that may impact the efficacy of such checklists, and highlight future research directions.', 'title': 'Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI - Microsoft Research', 'authors': ['Michael Madaio', 'Luke Stark', 'Jennifer Wortman Vaughan', 'Hanna Wallach'], 'date': '2020-03-01', 'conference': 'CHI Conference on Human Factors in Computing Systems'}
{'abstract': 'Recent debate within the FAT* community has focused on how the field conceptualizes the problems it seeks to address, what approach the field should take in attempting to address these problems, and whether the field should even pursue some of the proposed remedies. Questions regarding when not to design, build, or deploy a technology are perhaps the most common expression of this trend. Identifying the problems to address is inextricably linked to the broader question of how to collectively make decisions about what technologies our societies need and want.', 'title': 'When not to design, build, or deploy - Microsoft Research', 'authors': ['Solon Barocas', 'Asia J. Biega', 'Benjamin Fish', 'Jędrzej Niklas', 'Luke Stark'], 'date': '2020-01-01', 'conference': "2020 Conference on Fairness, Accountability, and Transparency (FAT* '20)"}
{'abstract': 'A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems — roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a\xa0diagnostic, helping us to understand and measure social problems with precision and clarity. As a\xa0formalizer, computing shapes how social problems are explicitly defined — changing how those problems, and possible responses to them, are understood. Computing serves as\xa0rebuttal\xa0when it illuminates the boundaries of what is possible through technical means. And computing acts as\xa0synecdoche\xa0when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing’s capacity to solve social problems on its own.', 'title': 'Roles for computing in social change - Microsoft Research', 'authors': ['Rediet Abebe', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy', 'Manish Raghavan', 'David Robinson'], 'date': '2020-01-01', 'conference': 'FAT*'}
{'abstract': 'The purpose of the SIGIR 2019 workshop on Fairness, Accountability, Confidentiality, Transparency, and Safety (FACTS-IR) was to explore challenges in responsible information retrieval system development and deployment. To this end, the workshop aimed to crowdsource from the larger SIGIR community and draft an actionable research agenda on five key dimensions of responsible information retrieval: fairness, accountability, confidentiality, transparency, and safety. Such an agenda can guide others in the community that are interested in pursuing FACTS-IR research, as well as inform potential funders about relevant research avenues. The workshop brought together a diverse set of researchers and practitioners interested in contributing to the development of a technical research agenda for responsible information retrieval.', 'title': 'FACTS-IR: Fairness, Accountability, Confidentiality, Transparency, and Safety in Information Retrieval - Microsoft Research', 'authors': ['Alexandra Olteanu', 'Jean Garcia-Gathright', 'Maarten de Rijke', 'Michael D. Ekstrand'], 'date': '2019-12-02', 'conference': 'ACM SIGIR Forum'}
{'abstract': 'Automated decision making has become widespread in recent years, largely due to advances in machine learning. As a result of this trend, machine learning systems are increasingly used to make decisions in high-stakes domains, such as employment or university admissions. The weightiness of these decisions has prompted the realization that, like humans, machines must also comply with the law. But human decision-making processes are quite different from automated decision-making processes, which creates a mismatch between laws and the decision makers to which they are intended to apply. In turn, this mismatch can lead to counterproductive outcomes.', 'title': "Stretching Human Laws to Apply to Machines: The Dangers of a 'Colorblind' Computer - Microsoft Research", 'authors': ['Zach Harned', 'Hanna Wallach'], 'date': '2019-12-01', 'conference': 'Florida State University Law Review, Forthcoming'}
{'abstract': '\n\n\nThe goal of the TREC Fair Ranking track was to develop a benchmark for evaluating retrieval systems in terms of fairness to different content providers in addition to classic notions of relevance. As part of the benchmark, we defined standardized fairness metrics with evaluation protocols and released a dataset for the fair ranking problem. The 2019 task focused on reranking academic paper abstracts given a query. The objective was to fairly represent relevant authors from several groups that were unknown at the system submission time. Thus, the track emphasized the development of systems which have robust performance across a variety of group definitions. Participants were provided with querylog data (queries, documents, and relevance) from Semantic Scholar. This paper presents an overview of the track, including the task definition, descriptions of the data and the annotation process, as well as a comparison of the performance of submitted systems.\n\n\n', 'title': 'Overview of the TREC 2019 Fair Ranking Track - Microsoft Research', 'authors': ['Asia J. Biega', 'Fernando Diaz', 'Michael D. Ekstrand', 'Sebastian Kohlmeier'], 'date': '2019-11-01', 'conference': 'TREC 2019'}
{'abstract': 'AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.', 'title': 'Toward Fairness in AI for People with Disabilities: A Research Roadmap - Microsoft Research', 'authors': ['Anhong Guo', 'Ece Kamar', 'Jennifer Wortman Vaughan', 'Hanna Wallach', 'Meredith Ringel Morris'], 'date': '2019-10-27', 'conference': 'ASSETS 2019 Workshop on AI Fairness for People with Disabilities'}
{'abstract': 'In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems fair regression. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness–accuracy frontiers on several standard datasets.', 'title': 'Fair Regression: Quantitative Definitions and Reduction-Based Algorithms - Microsoft Research', 'authors': ['Alekh Agarwal', 'Miro Dudík', 'Zhiwei Steven Wu'], 'date': '2019-09-01', 'conference': '36th International Conference on Machine Learning'}
{'abstract': 'Ride hailing platforms, such as Uber, Lyft, Ola or DiDi, have traditionally focused on the satisfaction of the passengers, or on boosting successful business transactions. However, recent studies provide a multitude of reasons to worry about the drivers in the ride hailing ecosystem. The concerns range from bad working conditions and worker manipulation to discrimination against minorities. With the sharing economy ecosystem growing, more and more drivers financially depend on online platforms and their algorithms to secure a living. It is pertinent to ask what a fair distribution of income on such platforms is and what power and means the platform has in shaping these distributions.', 'title': 'Two-Sided Fairness for Repeated Matchings in Two-Sided Markets: A Case Study of a Ride-Hailing Platform - Microsoft Research', 'authors': ['Tom Sühr', 'Asia J. Biega', 'Meike Zehlike', 'Krishna P. Gummadi', 'Abhijnan Chakraborty'], 'date': '2019-08-04', 'conference': 'KDD 2019'}
{'abstract': 'Social data in digital form, including user-generated content, expressed or implicit relations between people, and behavioral traces, are at the core of popular applications and platforms, driving the research agenda of many researchers. The promises of social data are many, including understanding “what the world thinks” about a social issue, brand, celebrity, or other entity, as well as enabling better decision-making in a variety of fields including public policy, healthcare, and economics. Many academics and practitioners have warned against the naive usage of social data. There are biases and inaccuracies occurring at the source of the data, but also introduced during processing. There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked. This paper recognizes the rigor with which these issues are addressed by different researchers varies across a wide range. We identify a variety of menaces in the practices around social data use, and organize them in a framework that helps to identify them.', 'title': 'Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries - Microsoft Research', 'authors': ['Alexandra Olteanu', 'Carlos Castillo', 'Fernando Diaz', 'Emre Kiciman'], 'date': '2019-07-11', 'conference': 'Frontiers in Big Data'}
{'abstract': 'The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams’ challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by industry practitioners and solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address industry practitioners’ needs.', 'title': 'Improving fairness in machine learning systems: What do industry practitioners need? - Microsoft Research', 'authors': ['Ken Holstein', 'Jennifer Wortman Vaughan', 'Hal Daumé III', 'Miro Dudík', 'Hanna Wallach'], 'date': '2019-06-01', 'conference': '2019 ACM CHI Conference on Human Factors in Computing Systems'}
{'abstract': '\n\n\nWe address a relatively under-explored aspect of human–computer interaction: people’s abilities to understand the relationship between a machine learning model’s stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople’s trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model’s stated accuracy on held-out data and on its observed accuracy in practice. We find that people’s trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to re- cent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.\n\n\n', 'title': 'Understanding the Effect of Accuracy on Trust in Machine Learning Models - Microsoft Research', 'authors': ['Ming Ying', 'Jennifer Wortman Vaughan', 'Hanna Wallach'], 'date': '2019-05-01', 'conference': '2019 ACM CHI Conference on Human Factors in Computing Systems'}
{'abstract': 'There is a growing body of work that proposes methods for mitigating bias in machine learning systems. These methods typically rely on access to protected attributes such as race, gender, or age. However, this raises two significant challenges: (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classiﬁcation, we propose a method for discouraging correlation between the predicted probability of an individual’s true occupation and a word embedding of their name. This method leverages the societal biases that are encoded in word embeddings, eliminating the need for access to protected attributes. Crucially, it only requires access to individuals’ names at training time and not at deployment time. We evaluate two variations of our proposed method using a large-scale data set of online biographies. We ﬁnd that both variations simultaneously reduce race and gender biases, with almost no reduction in the classiﬁer’s overall true positive rate.', 'title': 'What’s in a Name? Reducing Bias in Bios without Access to Protected Attributes - Microsoft Research', 'authors': ['Alexey Romanov', 'Maria De-Arteaga', 'Hanna Wallach', 'Jennifer Chayes', 'Christian Borgs', 'Alexandra Chouldechova', 'Sahin Geyik', 'krishnaram Kenthapadi', 'Anna Rumshisky', 'Adam Tauman Kalai'], 'date': '2019-04-10', 'conference': 'NAACL 2019'}
{'abstract': 'When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system’s approval. Models of agent responsiveness, termed “strategic manipulation,” analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to “trick” a published classifier. In cases of real world classification, however, an agent’s ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group’s costs are higher than the other’s, the learner’s equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner’s utility while actually making both candidate groups worse-off–even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual’s “quality” when agents’ capacities to adaptively respond differ.', 'title': 'The Disparate Effects of Strategic Manipulation - Microsoft Research', 'authors': ['Lily Hu', 'Nicole Immorlica', 'Jennifer Wortman Vaughan'], 'date': '2019-01-27', 'conference': '2nd ACM Conference on Fairness, Accountability, and Transparency'}
{'abstract': 'Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users for information that will lead to better decisions in the future. Recently, concerns have been raised about whether the process of exploration could be viewed as unfair, placing too much burden on certain individuals or groups. Motivated by these concerns, we initiate the study of the externalities of exploration – the undesirable side effects that the presence of one party may impose on another – under the linear contextual bandits model. We introduce the notion of a group externality, measuring the extent to which the presence of one population of users impacts the rewards of another. We show that this impact can in some cases be negative, and that, in a certain sense, no algorithm can avoid it. We then study externalities at the individual level, interpreting the act of exploration as an externality imposed on the current user of a system by future users. This drives us to ask under what conditions inherent diversity in the data makes explicit exploration unnecessary. We build on a recent line of work on the smoothed analysis of the greedy algorithm that always chooses the action that currently looks optimal, improving on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most O~(T1/3)', 'title': 'The Externalities of Exploration and How Data Diversity Helps Exploitation - Microsoft Research', 'authors': ['Manish Raghavan', 'Alex Slivkins', 'Jennifer Wortman Vaughan', 'Zhiwei Steven Wu'], 'date': '2018-02-01', 'conference': '31st Annual Conference on Learning Theory'}
{'abstract': 'To build machine learning systems that are reliable, trustworthy, and fair, we must be able to provide relevant stakeholders with an understanding of how these systems work. Yet what makes a system “intelligible” is difficult to pin down. Intelligibility is a fundamentally human-centered concept that lacks a one-size-fits-all solution. Although many intelligibility techniques have been proposed in the machine learning literature, there are many more open questions about how best to provide stakeholders with the information they need to achieve their desired goals. In this chapter, we begin with an overview of the intelligible machine learning landscape and give several examples of the diverse ways in which needs for intelligibility can arise. We provide an overview of the techniques for achieving intelligibility that have been proposed in the machine learning literature. We discuss the importance of taking a human-centered strategy when designing intelligibility techniques or when verifying that these techniques achieve their intended goals. We also argue that the notion of intelligibility should be expanded beyond machine learning models to other components of machine learning systems, such as datasets and performance metrics. Finally, we emphasize the necessity of tight integration between the machine learning and human–computer interaction communities.', 'title': 'A Human-Centered Agenda for Intelligible Machine Learning - Microsoft Research', 'authors': ['Jennifer Wortman Vaughan', 'Hanna Wallach'], 'date': '2021-04-27', 'conference': 'UNKNOWN'}
{'abstract': 'As AI plays an increasing role in the financial services industry, it is essential that financial services organizations anticipate and mitigate unintended consequences, including fairness-related harms, such as denying people services, initiating predatory lending, amplifying gender or racial biases, or violating laws such as the United States’ Equal Credit Opportunity Act (ECOA). To address these kinds of harms, fairness must be explicitly prioritized throughout the AI development and deployment lifecycle.', 'title': 'Assessing and mitigating unfairness in credit models with the Fairlearn toolkit - Microsoft Research', 'authors': ['Miro Dudík', 'William Chen', 'Solon Barocas', 'Mario Inchiosa', 'Nick Lewins', 'Miruna Oprescu', 'Joy Qiao', 'Mehrnoosh Sameki', 'Mario Schlener', 'Jason Tuo', 'Hanna Wallach'], 'date': '2020-09-22', 'conference': 'UNKNOWN'}
{'abstract': 'Past research shows that users benefit from systems that support them in their writing and exploration tasks. The autosuggestion feature of Web search engines is an example of such a system: It helps users in formulating their queries by offering a list of suggestions as they type. Autosuggestions are typically generated by machine learning (ML) systems trained on a corpus of search logs and document representations. Such automated methods can become prone to issues that result in problematic suggestions that are biased, racist, sexist or in other ways inappropriate. While current search engines have become increasingly proficient at suppressing such problematic suggestions, there are still persistent issues that remain. In this paper, we reflect on past efforts and on why certain issues still linger by covering explored solutions along a prototypical pipeline for identifying, detecting, and addressing problematic autosuggestions. To showcase their complexity, we discuss several dimensions of problematic suggestions, difficult issues along the pipeline, and why our discussion applies to the increasing number of applications beyond web search that implement similar textual suggestion features. By outlining persistent social and technical challenges in moderating web search suggestions, we provide a renewed call for action.', 'title': 'On the Social and Technical Challenges of Web Search Autosuggestion Moderation - Microsoft Research', 'authors': ['T. J. Hazen', 'Alexandra Olteanu', 'Gabriella Kazai', 'Fernando Diaz', 'Michael Golebiewski'], 'date': '2020-07-01', 'conference': 'UNKNOWN'}
{'abstract': 'We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.', 'title': 'Fairlearn: A toolkit for assessing and improving fairness in AI - Microsoft Research', 'authors': ['Sarah Bird', 'Miro Dudík', 'Richard Edgar', 'Brandon Horn', 'Roman Lutz', 'Vanessa Milan', 'Mehrnoosh Sameki', 'Hanna Wallach', 'Kathleen Walker'], 'date': '2020-05-18', 'conference': 'UNKNOWN'}
{'abstract': 'The deployment of facial recognition systems in high-stakes scenarios has sparked widespread concerns about privacy, fairness, and accountability. A common response to these concerns is the suggestion of adding a human in the loop to provide oversight and ensure fairness and accountability. However, the effectiveness of this approach is seldom studied empirically, and humans are known to have biases of their own. In this position paper, we argue for the necessity of empirical studies of human-in-the-loop facial recognition systems. We outline several technical and ethical challenges that arise when conducting such empirical studies and when interpreting their results. Our goal is to initiate a discussion about ways for AI and HCI researchers to work together on human-centered approaches to empirically studying human-in-the-loop facial recognition systems.', 'title': 'A Human in the Loop is Not Enough: The Need for Human-Subject Experiments in Facial Recognition - Microsoft Research', 'authors': ['Forough Poursabzi-Sangdeh', 'Samira Samadi', 'Jennifer Wortman Vaughan', 'Hanna Wallach'], 'date': '2020-04-01', 'conference': 'UNKNOWN'}
{'abstract': 'The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.', 'title': 'Datasheets for Datasets - Microsoft Research', 'authors': ['Timnit Gebru', 'Jamie Morgenstern', 'Briana Vecchione', 'Jennifer Wortman Vaughan', 'Hanna Wallach', 'Hal Daumé III', 'Kate Crawford'], 'date': '2018-03-01', 'conference': 'UNKNOWN'}
