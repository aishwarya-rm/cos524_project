{"title": "Practical Compositional Fairness: Understanding Fairness in Multi-Component Recommender Systems", "authors": ["Xuezhi Wang", "Nithum Thain", "Flavien Prost", "Ed H. Chi", "Alex Beutel"], "conference": "WSDM 2021", "date": "", "affiliation": "Google", "abstract": "Most literature in fairness has focused on improving fairness with respect to one single model or one single objective. However, real-world machine learning systems are usually composed of many different components. Unfortunately, recent research has shown that even if each component is \"fair\", the overall system can still be \"unfair\".  In this paper, we focus on how well fairness composes over multiple components in real systems. We consider two recently proposed fairness metrics for rankings: exposure and pairwise ranking accuracy gap. We provide theory that demonstrates a set of conditions under which fairness of individual models does compose. We then present an analytical framework for both understanding whether a system's signals can achieve compositional fairness, and diagnosing which of these signals lowers the overall system's end-to-end fairness the most. Despite previously bleak theoretical results, on multiple data-sets -- including a large-scale real-world recommender system -- we find that the overall system's end-to-end fairness is largely achievable by improving fairness in individual components."}
{"title": "Attribute-based Propensity for Unbiased Learning in Recommender Systems: Algorithm and Case Studies", "authors": ["Zhen Qin", "Don Metzler", "Xuanhui Wang"], "conference": "26TH ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) (2020)", "date": "", "affiliation": "Google", "abstract": "Many modern recommender systems train their models based on a large amount of implicit user feedback data. Due to the inherent bias in this data (e.g., position bias), learning from it directly can lead to suboptimal models. Recently, unbiased learning was proposed to address such problems by leveraging counterfactual techniques like inverse propensity weighting (IPW). In these methods, propensity scores estimation is usually limited to item's display position in a single user interface (UI)."}
{"title": "Bringing the People Back In: Contesting Benchmark Machine Learning Datasets", "authors": ["Alex Hanna"], "conference": "Participatory Approaches to Machine Learning, ICML 2020 Workshop (2020)", "date": "", "affiliation": "Google", "abstract": "In response to algorithmic unfairness embedded in sociotechnical systems, significant attention has been focused on the contents of machine learning datasets which have revealed biases towards white, cisgender, male, and Western data subjects. In contrast, comparatively less attention has been paid to the histories, values, and norms embedded in such datasets. In this work, we outline a research program - a genealogy of machine learning data - for investigating how and why these datasets have been created, what and whose values influence the choices of data to collect, the contextual and contingent conditions of their creation. We describe the ways in which benchmark datasets in machine learning operate as infrastructure and pose four research questions for these datasets. This interrogation forces us to \"bring the people back in\" by aiding us in understanding the labor embedded in dataset construction, and thereby presenting new avenues of contestation for other researchers encountering the data."}
{"title": "CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation", "authors": ["Xuezhi Wang", "Alex Beutel", "Ed H. Chi"], "conference": "EMNLP 2020", "date": "", "affiliation": "Google", "abstract": "NLP models are shown to suffer from robustness issues, for example, a model's prediction can be easily changed under small perturbations to the input. In this work, we aim to present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, it can generate adversarial texts through controllable attributes that are known to be invariant to task labels. For example, for a main task like sentiment classification, an example attribute can be different categories/domains, and a model should have similar performance across them; for a coreference resolution task, a model's performance should not differ across different demographic attributes. Different from many existing adversarial text generation approaches, we show that our model can generate adversarial texts that are more fluent, diverse, and with better task-label invariance guarantees. We aim to use this model to generate counterfactual texts that could better improve robustness in NLP models (e.g., through adversarial training), and we argue that our generation can create more natural attacks."}
{"title": "Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditin", "authors": ["Ben Hutchinson", "Daniel Theron"], "conference": "FAT* Barcelona, 2020 (2020)", "date": "", "affiliation": "Google", "abstract": "Rising concern for the societal implications of artificial intelligencesystems has inspired a wave of academic and journalistic literaturein which deployed systems are audited for harm by investigatorsfrom outside the organizations deploying the algorithms. However,it remains challenging for practitioners to identify the harmfulrepercussions of their own systems prior to deployment, and, oncedeployed, emergent issues can become difficult or impossible totrace back to their source.In this paper, we introduce a framework for algorithmic auditingthat supports artificial intelligence system development end-to-end,to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that togetherform an overall audit report, drawing on an organization\u00e2\u0080\u0099s valuesor principles to assess the fit of decisions made throughout the pro-cess. The proposed auditing framework is intended to contribute toclosing theaccountability gapin the development and deploymentof large-scale artificial intelligence systems by embedding a robustprocess to ensure audit integrity."}
{"title": "Deontological Ethics By Monotonicity Shape Constraints", "authors": ["Serena Wang"], "conference": "AISTATS (2020)", "date": "", "affiliation": "Google", "abstract": "We demonstrate how easy it is for modern machine-learned systems to violate common deontological ethical principles and social norms such as \"favor the less fortunate,\" and \"do not penalize good attributes.\" We propose that in some cases such ethical principles can be incorporated into a machine-learned model by adding shape constraints that constrain the model to respond only positively to relevant inputs. We analyze the relationship between these deontological constraints that act on individuals and the consequentialist group-based fairness goals of one-sided statistical parity and equal opportunity. This strategy works with sensitive attributes that are Boolean or real-valued such as income and age, and can help produce more responsible and trustworthy AI."}
{"title": "Diversity and Inclusion Metrics for Subset Selection", "authors": ["Dylan Baker", "Ben Hutchinson", "Alex Hanna"], "conference": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES) (2020)", "date": "", "affiliation": "Google", "abstract": "The concept of fairness has recently been applied in machine learning settings to describe a wide range of constraints and objectives. When applied to ranking, recommendation, or subset selection problems for an individual, it becomes less clear that fairness goals are more applicable than goals that prioritize diverse outputs and instances that represent the individual's goals well.  In this work, we discuss the relevance of the concept of fairness to the concepts of diversity and inclusion, and introduce metrics that quantify the diversity and inclusion of an instance or set.  Diversity and inclusion metrics can be used in tandem, including additional fairness constraints, or may be used separately, and we detail how the different metrics interact.  Results from human subject experiments demonstrate that the proposed criteria for diversity and inclusion are consistent with social notions of these two concepts, and human judgments on the diversity and inclusion of example instances are correlated with the defined metrics."}
{"title": "Evading the Curse of Dimensionality in Unconstrained Private Generalized Linear Problems", "authors": ["Shuang Song", "Om Thakkar"], "conference": "24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021) (2020)", "date": "", "affiliation": "Google", "abstract": "Differentially private gradient descent (DP-GD) has been extremely effective both theoretically, and in practice, for solving private empirical risk minimization (ERM) problems. In this paper, we focus on understanding the impact of the clipping norm, a critical component of DP-GD, on its convergence. We provide the first formal convergence analysis of clipped DP-GD."}
{"title": "Explaining Deep Neural Networks using Unsupervised Clustering", "authors": ["Sercan Arik"], "conference": "2020 Workshop on Human Interpretability in Machine Learning (2020)", "date": "", "affiliation": "Google", "abstract": "We propose a novel method to explain trained deep neural networks (DNNs), by distilling them into surrogate models using unsupervised clustering. Our method can be flexibly applied to any subset of layers of a DNN architecture and can incorporate low-level and high-level information. On image datasets given pre-trained DNNs, we demonstrate strength of our method in finding similar training samples, and shedding light on the concepts the DNN bases its decision on. Via user studies, we show that our model can improve user trust in model\u00e2\u0080\u0099s prediction."}
{"title": "Fairness Indicators Demo: Scalable Infrastructure for Fair ML Systems", "authors": ["Manasi N Joshi"], "conference": "(2020) (2020)", "date": "", "affiliation": "Google", "abstract": "The rise of machine learning around the globe in fields like medicine, education, employment, credit lending, and criminal sentencing has the potential to reflect and reinforce societal biases at large scale through the models deployed. While fairness concerns are multifaceted, technical evaluations and improvements of models are a critical aspect of a developer's role. And, for these considerations to truly scale, they must integrate into existing processes. In particular, we focus on seamlessly integrating known technical methods with existing libraries used for the training, evaluation, and deployment of models.  To showcase the suite of tools built in Tensorflow, we present an interactive case study demo in conjunction with Conversation AI, an ML research initiative to make online conversations more inclusive."}
{"title": "Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives", "authors": ["Ben Hutchinson"], "conference": "Proceedings of ICML 2020 Workshop on Participatory Approaches to Machine Learning", "date": "", "affiliation": "Google", "abstract": "How should we decide which fairness criteria or\ndefinitions to adopt in machine learning systems?\nTo answer this question, we must study the fair-\nness preferences of actual users of machine learn-\ning systems. Stringent parity constraints on treat-\nment or impact can come with trade-offs, and\nmay not even be preferred by the social groups\nin question (Zafar et al., 2017). Thus it might\nbe beneficial to elicit what the group\u00e2\u0080\u0099s prefer-\nences are, rather than rely on a priori defined\nmathematical fairness constraints. Simply asking\nfor self-reported rankings of users is challenging\nbecause research has shown that there are often\ngaps between people\u00e2\u0080\u0099s stated and actual prefer-\nences(Bernheim et al., 2013)."}
{"title": "Federated Heavy Hitters with Differential Privacy", "authors": ["Wennan Zhu", "Peter Kairouz", "Brendan McMahan"], "conference": "International Conference on Artificial Intelligence and Statistics (AISTATS) 2020", "date": "", "affiliation": "Google", "abstract": "The discovery of heavy hitters (most frequent items) in user-generated data streams drives improvements in the app and web ecosystems, but can incur substantial privacy risks if not done with care. To address these risks, we propose a distributed and privacy-preserving algorithm for discovering the heavy hitters in a population of user-generated data streams. We leverage the sampling property of our distributed algorithm to prove that it is inherently differentially private, without requiring additional noise. We also examine the trade-off between privacy and utility, and show that our algorithm provides excellent utility while also achieving strong privacy guarantees. A significant advantage of this approach is that it eliminates the need to centralize raw data while also avoiding the significant loss in utility incurred by local differential privacy. We validate our findings both theoretically, using worst-case analyses, and practically, using a Twitter dataset with 1.6M tweets and over 650k users. Finally, we carefully compare our approach to Apple's local differential privacy method for discovering heavy hitters."}
{"title": "On Completeness-aware Concept-Based Explanations in Deep Neural Networks", "authors": ["Been Kim", "Sercan Arik", "Chun-Liang Li", "Tomas Pfister"], "conference": "NeurIPS (2020)", "date": "", "affiliation": "Google", "abstract": "Concept-based explanations can be a key direction to understand how DNNs make decisions. In this paper, we study concept-based explainability in a systematic framework. First, we define the notion of completeness, which quantifies how sufficient a particular set of concepts is in explaining the model's behavior. Based on performance and variability motivations, we propose two definitions to quantify completeness. We show that they yield the commonly-used PCA method under certain assumptions. Next, we study two additional constraints to ensure the interpretability of discovered concept, based on sparsity principles. Through systematic experiments, on specifically-designed synthetic dataset and real-world text and image datasets, we demonstrate the superiority of our framework in finding concepts that are complete (in explaining the decision) and that are interpretable."}
{"title": "Pairwise Fairness for Ranking and Regression", "authors": ["Harikrishna Narasimhan", "Andy Cotter", "Serena Lutong Wang"], "conference": "33rd AAAI Conference on Artificial Intelligence (2020)", "date": "", "affiliation": "Google", "abstract": "Improving fairness for ranking and regression models has less mature algorithmic tooling than classifiers. Here, we present pairwise formulations of fairness for ranking and regression models that can express analogues of statistical fairness notions like equal opportunity or equal accuracy, as well as statistical parity. The proposed framework supports both discrete protected groups, and continuous protected attributes. We show that the resulting training problems can be efficiently and effectively solved using constrained optimization or robust optimization algorithms. Experiments illustrate the broad applicability and trade-offs of these methods."}
{"title": "Privacy Amplification via Random Check-Ins", "authors": ["Peter Kairouz", "H. Brendan McMahan", "Om Thakkar"], "conference": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020", "date": "", "affiliation": "Google", "abstract": "Differentially Private Stochastic Gradient Descent (DP-SGD) forms a fundamental building block in many applications for learning over sensitive data. Two standard approaches, privacy amplification by subsampling, and privacy amplification by shuffling, permit adding lower noise in DP-SGD than via na\\\"&lbrace;\\i&rbrace;ve schemes. A key assumption in both these approaches is that the elements in the data set can be uniformly sampled, or be uniformly permuted ---  constraints that may become prohibitive when the data is processed in a decentralized or distributed fashion. In this paper, we focus on conducting iterative methods like DP-SGD in the setting of federated learning (FL) wherein the data is distributed among many devices (clients). Our main contribution is the random check-in distributed protocol, which crucially relies only on randomized participation decisions made locally and independently by each client. It has privacy/accuracy trade-offs similar to privacy amplification by subsampling/shuffling. However, our method does not require server-initiated communication, or even knowledge of the population size. To our knowledge, this is the first privacy amplification tailored for a distributed learning framework, and it may have broader applicability beyond FL. Along the way, we extend privacy amplification by shuffling to incorporate $(\\epsilon,\\delta)$-DP local randomizers, and exponentially improve its guarantees. In practical regimes, this improvement allows for similar privacy and utility using data from an order of magnitude fewer users."}
{"title": "ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring", "authors": ["Alex Kurakin", "Ekin Dogus Cubuk", "Kihyuk Sohn", "Nicholas Carlini"], "conference": "ICLR (2020)", "date": "", "affiliation": "Google", "abstract": "We improve the recently-proposed ``MixMatch'' semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring. Distribution alignment encourages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of groundtruth labels. Augmentation anchoring feeds multiple strongly augmented versions of an input into the model and encourages each output to be close to the prediction for a weakly-augmented version of the same input. To produce strong augmentations, we propose a variant of AutoAugment which learns the augmentation policy while the model is being trained. Our new algorithm, dubbed ReMixMatch, is significantly more data-efficient than prior work, requiring between 5x and 16x less data to reach the same accuracy. For example, on CIFAR10 with 250 labeled examples we reach 93.73% accuracy (compared to MixMatch\u00e2\u0080\u0099s accuracy of 93.58% with 4,000 examples) and a median accuracy of 84.92% with just four labels per class. We make our code and data open-source at https://github.com/google-research/remixmatch."}
{"title": "Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing", "authors": ["Joonseok Lee"], "conference": "Proceedings of the 3rd AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) (2020)", "date": "", "affiliation": "Google", "abstract": "Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of five ethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal."}
{"title": "Social Biases in NLP Models as Barriers for Persons with Disabilities", "authors": ["Ben Hutchinson", "Vinodkumar Prabhakaran", "Kellie Webster", "Yu Zhong"], "conference": "Proceedings of ACL 2020", "date": "", "affiliation": "Google", "abstract": "Building equitable and inclusive technologies\ndemands paying attention to how social attitudes towards persons with disabilities are\nrepresented within technology. Representations perpetuated by NLP models often inadvertently encode undesirable social biases\nfrom the data on which they are trained. In this\npaper, first we present evidence of such undesirable biases towards mentions of disability in\ntwo different NLP models: toxicity prediction\nand sentiment analysis. Next, we demonstrate\nthat neural embeddings that are critical first\nsteps in most NLP pipelines also contain undesirable biases towards mentions of disabilities.\nWe then expose the topical biases in the social\ndiscourse about some disabilities which may\nexplain such biases in the models; for instance,\nterms related to gun violence, homelessness,\nand drug addiction are over-represented in discussions about mental illness."}
{"title": "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?", "authors": ["Jasmijn Bastings", "Katja Filippova"], "conference": "Proceedings of the 2020 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP", "date": "", "affiliation": "Google", "abstract": "There is a recent surge of papers that focus on attention as explanation of model predictions, giving mixed evidence on whether attention can be used as such. This has led some to try and `improve' attention so as to make it more interpretable. We argue that we should pay attention no heed.\nWhile attention conveniently gives us one weight per input token and is easily extracted, it is often unclear towards what goal it is used as explanation. We argue that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction. When that is the case, input saliency methods better suit our needs, and there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal for their explanations."}
{"title": "The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models", "authors": ["Ian Tenney", "James Wexler", "Jasmijn Bastings", "Tolga Bolukbasi", "Sebastian Gehrmann", "Mahima Pushkarna", "Emily Reif"], "conference": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations", "date": "", "affiliation": "Google", "abstract": "We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models--including classification, seq2seq, and structured prediction--and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit."}
{"title": "Thieves of Sesame Street: Model Extraction on BERT-based APIs", "authors": ["Gaurav Singh Tomar", "Ankur Parikh", "Nicolas Papernot"], "conference": "ICLR 2020 (2020)", "date": "", "affiliation": "Google", "abstract": "We study the problem of model extraction in natural language processing, where an adversary with query access to a victim model attempts to reconstruct a local copy of the model. We show that when both the adversary and victim model fine-tune existing pretrained models such as BERT, the adversary does not need to have access to any training data to mount the attack. Indeed, we show that randomly sampled sequences of words, which do not satisfy grammar structures, make effective queries to extract textual models. This is true even for complex tasks such as natural language inference or question answering. "}
{"title": "Towards a Critical Race Methodology in Algorithmic Fairness", "authors": ["Alex Hanna"], "conference": "ACM Conference on Fairness, Accountability, and  Transparency (ACM FAT*) (2020)", "date": "", "affiliation": "Google", "abstract": "We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems."}
{"title": "Why Reliabilism Is Not Enough:Epistemic and Moral Justification in Machine Learning", "authors": ["Ben Hutchinson"], "conference": "AIES 2020 (2020)", "date": "", "affiliation": "Google", "abstract": "In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed widespread adoption of machine learning? We argue that, in general, people implicitly adopt reliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method. We argue that, in cases where model deployments require moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral \u00e2\u0080\u009cwrapper\u00e2\u0080\u009d around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification\u00e2\u0080\u0094moral justification. Finally, we offer cautions relevant to the (implicit or explicit)adoption of the reliabilist interpretation of machine learning."}
{"title": "50 Years of Test (Un)fairness: Lessons for Machine Learning", "authors": ["Ben Hutchinson"], "conference": "Proceedings of FAT* 2019.", "date": "", "affiliation": "Google", "abstract": "Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past."}
{"title": "Advances and Open Problems in Federated Learning", "authors": ["Peter Kairouz", "H. Brendan McMahan", "K. A. Bonawitz", "Zachary Charles", "Zachary Garrett", "Badih Ghazi", "Ben Hutchinson", "Jakub Kone\u010dn\u00fd", "Mehryar Mohri", "Hang Qi", "Daniel Ramage", "Ananda Theertha Suresh", "Zheng Xu", "Sen Zhao"], "conference": "Arxiv (2019)", "date": "", "affiliation": "Google", "abstract": "Federated learning (FL) is a machine learning setting where many clients (e.g., mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g., service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and mitigates many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents a comprehensive list of open problems and challenges."}
{"title": "Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity", "authors": [], "conference": "ACM-SIAM Symposium on Discrete Algorithms (SODA) (2019)", "date": "", "affiliation": "Google", "abstract": "Sensitive statistics are often collected across sets of users, with repeated collection of reports done over time. For example, trends in users' private preferences or software usage may be monitored via such reports. We study the collection of such statistics in the local differential privacy (LDP) model, and describe an algorithm whose privacy cost is polylogarithmic in the number of changes to a user's value. \n"}
{"title": "Audio De-identification: A New Entity Recognition Task", "authors": ["Ido Cohn", "Itay Laish", "Genady Beryozkin", "Gang Li", "Izhak Shafran", "Idan Szpektor", "Avinatan Hassidim", "Yossi Matias"], "conference": "NAACL (2019)", "date": "", "affiliation": "Google", "abstract": "Named Entity Recognition (NER) has been mostly studied in the context of written text. Specifically, NER is an important step in de-identification (de-ID) of medical records, many of which are recorded conversations between a patient and a doctor. In such recordings, audio spans with personal information should be redacted, similar to the redaction of sensitive character spans in de-ID for written text. The application of NER in the context of audio de-identification has yet to be fully investigated. To this end, we define the task of audio de-ID, in which audio spans with entity mentions should be detected. We then present our pipeline for this task, which involves Automatic Speech Recognition (ASR), NER on the transcript text, and text-to-audio alignment. Finally, we introduce a novel metric for audio de-ID and a new evaluation benchmark consisting of a large labeled segment of the Switchboard and Fisher audio datasets and detail our pipeline's results on it."}
{"title": "Characterizing Sources of Uncertainty to Proxy Calibration and Disambiguate Annotator and Data Bias", "authors": ["Brendan Jou"], "conference": "ICCV Workshop on Interpreting and Explaining Visual Artificial Intelligence Models (2019)", "date": "", "affiliation": "Google", "abstract": "Supporting model interpretability for complex phenomena where annotators can legitimately disagree, such as emotion recognition, is a challenging machine learning task. In this work, we show that explicitly quantifying the uncertainty in such settings has interpretability benefits. We use a simple modification of a classical network inference using Monte Carlo dropout to give measures of epistemic and aleatoric uncertainty. We identify a significant correlation between aleatoric uncertainty and human annotator disagreement (r \u00e2\u0089\u0088 .3). Additionally, we demonstrate how difficult and subjective training samples can be identified using aleatoric uncertainty and how epistemic uncertainty can reveal data bias that could result in unfair predictions. We identify the total uncertainty as a suitable surrogate for model calibration, i.e. the degree we can trust model's predicted confidence. In addition to explainability benefits, we observe modest performance boosts from incorporating model uncertainty."}
{"title": "Counterfactual Fairness in Text Classification through Robustness", "authors": ["Vincent Perot", "Ed H. Chi", "Alex Beutel"], "conference": "AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) (2019)", "date": "", "affiliation": "Google", "abstract": "In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that \"Some people are gay'' is toxic while \"Some people are straight'' is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification."}
{"title": "Debiasing Embeddings for Fairer Text Classification", "authors": ["Flavien Prost", "Nithum Thain", "Tolga Bolukbasi"], "conference": "1st ACL Workshop on Gender Bias for Natural Language Processing (2019)", "date": "", "affiliation": "Google", "abstract": "(Bolukbasi et al., 2016) demonstrated that pre-trained  word embeddings  can  inherit  gender bias from the data they were trained on.  We investigate  how  this  bias  affects  downstream classification  tasks,  using  the  case  study  of occupation  classification  (De-Arteaga  et  al.,2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy  channel  for  communicating  gender information.   With  a  relatively  minor  adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and obtain high classification accuracy."}
{"title": "Deep determinantal generative classifier: robustness on noisy and adversarial samples", "authors": [], "conference": "ICML (2019)", "date": "", "affiliation": "Google", "abstract": "Large-scale datasets may contain significant proportions of noisy (incorrect) class labels, and it is well-known that modern deep neural networks (DNNs) poorly generalize from such noisy training datasets. To mitigate the issue, we propose a novel inference method, termed Robust Generative classifier (RoG), applicable to any discriminative (e.g., softmax) neural classifier pre-trained on noisy datasets. In particular, we induce a generative classifier on top of hidden feature spaces of the pre-trained DNNs, for obtaining a more robust decision boundary. By estimating the parameters of generative classifier using the minimum covariance determinant estimator, we significantly improve the classification accuracy with neither re-training of the deep model nor changing its architectures. With the assumption of Gaussian distribution for features, we prove that RoG generalizes better than baselines under noisy labels. Finally, we propose the ensemble version of RoG to improve its performance by investigating the layer-wise characteristics of DNNs. Our extensive experimental results demonstrate the superiority of RoG given different learning models optimized by several training techniques to handle diverse scenarios of noisy labels."}
{"title": "Detecting Bias with Generative Counterfactual Face  Attribute Augmentation", "authors": ["Ben Hutchinson"], "conference": "Fairness, Accountability, Transparency and Ethics in Computer Vision Workshop (in conjunction with CVPR) (2019)", "date": "", "affiliation": "Google", "abstract": "We introduce a simple framework for identifying biases of a smiling attribute classifier. Our method poses counterfactual questions of the form: how would the prediction change if this face characteristic had been different? We leverage recent advances in generative adversarial networks to build a realistic generative model of faces that affords controlled manipulation of specific facial characteristics. Empirically, we identify several different factors of variation (that we believe should be in-dependent of a smiling) that  affect the predictions of a smiling classifier trained on CelebA."}
{"title": "Direct Uncertainty Prediction for Medical Second Opinions", "authors": ["Maithra Raghu", "Rory Abbott Sayres"], "conference": "ICML (2019)", "date": "", "affiliation": "Google", "abstract": "The issue of disagreements amongst human experts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresponds to doctor disagreements on a patient diagnosis. In this work, we show that machine learning models can be successfully trained to give uncertainty scores to data instances that result in high expert disagreements. In particular, they can identify patient cases that would benefit most from a medical second opinion. Our central methodological finding is that Direct Uncertainty Prediction (DUP), training a model to predict an uncertainty score directly from the raw patient features, works better than Uncertainty Via Classification, the two step process of training a classifier and postprocessing the output distribution to give an uncertainty score. We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging application."}
{"title": "Discovering User Bias in Ordinal Voting Systems", "authors": ["Alyssa Whitlock Lees", "Chris Welty"], "conference": "SAD-2019: Workshop on Subjectivity, Ambiguity and Disagreement", "date": "", "affiliation": "Google", "abstract": "Crowdsourcing systems increasingly rely on users to provide more\nsubjective ground truth for intelligent systems - e.g. ratings, aspect\nof quality and perspectives on how expensive or lively a place feels,\netc. We focus on the ubiquitous implementation of online user ordinal voting (e.g 1-5, 1 star-4 stars) on some aspect of an entity, to\nextract a relative truth, measured by a selected metric such as vote\nplurality or mean. We argue that this methodology can aggregate\nresults that yield little information to the end user. In particular,\nordinal user rankings often converge to a indistinguishable rating.\nThis is demonstrated by the trend in certain cities for the majority of restaurants to all have a 4 star rating. Similarly, the rating of an establishment can be significantly affected by a few users.\nUser bias in voting is not spam, but rather a preference that can\nbe harnessed to provide more information to users. We explore\nnotions of both global skew and user bias. Leveraging these bias\nand preference concepts, the paper suggests explicit models for\nbetter personalization and more informative ratings."}
{"title": "Diversity-Sensitive Conditional Generative Adversarial Networks", "authors": [], "conference": "ICLR (2019)", "date": "", "affiliation": "Google", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse\nproblem in the Conditional Generative Adversarial Network (cGAN). Although\nconditional distributions are multi-modal (i.e., having many modes) in practice,\nmost cGAN approaches tend to learn an overly simplified distribution where an\ninput is always mapped to a single output regardless of variations in latent code.\nTo address such issue, we propose to explicitly regularize the generator to produce\ndiverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives.\nAdditionally, explicit regularization on generator allows our method to control a\nbalance between visual quality and diversity. We demonstrate the effectiveness\nof our method on three conditional generation tasks: image-to-image translation,\nimage inpainting, and future video prediction. We show that simple addition of\nour regularization to existing models leads to surprisingly diverse generations,\nsubstantially outperforming the previous approaches for multi-modal conditional\ngeneration specifically designed in each individual task."}
{"title": "Fairness in Recommendation Ranking through Pairwise Comparisons", "authors": ["Alex Beutel", "Zhe Zhao", "Ed H. Chi"], "conference": "KDD (2019)", "date": "", "affiliation": "Google", "abstract": "Recommender systems are one of the most pervasive applications of machine learning in industry, with many services using them to match users to products or information.  As such it is important to ask: what are the possible fairness risks, how can we quantify them, and how should we address them?"}
{"title": "Fairness Sample Complexity and the Case for Human Intervention", "authors": ["Alyssa Whitlock Lees"], "conference": "Where is the Human? Bridging the Gap Between AI and HCI at Chi (2019)", "date": "", "affiliation": "Google", "abstract": "With the aim of building machine learning systems that incorporate standards of fairness and accountability, we explore explicit subgroup sample complexity bounds. The work is motivated by the observation that classifier predictions for real world datasets often demonstrate drastically different metrics, such as accuracy, when subdivided by specific sensitive variable subgroups.  The reasons for these discrepancies are varied and not limited to the influence of mitigating variables, institutional bias, underlying population distributions as well as selection bias. Among the numerous definitions of fairness that exist, we argue that at a minimum, principled ML practices should ensure that classification predictions are able to mirror the underlying sub-population distributions as a prelude to bias mitigation, and not amplify discrepancies due to sampling/selection bias. However, as the number of sensitive variables grow, populations meeting at the intersectionality of these variables may simply not exist or be large enough to accurately sample from. In theses increasingly likely scenarios, the case for human intervention and applying situational and individual definitions of fairness should be made..    In this paper we explore, setting Pareto-efficient subgroup sample complexity lower bounds based on the complexity of the ML classifier using VC dimension and Rademacher complexity.  We demonstrate that for a classifier to approach a definition of fairness in terms of specific sensitive variables, adequate subgroup population samples need to exist and the model dimensionality has to be aligned with subgroup population distributions.  In cases where this is not feasible human intervention is explored.  We look at two commonly explored UCI datasets under this lens."}
{"title": "Flexibly Fair Representation Learning by Disentanglement", "authors": ["Kevin Jordan Swersky"], "conference": "ICML (2019)", "date": "", "affiliation": "Google", "abstract": "We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also \\emph&lbrace;flexibly fair&rbrace;, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder---which does not require the sensitive attributes for inference---allows for the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions."}
{"title": "Generative Models for Effective ML on Private, Decentralized Datasets", "authors": ["Sean Augenstein", "Brendan McMahan", "Daniel Ramage", "Swaroop Ramaswamy", "Peter Kairouz", "Mingqing Chen", "Rajiv Mathews", "Blaise Aguera-Arcas"], "conference": "Arxiv (2019)", "date": "", "affiliation": "Google", "abstract": "To improve real-world applications of machine learning, experienced practitioners develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data - of representative samples, of outliers, of misclassifications, or alike - is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses, and c) assigning human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the  practitioner may only access aggregated outputs such as metrics or model parameters. This paper outlines a research agenda to address data-oriented tooling needs of ML practitioners who work with privacy-sensitive or decentralized datasets. We demonstrate that generative models - trained using federated methods and with formal differential privacy guarantees - can be used to effectively debug data issues even when the data cannot be directly inspected."}
{"title": "Hiding Images Within Images", "authors": ["Shumeet Baluja"], "conference": "IEEE Transactions on Pattern Analysis and Machine Intelligence (2019)", "date": "", "affiliation": "Google", "abstract": "We present a system to hide a full color image inside another of the\nsame size with minimal quality loss to either image.  Deep neural\nnetworks are simultaneously trained to create the hiding and revealing\nprocesses and are designed to specifically work as a pair.  The system\nis trained on images drawn randomly from the ImageNet database, and\nworks well on natural images from a wide variety of sources.  Beyond\ndemonstrating the successful application of deep learning to hiding\nimages, we examine how the result is achieved and apply numerous\ntransformations to analyze if image quality in the host and hidden\nimage can be maintained.  These transformation range from simple image\nmanipulations to sophisticated machine learning-based adversaries.\nTwo extensions to the basic system are presented that mitigate the\npossibility of discovering the content of the hidden image.  With\nthese extensions, not only can the hidden information be kept secure,\nbut the system can be used to hide even more than a single image.\nApplications for this technology include image authentication,\ndigital watermarks, finding exact regions of image manipulation, and\nstoring meta-information about image rendering and content."}
{"title": "Human-Centered Tools for Coping with Imperfect Algorithms during Medical Decision-Making", "authors": ["Carrie Jun Cai", "Emily Reif", "Narayan G Hegde", "Been Kim", "Daniel Smilkov", "Martin Wattenberg", "Fernanda Vi\u00e9gas", "Greg Corrado"], "conference": "Conference on Human Factors in Computing Systems (2019)", "date": "", "affiliation": "Google", "abstract": "Machine learning (ML) is increasingly being used in image retrieval systems for medical decision making. One application of ML is to retrieve visually similar medical images from past patients (e.g. tissue from biopsies) to reference when making a medical decision with a new patient. However, no algorithm can perfectly capture an expert's ideal notion of similarity for every case: an image that is algorithmically determined to be similar may not be medically relevant to a doctor's specific diagnostic needs. In this paper, we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm, and developed tools that empower users to cope with the search algorithm on-the-fly, communicating what types of similarity are most important at different moments in time. In two evaluations with pathologists, we found that these refinement tools increased the diagnostic utility of images found and increased user trust in the algorithm. The tools were preferred over a traditional interface, without a loss in diagnostic accuracy. We also observed that users adopted new strategies when using refinement tools, re-purposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors. Taken together, these findings inform future human-ML collaborative systems for expert decision-making."}
{"title": "Identifying and Correcting Label Bias in Machine Learning", "authors": ["Ofir Nachum"], "conference": "arxiv (2019)", "date": "", "affiliation": "Google", "abstract": "Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained\non such datasets can inherit these biases. In this\npaper, we provide a mathematical formulation of\nhow this bias can arise. We do so by assuming\nthe existence of underlying, unknown, and unbiased labels which are overwritten by an agent who\nintends to provide accurate labels but may have\nbiases against certain groups. Despite the fact that\nwe only observe the biased labels, we are able to\nshow that the bias may nevertheless be corrected\nby re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset\ncorresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine\nlearning classifier. Our procedure is fast and robust and can be used with virtually any learning\nalgorithm. We evaluate on a number of standard\nmachine learning fairness datasets and a variety\nof fairness notions, finding that our method outperforms standard approaches in achieving fair\nclassification."}
{"title": "Interpreting Social Respect:  A Normative Lens for ML Models", "authors": ["Ben Hutchinson"], "conference": "(2019) (2019)", "date": "", "affiliation": "Google", "abstract": "Machine learning is often viewed as an inherently value-neutral process:\nstatistical tendencies in the training inputs are simply''\nused to generalize to new examples. However when models impact social\nsystems such as interactions between humans, these patterns learned by models\nhave normative implications. It is important that we ask not onlywhat\npatterns exist in the data?'', but also ``how do we want our system \nto impact people?'' In particular, because minority and marginalized\nmembers of society are often statistically underrepresented in data sets, models\nmay have undesirable disparate impact on such groups. As such, objectives of\nsocial equity and distributive justice require that we develop tools for both\nidentifying and interpreting harms introduced by models."}
{"title": "Investigating the Effects of Gender Bias on GitHub", "authors": ["Emerson Murphy-Hill"], "conference": "Proceedings of the 2019 International Conference on Software Engineering", "date": "", "affiliation": "Google", "abstract": "Diversity, including gender diversity, is valued by many software development organizations, yet the field remains dominated by men. One reason for this lack of diversity is gender bias. In this paper, we study the effects of that bias by using an existing framework derived from the gender studies literature. We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub, then evaluate those hypotheses quantitatively. While our results show that effects of gender bias are largely invisible on the GitHub platform itself, there are still signals of women concentrating their work in fewer places and being more restrained in communication than men."}
{"title": "Matroids, Matchings, and Fairness", "authors": ["Ravi Kumar", "Silvio Lattanzi", "Sergei Vassilvitskii"], "conference": "AISTATS 2019 (2019)", "date": "", "affiliation": "Google", "abstract": "The desire to use machine learning to assist in human decision making has spawned a large area of research in understanding the impact of such systems not only on the society as a whole, but also the specific impact on different subpopulations. Recent work has shown that  while there are several natural ways to quantify the fairness of a particular system, none of them are universal, and except for trivial cases, satisfying one means violating another~\\citet&lbrace;Kleinberg, Goel, Kleinberg2&rbrace;.  "}
{"title": "Metric-Optimized Example Weights", "authors": ["Sen Zhao", "Harikrishna Narasimhan"], "conference": "Proceedings of the 36th International Conference on Machine Learning (2019)", "date": "", "affiliation": "Google", "abstract": "Real-world machine learning applications often have complex test metrics, and may have training and test data that are not identically distributed. Motivated by known connections between complex test metrics and cost-weighted learning, we propose addressing these issues by using a weighted loss function with a standard loss, where the weights on the training examples are learned to optimize the test metric on a validation set. These metric-optimized example weights can be learned for any test metric, including black box and customized ones for specific applications. We illustrate the performance of the proposed method on diverse public benchmark datasets and real-world applications. We also provide a generalization bound for the method."}
{"title": "Model Cards for Model Reporting", "authors": ["Andrew Zaldivar", "Ben Hutchinson", "Lucy Vasserman"], "conference": "(2019) (2019)", "date": "", "affiliation": "Google", "abstract": "Trained machine learning models are increasingly used to perform high impact tasks such as determining crime recidivism rates and predicting health risks. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts they are not well-suited for, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards (or M-cards) to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic subgroups (e.g., race, geographic location, sex, Fitzpatrick skin tone) and intersectional subgroups (e.g., age and race, or sex and Fitzpatrick skin tone) that are relevant to the intended application domains. Model cards also disclose the context under which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for models trained to detect smiling faces on the CelebA dataset (Liu et al., 2015) and models trained to detect toxicity in the Conversation AI dataset (Dixon et al., 2018). We propose this work as a step towards the responsible democratization of machine learning and related AI technology, providing context around machine learning models and increasing the transparency into how well such models work.  We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed documentation."}
{"title": "Optimal Noise-Adding Mechanism in Additive Differential Privacy", "authors": ["Wei Ding", "Ruiqi Guo", "Sanjiv Kumar"], "conference": "Proceedings of the 22th International Conference on Artificial Intelligence and Statistics (AISTATS) (2019)", "date": "", "affiliation": "Google", "abstract": "We derive the optimal $(0, \\delta)$-differentially private query-output independent noise-adding mechanism for single real-valued query function under a general cost-minimization framework. Under a mild technical condition, we show that the optimal noise probability distribution is a uniform distribution with a probability mass at the origin. We explicitly derive the optimal noise distribution for general $\\ell^p$ cost functions, including $\\ell^1$ (for noise magnitude) and $\\ell^2$ (for noise power) cost functions, and show that the probability concentration on the origin occurs when $\\delta > \\frac&lbrace;p&rbrace;&lbrace;p+1&rbrace;$. Our result demonstrates an improvement over the existing Gaussian mechanisms  by a factor of two and three for $(0,\\delta)$-differential privacy in the high privacy regime in the context of minimizing the noise magnitude and noise power, and the gain is more pronounced in the low privacy regime. Our result is consistent with the existing result for $(0,\\delta)$-differential privacy in the discrete setting, and identifies a probability concentration phenomenon in the continuous setting."}
{"title": "Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals", "authors": ["Andrew Cotter", "Serena Wang"], "conference": "Journal of Machine Learning Research (2019)", "date": "", "affiliation": "Google", "abstract": "Machine learning models often need to satisfy many real-world policy goals and capture some\nkinds of side information. We show that many such goals can be mathematically expressed as\nconstraints on the model\u00e2\u0080\u0099s predictions on the data, which we call rate constraints. In this paper, we\nstudy the specific problem of training non-convex models subject to these rate constraints (which\nare non-convex and non-differentiable), and the general problem of constrained optimization of\npossibly non-convex objectives with possibly non-convex and non-differentiable constraints. In the\nnon-convex setting, the Lagrangian may not have an equilibrium to converge to, and thus using the\nstandard approach of Lagrange multipliers may fail as a deterministic solution may not even exist.\nFurthermore, if the constraints are non-differentiable, then one cannot optimize the Lagrangian\nwith gradient-based methods by definition. To solve these issues, we present the proxy-Lagrangian,\nwhich leads to an algorithm that produces a stochastic classifier with theoretical guarantees by\nplaying a two-player non-zero-sum game. The first player minimizes external regret in terms of a\ndifferentiable relaxation of the constraints and the second player enforces the original constraints\nby minimizing swap-regret: this leads to finding a solution concept which we call a semi-coarse\ncorrelated equilibrium which we interestingly show corresponds to an approximately optimal and\nfeasible solution to the constrained optimization problem. We then give a procedure which shrinks\nthe randomized solution down to one that is a mixture of at most m + 1 deterministic solutions.\nThis culminates into end-to-end procedures which can provably solve non-convex constrained\noptimization problems with possibly non-differentiable and non-convex constraints. We provide\nextensive experimental results on benchmark and real-world problems, enforcing a broad range of\npolicy goals including different fairness metrics, and other goals on accuracy, coverage, recall, and\nchurn."}
{"title": "Pareto-Efficient Fairness for Skewed Subgroup Data", "authors": ["Alyssa Whitlock Lees", "Chris Welty"], "conference": "AISG (2019)", "date": "", "affiliation": "Google", "abstract": "As awareness of the potential for learned models to amplify existing societal biases increases,\nthe field of ML fairness has developed mitigation techniques. A prevalent method applies constraints, including equality of performance, with respect to subgroups defined over the intersection of sensitive attributes such as race and gender. Enforcing such constraints when the subgroup populations are considerably skewed with respect to a target can lead to unintentional degradation in performance, without benefiting any individual subgroup, counter to the United Nations Sustainable Development goals of reducing inequalities and promoting growth. In order to avoid such performance degradation while ensuring equitable treatment to all groups, we propose Pareto-Efficient Fairness (PEF), which identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane. Specifically, PEF finds a Pareto Optimal point which maximizes multiple subgroup\naccuracy measures. The algorithm scalarizes using the adaptive weighted metric norm by iteratively searching the Pareto region of all models enforcing the fairness constraint. PEF is backed\nby strong theoretical results on discoverability and provides domain practitioners finer control in\nnavigating both convex and non-convex accuracyfairness trade-offs. Empirically, we show that PEF\nincreases performance of all subgroups in skewed synthetic data and UCI datasets."}
{"title": "Perturbation Sensitivity Analysis to Detect Unintended Model Biases", "authors": ["Vinodkumar Prabhakaran", "Ben Hutchinson"], "conference": "EMNLP 2019 (2019)", "date": "", "affiliation": "Google", "abstract": "Data-driven statistical Natural Language Processing (NLP) techniques leverage large amounts of language data to build models that can understand language. However, most language data reflect the public discourse at the time the data was produced, and hence NLP models are susceptible to learning incidental associations around named referents at a particular point in time, in addition to general linguistic meaning. An NLP system designed to model notions such as sentiment and toxicity should ideally produce scores that are independent of the identity of such entities mentioned in text and their social associations. For example, in a general purpose sentiment analysis system, a phrase such as I hate Katy Perry should be interpreted as having the same sentiment as I hate Taylor Swift. Based on this idea, we propose a generic evaluation framework, Perturbation Sensitivity Analysis, which detects unintended model biases related to named entities, and requires no new annotations or corpora. We demonstrate the utility of this analysis by employing it on two different NLP models --- a sentiment model and a toxicity model --- applied on online comments in English language from four different genres."}
{"title": "Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements", "authors": ["Alex Beutel", "Ed H. Chi"], "conference": "AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) (2019)", "date": "", "affiliation": "Google", "abstract": "As more researchers have become aware of and passionate about algorithmic fairness, there has been an explosion in papers laying out new metrics, suggesting algorithms to address issues, and calling attention to issues in existing applications of machine learning. This research has greatly expanded our understanding of the concerns and challenges in deploying machine learning, but there has been much less work in seeing how the rubber meets the road. "}
{"title": "Shape Constraints for Set Functions", "authors": ["Andrew Cotter", "Serena Wang"], "conference": "International Conference on Machine Learning (2019)", "date": "", "affiliation": "Google", "abstract": "Set functions predict a label from a permutation-invariant variable-size collection of feature vectors. We propose making set functions more understandable and regularized by capturing domain knowledge through shape constraints. We show how prior work in monotonic constraints can be adapted to set functions. Then we propose two new shape constraints designed to generalize the conditioning role of weights in a weighted mean. We show how one can train standard functions and set functions that satisfy these shape constraints with a deep lattice network. We propose a non-linear estimation strategy we call the semantic feature engine that uses set functions with the proposed shape constraints to estimate labels for compound sparse categorical features. Experiments on real-world data show the achieved accuracy is similar to deep sets or deep neural networks, but provides guarantees of the model behavior and is thus easier to explain and debug."}
{"title": "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks", "authors": ["Nicholas Carlini"], "conference": "USENIX Security (2019)", "date": "", "affiliation": "Google", "abstract": "This paper describes a testing methodology for quantitatively assessing the risk of \\emph&lbrace;unintended memorization&rbrace; of rare or unique sequences in generative sequence models---a common type of neural network. Such models are sometimes trained on sensitive data (e.g., the text of users' private messages); our methodology allows deep-learning to choose configurations that minimize memorization during training, thereby  benefiting privacy."}
{"title": "Tough Times at Transitional Homeless Shelters: Considering the Impact of Financial Insecurity on Digital Security and Privacy", "authors": ["Manya Sleeper", "Kathleen O'Leary", "Anna Turner", "Jill Palzkill Woelfer", "Sunny Consolvo"], "conference": "Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems", "date": "", "affiliation": "Google", "abstract": "Addressing digital security and privacy issues can be particularly difficult for users who face challenging circumstances. We performed semi-structured interviews with residents and staff at 4 transitional homeless shelters in the U.S. San Francisco Bay Area (n=15 residents, 3 staff) to explore their digital security and privacy challenges. Based on these interviews, we outline four tough times themes -- challenges experienced by our financially insecure participants that impacted their digital security and privacy -- which included: (1) limited financial resources, (2) limited access to reliable devices and Internet, (3) untrusted relationships, and (4) ongoing stress. We provide examples of how each theme impacts digital security and privacy practices and needs. We then use these themes to provide a framework outlining opportunities for technology creators to better support users facing security and privacy challenges related to financial insecurity."}
{"title": "Toward a better trade-off between performance and fairness with kernel-based distribution matching", "authors": ["Flavien Prost", "Ed H. Chi", "Alex Beutel"], "conference": "Neurips (2019)", "date": "", "affiliation": "Google", "abstract": "As recent literature has demonstrated how classifiers often carry unintended biases toward some subgroups, deploying machine learned models to users demands careful consideration of the social consequences. How should we address this problem in a real-world system? How should we balance core performance and fairness metrics? In this paper, we introduce a MinDiff framework for regularizing classifiers toward different fairness metrics and analyze a technique with kernel-based statistical dependency tests. We run a thorough study on an academic dataset to compare the Pareto frontier achieved by different regularization approaches, and apply our kernel-based method to two large-scale industrial systems demonstrating real-world improvements."}
{"title": "Towards Automatic Concept-based Explanations", "authors": ["James Wexler", "Been Kim"], "conference": "NeurIPS (2019)", "date": "", "affiliation": "Google", "abstract": "Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are salient for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \\emph&lbrace;concept&rbrace; based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent and salient for the neural network's predictions."}
{"title": "Towards Equitable AI for the Next Billion Users", "authors": ["Nithya Sambasivan", "Jess Scon Holbrook"], "conference": "ACM interactions (2019)", "date": "", "affiliation": "Google", "abstract": "In this article, we present research provocations for AI in the Global South, to spur a conversation on the implicit beliefs, biases, and issues that may be normalized in AI. As much of AI\u00e2\u0080\u0099s functioning is still not well understood or fully developed, we believe these critical areas for research are crucial to shaping inclusive AI as it becomes more complex and powerful. We bring our perspectives as HCI researchers and social scientists that work closely with AI researchers. We have started to address some of these areas in our research and invite further explorations from the research community."}
{"title": "Towards Federated Learning at Scale: System Design", "authors": ["K. A. Bonawitz", "Alex Ingerman", "Jakub Kone\u010dn\u00fd", "Stefano Mazzocchi", "Brendan McMahan", "Daniel Ramage"], "conference": "SysML 2019", "date": "", "affiliation": "Google", "abstract": "Federated Learning is a distributed machine learning approach which enables training on a large corpus of data which never needs to leave user devices. We have spent some effort over the last two years building a scalable production system for FL. In this paper, we report about the resulting high-level design, sketching the challenges and the solutions, as well as touching the open problems and future directions."}
{"title": "Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints", "authors": ["Andrew Cotter", "Serena Wang"], "conference": "International Conference on Machine Learning (2019)", "date": "", "affiliation": "Google", "abstract": "Classifiers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other policy goals. We study the generalization performance for such constrained optimization problems, in terms of how well the constraints are satisfied at evaluation time, given that they are satisfied at training time. To improve generalization, we frame the problem as a two-player game where one player optimizes the model parameters on a training dataset, and the other player enforces the constraints on an independent validation dataset. We build on recent work in two-player constrained optimization to show that if one uses this two-dataset approach, then constraint generalization can be significantly improved. As we illustrate experimentally, this approach works not only in theory, but also in practice."}
41
{"title": "Transfer of Machine Learning Fairness across Domains", "authors": ["Xuezhi Wang", "Alex Beutel", "Ed H. Chi"], "conference": "(2019) (2019)", "date": "", "affiliation": "Google", "abstract": "If our models are used in new or unexpected cases, do we know if they will make fair predictions? Previously, researchers developed ways to debias a model for a single problem domain. However, this is often not how models are trained and used in practice. For example, labels and demographics (sensitive attributes) are often hard to observe, resulting in auxiliary or synthetic data to be used for training, and proxies of the sensitive attribute to be used for evaluation of fairness. A model trained for one setting may be picked up and used in many others, particularly as is common with pre-training and cloud APIs. Despite the pervasiveness of these complexities, remarkably little work in the fairness literature has theoretically examined these issues. We frame all of these settings as domain adaptation problems: how can we use what we have learned in a source domain to debias in a new target domain, without directly debiasing on the target domain as if it is a completely new problem? We offer new theoretical guarantees of improving fairness across domains, and offer a modeling approach to transfer to data-sparse target domains. We give empirical results validating the theory and showing that these modeling approaches can improve fairness metrics with less data."}
{"title": "Transparent, Scrutable and Explainable User Models for Personalized Recommendation", "authors": ["Filip Radlinski"], "conference": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19) (2019)", "date": "", "affiliation": "Google", "abstract": "Most recommender systems base their recommendations on implicit or explicit item-level feedback provided by users. These item ratings are combined into a complex user model, which then predicts the suitability of other items. While effective, such methods have limited scrutability and transparency. For instance, if a user's interests change, then many item ratings would usually need to be modified to significantly shift the user's recommendations. Similarly, explaining how the system characterizes the user is impossible, short of presenting the entire list of known item ratings. In this paper, we present a new set-based recommendation technique that permits the user model to be explicitly presented to users in natural language, empowering users to understand recommendations made and improve the recommendations dynamically. While performing comparably to traditional collaborative filtering techniques in a standard static setting, our approach allows users to efficiently improve recommendations. Further, it makes it easier for the model to be validated and adjusted, building user trust and understanding."}
{"title": "Understanding and correcting pathologies in the training of learned optimizers", "authors": ["Luke Metz", "Niru Maheswaranathan", "Daniel Freeman", "Jascha Sohl-dickstein"], "conference": "ICML (2019)", "date": "", "affiliation": "Google", "abstract": "Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned optimizers may similarly outperform current hand-designed optimizers, especially for specific problems. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process. The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance. This allows us to train neural networks to perform optimization of a specific task faster than tuned first-order methods. Moreover, by training the optimizer against validation loss (as opposed to training loss), we are able to learn optimizers that train networks to generalize better than first order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks faster in wall-clock time compared to tuned first-order methods and with an improvement in test loss."}
{"title": "What is Fair?  Exploring Pareto-Efficiency for Fairness Constraint Classifiers", "authors": ["Alyssa Whitlock Lees", "Chris Welty"], "conference": "arxiv (2019)", "date": "", "affiliation": "Google", "abstract": "The potential for learned models to amplify existing societal biases has been broadly recognized. Fairness-aware classifier constraints, which apply equality metrics of performance across subgroups defined on sensitive attributes such as race and gender, seek to rectify inequity but can yield non-uniform degradation in performance for skewed datasets.  In certain domains, imbalanced degradation of performance can yield another form of unintentional bias. In the spirit of constructing fairness-aware algorithms as societal imperative, we explore an alternative: Pareto-Efficient Fairness (PEF).  PEF identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane, maximizing multiple subgroup accuracies. Empirically we demonstrate that PEF increases performance of all subgroups in several UCI datasets."}
{"title": "A General Approach to Adding Differential Privacy to Iterative Training Procedures", "authors": ["Brendan McMahan", "Nicolas Papernot", "Peter Kairouz"], "conference": "NIPS (2018)", "date": "", "affiliation": "Google", "abstract": "In this work we address the practical challenges of training machine learning models on privacy-sensitive datasets by introducing a modular approach that minimizes changes to training algorithms, provides a variety of configuration strategies for the privacy mechanism, and then isolates and simplifies the critical logic that computes the final privacy guarantees. A key challenge is that training algorithms often require estimating many different quantities (vectors) from the same set of examples --- for example, gradients of different layers in a deep learning architecture, as well as metrics and batch normalization parameters. Each of these may have different properties like dimensionality, magnitude, and tolerance to noise. By extending previous work on the Moments Accountant for the subsampled Gaussian mechanism, we can provide privacy for such heterogeneous sets of vectors, while also structuring the approach to minimize software engineering challenges."}
{"title": "A Qualitative Exploration of Perceptions of Algorithmic Fairness", "authors": [], "conference": "CHI 2018 (2018)", "date": "", "affiliation": "Google", "abstract": "Algorithmic systems increasingly shape information people are exposed to as well as influence decisions about employment, finances, and other opportunities. In some cases, algorithmic systems may be more or less favorable to certain groups or individuals, sparking substantial discussion of algorithmic fairness in public policy circles, academia, and the press. We broaden this discussion by exploring how members of potentially affected communities feel about algorithmic fairness. We conducted workshops and interviews with 44 participants from several populations traditionally marginalized by categories of race or class in the United States. While the concept of algorithmic fairness was largely unfamiliar, learning about algorithmic (un)fairness elicited negative feelings that connect to current national discussion about racial injustice and economic inequality. In addition to their concerns about potential harms to themselves and society, participants also indicated that algorithmic fairness (or lack thereof) could substantially affect their trust in a company or product."}
{"title": "Adversarial Examples as an Input-Fault Tolerance Problem", "authors": [], "conference": "NeurIPS Workshop on Security in Machine Learning (2018)", "date": "", "affiliation": "Google", "abstract": "We analyze the adversarial examples problem in terms of a model\u00e2\u0080\u0099s fault tolerance\nwith respect to its input. Whereas previous work focuses on arbitrarily strict threat\nmodels, i.e., -perturbations, we consider arbitrary valid inputs and propose an\ninformation-based characteristic for evaluating tolerance to diverse input faults."}
{"title": "Adversarial Spheres", "authors": ["Luke Metz", "Maithra Raghu", "Martin Wattenberg"], "conference": "ICLR Workshop (2018)", "date": "", "affiliation": "Google", "abstract": "State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size O(1/sqrt(d)). Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples."}
{"title": "Adversarially Robust Generalization Requires More Data", "authors": [], "conference": "NeurIPS (Spotlight) (2018)", "date": "", "affiliation": "Google", "abstract": "Machine learning models are often susceptible to adversarial perturbations of their inputs.\nEven small perturbations can cause state-of-the-art classifiers with high \u00e2\u0080\u009cstandard\u00e2\u0080\u009d accuracy to\nproduce an incorrect prediction with high confidence. To better understand this phenomenon, we\nstudy adversarially robust learning from the viewpoint of generalization. We show that already\nin a simple natural data model, the sample complexity of robust learning can be significantly\nlarger than that of \u00e2\u0080\u009cstandard\u00e2\u0080\u009d learning. This gap is information theoretic and holds irrespective\nof the training algorithm or the model family. We complement our theoretical results with\nexperiments on popular image classification datasets and show that a similar gap exists here as\nwell. We postulate that the difficulty of training robust classifiers stems, at least partially, from\nthis inherently larger sample complexity."}
{"title": "Anatomy of a Privacy-Safe Large-Scale Information Extraction System Over Email", "authors": ["Ying Sheng", "Sandeep Tata", "James B. Wendt", "Jing Xie", "Qi Zhao", "Marc Najork"], "conference": "24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2018)", "date": "", "affiliation": "Google", "abstract": "Extracting structured data from emails can enable several assistive experiences, such as reminding the user when a bill payment is due, answering queries about the departure time of a booked flight, or proactively surfacing an emailed discount coupon while the user is at that store.\nThis paper presents Juicer, a system for extracting information from email that is serving over a billion Gmail users daily. We describe how the design of the system was informed by three key principles: scaling to a planet-wide email service, isolating the complexity to provide a simple experience for the developer, and safeguarding the privacy of users (our team and the developers we support are not allowed to view any single email). We describe the design tradeoffs made in building this system, the challenges faced and the approaches used to tackle them. We present case studies of three extraction tasks implemented on this platform\u00e2\u0080\u0094bill reminders, commercial offers, and hotel reservations\u00e2\u0080\u0094to illustrate the effectiveness of the platform despite challenges unique to each task. Finally, we outline several areas of ongoing research in large-scale machine-learned information extraction from email."}
{"title": "Diminishing Returns Shape Constraints for Interpretability and Regularization", "authors": ["Dara Bahri", "Andy Cotter", "Kevin Canini"], "conference": "NIPS 2018 (2018)", "date": "", "affiliation": "Google", "abstract": "We investigate machine learning models that can provide diminishing returns\nand accelerating returns guarantees to capture prior knowledge or policies\nabout how outputs should depend on inputs. We show that one can build\nflexible, nonlinear, multi-dimensional models using lattice functions with any\ncombination of concavity/convexity and monotonicity constraints on any\nsubsets of features, and compare to new shape-constrained neural networks.\nWe demonstrate on real-world examples that these shape constrained models\ncan provide tuning-free regularization and improve model understandability."}
{"title": "Ensuring Fairness in Machine Learning to Advance Health Equity", "authors": ["Alvin Rishi Rajkomar", "Greg Corrado", "Michael Howell"], "conference": "Annals of Internal Medicine (2018)", "date": "", "affiliation": "Google", "abstract": "A central promise of machine learning (ML) is to use historical data to project the future trajectories of patients.  Will they have a good or bad outcome? What diagnoses will they have? What treatments should they be given?  But in many cases, we do not want the future to look like the past, especially when the past contains patterns of human or structural biases against vulnerable populations."}
{"title": "Failure Modes of Variational Inference for Decision Making", "authors": ["Carlos Riquelme", "Matt Hoffman"], "conference": "ICML Workshop (2018)", "date": "", "affiliation": "Google", "abstract": "In this paper we highlight the risks of relying on\nmean-field variational inference to learn models\nthat are used as simulators for decision making.\nWe study the role of accurate inference for latent\nvariable models in terms of cumulative reward\nperformance. We show how naive mean-field\nvariational inference at test time can lead to poor\ndecisions in basic but fundamental quadratic control problems with continuous actions, as relevant\ncorrelations in the latent space are ignored. We\nthen extend these examples to a more complex\nnon-linear scenario with asymmetric costs, where\nregret is even more significant."}
{"title": "Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy", "authors": ["Jonathan Krause", "Varun Gulshan", "Kasumi Widner", "Greg Corrado", "Lily Peng", "Dale Webster"], "conference": "Ophthalmology (2018)", "date": "", "affiliation": "Google", "abstract": "Purpose\nUse adjudication to quantify errors in diabetic retinopathy (DR) grading based on individual graders and majority decision, and to train an improved automated algorithm for DR grading."}
{"title": "Human-in-the-Loop Interpretability Prior", "authors": ["Been Kim"], "conference": "NeurIPS (Spotlight) (2018)", "date": "", "affiliation": "Google", "abstract": "We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks."}
{"title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)", "authors": ["Been Kim", "Martin Wattenberg", "Carrie Jun Cai", "James Wexler", "Fernanda Viegas", "Rory Abbott Sayres"], "conference": "ICML (2018)", "date": "", "affiliation": "Google", "abstract": "The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of \u00e2\u0080\u009czebra\u00e2\u0080\u009d is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application."}
{"title": "Intriguing Properties of Adversarial Examples", "authors": ["Ekin Dogus Cubuk", "Quoc V. Le"], "conference": "ICLR (2018)", "date": "", "affiliation": "Google", "abstract": "It is becoming increasingly clear that many machine learning classifiers are vulnerable\nto adversarial examples. In attempting to explain the origin of adversarial\nexamples, previous studies have typically focused on the fact that neural networks\noperate on high dimensional data, they overfit, or they are too linear. Here we\nargue that the origin of adversarial examples is primarily due to an inherent uncertainty\nthat neural networks have about their predictions. We show that the functional\nform of this uncertainty is independent of architecture, dataset, and training\nprotocol; and depends only on the statistics of the logit differences of the network,\nwhich do not change significantly during training. This leads to adversarial error\nhaving a universal scaling, as a power-law, with respect to the size of the adversarial\nperturbation. We show that this universality holds for a broad range of datasets\n(MNIST, CIFAR10, ImageNet, and random data), models (including state-of-theart\ndeep networks, linear models, adversarially trained networks, and networks\ntrained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated\nby these results, we study the effects of reducing prediction entropy on\nadversarial robustness. Finally, we study the effect of network architectures on\nadversarial sensitivity. To do this, we use neural architecture search with reinforcement\nlearning to find adversarially robust architectures on CIFAR10. Our\nresulting architecture is more robust to white and black box attacks compared to\nprevious attempts."}
{"title": "Learning Differentially Private Recurrent Language Models", "authors": ["Brendan McMahan", "Daniel Ramage", "Li Zhang"], "conference": "International Conference on Learning Representations (ICLR) (2018)", "date": "", "affiliation": "Google", "abstract": "We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes \"large step\" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset."}
{"title": "Learning how to explain neural networks: PatternNet and PatternAttribution", "authors": ["Pieter-jan Kindermans", "Dumitru Erhan", "Been Kim"], "conference": "ICLR (2018)", "date": "", "affiliation": "Google", "abstract": "DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of  simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks."}
{"title": "Learning to Attack: Adversarial Transformation Networks", "authors": ["Shumeet Baluja", "Ian Fischer"], "conference": "Proceedings of AAAI-2018", "date": "", "affiliation": "Google", "abstract": "With the rapidly increasing popularity of deep neural networks\nfor image recognition tasks, a parallel interest in generating\nadversarial examples to attack the trained models has\narisen. To date, these approaches have involved either directly\ncomputing gradients with respect to the image pixels or directly\nsolving an optimization on the image pixels. We generalize\nthis pursuit in a novel direction: can a separate network\nbe trained to efficiently attack another fully trained network?\nWe demonstrate that it is possible, and that the generated\nattacks yield startling insights into the weaknesses of\nthe target network. We call such a network an Adversarial\nTransformation Network (ATN). ATNs transform any input\ninto an adversarial attack on the target network, while being\nminimally perturbing to the original inputs and the target network\u00e2\u0080\u0099s\noutputs. Further, we show that ATNs are capable of\nnot only causing the target network to make an error, but can\nbe constructed to explicitly control the type of misclassification\nmade. We demonstrate ATNs on both simple MNIST digit\nclassifiers and state-of-the-art ImageNet classifiers deployed\nby Google, Inc.: Inception ResNet-v2."}
{"title": "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning", "authors": ["Shane Gu", "Julian Ibarz", "Sergey Levine"], "conference": "ICLR (2018)", "date": "", "affiliation": "Google", "abstract": "Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states."}
{"title": "Measuring and Mitigating Unintended Bias in Text Classification", "authors": ["Lucas Dixon", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman"], "conference": "AAAI/ACM Conference on AI, Ethics, and Society (2018)", "date": "", "affiliation": "Google", "abstract": "We introduce and illustrate a new approach to measuring and\nmitigating unintended bias in machine learning models. Our\ndefinition of unintended bias is parameterized by a test set\nand a subset of input features. We illustrate how this can\nbe used to evaluate text classifiers using a synthetic test set\nand a public corpus of comments annotated for toxicity from\nWikipedia Talk pages. We also demonstrate how imbalances\nin training data can lead to unintended bias in the resulting\nmodels, and therefore potentially unfair applications. We use\na set of common demographic identity terms as the subset of\ninput features on which we measure bias. This technique permits\nanalysis in the common scenario where demographic information\non authors and readers is unavailable, so that bias\nmitigation must focus on the content of the text itself. The\nmitigation method we introduce is an unsupervised approach\nbased on balancing the training dataset. We demonstrate that\nthis approach reduces the unintended bias without compromising\noverall model quality"}
{"title": "Mitigating Unwanted Biases with Adversarial Learning", "authors": ["Blake Lemoine"], "conference": "(2018) (2018)", "date": "", "affiliation": "Google", "abstract": "Machine learning can be used to train a model that accurately represents\nthe data on which it is trained. The most common loss functions\nminimized by gradient descent involve accuracy. However,\nmodeling the training data optimally requires accurately modeling\nany undesirable biases present in that training data. One task\nwhich easily demonstrates this phenomenon is word embeddings\nlearned from standard corpora. When such word embeddings are\nused to perform tasks like analogy completion, the bias in the word\nembeddings propagates to the predicted analogy completions. Ideally\nwe would like to remove the biased information which might\nimpact task performance while retaining as much other semantic\ninformation as possible. We present here a method for debiasing\nnetworks using an adversary. First we formalize this problem by\ndescribing the nature of the input to our network X, describing\nthe prediction which is desired Y and the protected variable Z. The\nobjective then becomes to maximize the primary network\u00e2\u0080\u0099s ability\nto predict Y while minimizing the adversary\u00e2\u0080\u0099s ability to use that\nprediction to predict Z. When applied to analogy completion this\nmethod results in embeddings which are still quite useful for performing\nanalogy completion but without producing predictions\nimpacted by bias prediction. When applied to a categorization task\nsuch as the one in the UCI Adult Dataset it results in a predictive\nmodel that maintains accuracy while ensuring equality of odds.\nThis method is quite flexible and is applicable to any problem set\nwhich is expressible as a model which predicts a label Y using an\ninput X while trying to be fair with respect to a protected variable\nZ."}
{"title": "Motivating the Rules of the Game for Adversarial Example Research", "authors": ["George E. Dahl"], "conference": "arxiv (2018)", "date": "", "affiliation": "Google", "abstract": "Advances in machine learning have led to broad deployment of systems with impressive\nperformance on important problems. Nonetheless, these systems can be induced\nto make errors on data that are surprisingly similar to examples the learned system\nhandles correctly. The existence of these errors raises a variety of questions about\nout-of-sample generalization and whether bad actors might use such examples to abuse\ndeployed systems. As a result of these security concerns, there has been a flurry of\nrecent papers proposing algorithms to defend against such malicious perturbations of\ncorrectly handled examples. It is unclear how such misclassifications represent a different\nkind of security problem than other errors, or even other attacker-produced\nexamples that have no specific relationship to an uncorrupted input. In this paper,\nwe argue that adversarial example defense papers have, to date, mostly considered\nabstract, toy games that do not relate to any specific security concern. Furthermore,\ndefense papers have not yet precisely described all the abilities and limitations of attackers\nthat would be relevant in practical security. Towards this end, we establish a\ntaxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally,\nwe provide a series of recommendations outlining a path forward for future work\nto more clearly articulate the threat model and perform more meaningful evaluation."}
{"title": "Multi-Task Learning for Personal Search Ranking with Query Clustering", "authors": ["Maryam Karimzadehgan", "Michael Bendersky", "Zhen Qin", "Don Metzler"], "conference": "Proceedings of ACM Conference on Information and Knowledge Management (CIKM) (2018)", "date": "", "affiliation": "Google", "abstract": "User needs vary significantly across different tasks, and therefore\ntheir queries will also vary significantly in their expressiveness\nand semantics. Many studies have been proposed\nto model such query diversity by obtaining query types and\nbuilding query-dependent ranking models. To obtain query\ntypes, these studies typically require either a labeled query\ndataset or clicks from multiple users aggregated over the\nsame document. These techniques, however, are not applicable\nwhen manual query labeling is not viable, and aggregated\nclicks are unavailable due to the private nature of the document\ncollection, e.g., in personal search scenarios. Therefore,\nin this paper, we study the problem of how to obtain query\ntype in an unsupervised fashion and how to leverage this information\nusing query-dependent ranking models in personal\nsearch. We first develop a hierarchical clustering algorithm\nbased on truncated SVD and varimax rotation to obtain\ncoarse-to-fine query types. Then, we propose three query-dependent\nranking models, including two neural models that\nleverage query type information as additional features, and\none novel multi-task neural model that is trained to simultaneously\nrank documents and predict query types. We evaluate\nour ranking models using the click data collected from one of\nthe world\u00e2\u0080\u0099s largest personal search engines. The experiments\ndemonstrate that the proposed multi-task model can significantly\noutperform the baseline neural models, which either\ndo not incorporate query type information or just simply\nfeed query type as an additional feature. To the best of our\nknowledge, this is the first successful application of query-dependent\nmulti-task learning in personal search ranking."}
{"title": "Playing the Game of Universal Adversarial Perturbations", "authors": ["Mateusz Malinowski", "Olivier Pietquin"], "conference": "(2018) (2018)", "date": "", "affiliation": "Google", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely \\fp,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet."}
{"title": "Privacy Amplification by Iteration", "authors": [], "conference": "2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS)", "date": "", "affiliation": "Google", "abstract": "Most commonly used learning algorithms work by iteratively updating an intermediate solution using one or a few data points in each iteration.  Analysis of differential privacy for such algorithms often involves ensuring privacy of each step and then reasoning about the cumulative privacy cost of the algorithm. This is enabled by composition theorems for differential privacy that allow releasing of all the intermediate results.  In this work, we demonstrate that for contractive iterations, not releasing the intermediate results strongly amplifies the privacy guarantees.\n"}
{"title": "Privacy in Geospatial Applications and Location-Based Social Networks", "authors": ["Igor Bilogrevic"], "conference": "Handbook of Mobile Data Privacy (2018)", "date": "", "affiliation": "Google", "abstract": "The use of location data has greatly benefited from the availability of location-based services, the popularity of social networks, and the accessibility of public location data sets. However, in addition to providing users with the ability to obtain accurate driving directions or the convenience of geo-tagging friends and pictures, location is also a very sensitive type of data, as attested by more than a decade of research on different aspects of privacy related to location data."}
{"title": "Privacy-preserving Prediction", "authors": [], "conference": "Conference on Learning Theory (COLT) 2018", "date": "", "affiliation": "Google", "abstract": "Ensuring differential privacy of models learned from sensitive user data is an important goal that has been studied extensively in recent years.\nIt is now known that for some basic learning problems, especially those involving high-dimensional data, producing an accurate private model requires much more data than learning without privacy. At the same time, in many applications it is not necessary to expose the model itself. Instead users may be allowed to query the prediction model on their inputs only through an appropriate interface. Here we formulate the problem of ensuring privacy of individual predictions and investigate the overheads required to achieve it in several standard models of classification and regression."}
{"title": "Risk-Sensitive Generative Adversarial Imitation Learning", "authors": ["Yinlam Chow"], "conference": "AISTATS (2018)", "date": "", "affiliation": "Google", "abstract": "We study risk-sensitive imitation learning where the agent's goal is to perform at least as well as the expert in terms of a risk profile. We first formulate our risk-sensitive imitation learning setting. We consider the generative adversarial approach to imitation learning (GAIL) and derive an optimization problem for our formulation, which we call it risk-sensitive GAIL (RS-GAIL). We then derive two different versions of our RS-GAIL optimization problem that aim at matching the risk profiles of the agent and the expert w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop risk-sensitive generative adversarial imitation learning algorithms based on these optimization problems. We evaluate the performance of our algorithms and compare them with GAIL and the risk-averse imitation learning (RAIL) algorithms in two MuJoCo and two OpenAI classical control tasks."}
{"title": "Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints", "authors": ["Morteza Zadimoghaddam"], "conference": "Thirty-fifth International Conference on Machine Learning, ICML 2018", "date": "", "affiliation": "Google", "abstract": "Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation. We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against \\textit&lbrace;any&rbrace; number of adversarial deletions. We extensively evaluate the performance of our algorithms against prior state-of-the-art on real-world applications, including (i) Uber-pick up locations with location privacy constraints; (ii) feature selection with fairness constraints for income prediction and crime rate prediction; and (iii) robust to deletion summarization of census data, consisting of 2,458,285 feature vectors."}
{"title": "Scalable Private Learning with PATE", "authors": ["Nicolas Papernot", "Shuang Song"], "conference": "International Conference on Learning Representations (ICLR) (2018)", "date": "", "affiliation": "Google", "abstract": "The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a \"student\" model the knowledge of an ensemble of \"teacher\" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers\u00e2\u0080\u0099 answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets."}
{"title": "Text Embeddings Contain Bias. Here's Why That Matters.", "authors": ["Mario Guajardo-C\u00e9spedes"], "conference": "(2018) (2018)", "date": "", "affiliation": "Google", "abstract": "With the public release of embedding models, it\u00e2\u0080\u0099s important to understand the various biases that they contain. Developers who use them should be aware of the biases inherent in the models as well as how biases can manifest in downstream applications that use these models. In this post, we examine a few specific forms of bias and suggest tools for evaluating as well as mitigating bias."}
{"title": "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks", "authors": [], "conference": "ArXiv e-prints (2018)", "date": "", "affiliation": "Google", "abstract": "This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization."}
{"title": "To Trust Or Not To Trust A Classifier", "authors": ["Been Kim"], "conference": "NeurIPS (2018)", "date": "", "affiliation": "Google", "abstract": "Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the &lbrace;\\it trust score&rbrace;, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis."}
{"title": "Training On-Device Ranking Models from Cross-User Interactions in a Privacy-Preserving Fashion", "authors": ["Marc Najork"], "conference": "Proc. of the First Biennial Conference on Design of Experimental Search & Information Retrieval Systems (DESIRES) (2018)", "date": "", "affiliation": "Google", "abstract": "(See the attached PDF -- a one-page abstract for the upcoming DESIRES 2018 workshop)"}
{"title": "Towards A Rigorous Science of Interpretable Machine Learning", "authors": ["Been Kim"], "conference": "arXiv (2017)", "date": "", "affiliation": "Google", "abstract": "As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning."}
{"title": "Designing Unbiased Surveys for HCI Research", "authors": ["Hendrik M\u00fcller", "Aaron Sedley"], "conference": "CHI '14 Extended Abstracts on Human Factors in Computing Systems (2014)", "date": "", "affiliation": "Google", "abstract": "Surveys are a commonly used method within HCI research. While it initially appears easy and inexpensive to conduct surveys, overlooking key considerations in questionnaire design and the survey research process can yield skewed, biased, or entirely invalid survey results. Fortunately decades of academic research and analysis exist on optimizing the validity and reliability of survey data, from which this course will draw. To enable the creation of unbiased surveys, this course demonstrates questionnaire design biases and pitfalls, provides best practices for minimizing these, and reviews different uses of surveys within HCI."}
{"title": "WCMP: Weighted Cost Multipathing for Improved Fairness in Data Centers", "authors": ["Junlan Zhou", "Leon Poutievski", "Amin Vahdat"], "conference": "EuroSys '14: Proceedings of the Ninth European Conference on Computer Systems (2014)", "date": "", "affiliation": "Google", "abstract": "Data Center topologies employ multiple paths among servers to deliver scalable, cost-effective network capacity. The simplest and the most widely deployed approach for load balancing among these paths, Equal Cost Multipath (ECMP), hashes flows among the shortest paths toward a destination. ECMP leverages uniform hashing of balanced flow sizes to achieve fairness and good load balancing in data centers. However, we show that ECMP further assumes a balanced, regular, and fault-free topology, which are invalid assumptions in practice that can lead to substantial performance degradation and, worse, variation in flow bandwidths even for same size flows."}
{"title": "A practical algorithm for balancing the max-min fairness and throughput objectives in traffic engineering", "authors": ["Subhasree Mandal"], "conference": "INFOCOM (2012)", "date": "", "affiliation": "Google", "abstract": "One of the goals of traffic engineering is to achieve a\nflexible trade-off between fairness and throughput so that users\nare satisfied with their bandwidth allocation and the network\noperator is satisfied with the utilization of network resources. In\nthis paper, we propose a novel way to balance the throughput\nand fairness objectives with linear programming. It allows the\nnetwork operator to precisely control the trade-off by bounding\nthe fairness degradation for each commodity compared to the\nmax-min fair solution or the throughput degradation compared\nto the optimal throughput. We also present improvements to a\nprevious algorithm that achieves max-min fairness by solving a\nseries of linear programs. We significantly reduce the number\nof steps needed when the access rate of commodities is limited.\nWe extend the algorithm to two important practical use cases:\nimportance weights and piece-wise linear utility functions for\ncommodities. Our experiments on synthetic and real networks\nshow that our algorithms achieve a significant speedup and\nprovide practical insights on the trade-off between fairness and\nthroughput."}
{"title": "Upward Max Min Fairness", "authors": ["Avinatan Hassidim", "Haim Kaplan", "Alok Kumar", "Danny Raz", "Michal Segalov"], "conference": "INFOCOM (2012)", "date": "", "affiliation": "Google", "abstract": "No abstract available; check out the Download or Google Scholar links above for publications details."}
